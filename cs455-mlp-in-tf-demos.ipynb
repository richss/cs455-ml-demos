{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CS 455/595a: MLP Demo using TensorFlow</center></h1>\n",
    "<center>Richard S. Stansbury</center>\n",
    "\n",
    "This notebook applies the ANN techniques for the Titanic Survivors and Boston Housing Prediction models covered in [1] with the [Titanic](https://www.kaggle.com/c/titanic/) and [Boston Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) data sets for DT-based classification and regression, respectively.\n",
    "\n",
    "Several different approaches to model construction are shown ihe demos below\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1] Aurelen Geron. *Hands on Machine Learning with Scikit-Learn & TensorFlow* O'Reilley Media Inc, 2017.\n",
    "\n",
    "[2] Aurelen Geron. \"ageron/handson-ml: A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow.\" Github.com, online at: https://github.com/ageron/handson-ml [last accessed 2019-03-01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "1. [Titanic Survivor ANN Classifiers](#Titanic-Survivor-Classifier)\n",
    " \n",
    "2. [Boston Housing Cost Ensemble ANN Regressor](#Boston-Housing-Cost-Estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survivor Classifier\n",
    "\n",
    "## Set up - Imports of libraries and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# From: https://github.com/ageron/handson-ml/blob/master/09_up_and_running_with_tensorflow.ipynb    \n",
    "def reset_graph():\n",
    "    tf.reset_default_graph() \n",
    "    \n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"titanic-logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data and apply pipelines to pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 9)\n",
      "(891, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data from input files into Pandas data frames\n",
    "data_path = os.path.join(\"datasets\",\"titanic\")\n",
    "train_filename = \"train.csv\"\n",
    "test_filename = \"test.csv\"\n",
    "\n",
    "def read_csv(data_path, filename):\n",
    "    joined_path = os.path.join(data_path, filename)\n",
    "    return pd.read_csv(joined_path)\n",
    "\n",
    "# Read CSV file into Pandas Dataframes\n",
    "train_df = read_csv(data_path, train_filename)\n",
    "\n",
    "# Defining Data Pre-Processing Pipelines\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, attributes):\n",
    "        self.attributes = attributes\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attributes]\n",
    "\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent = pd.Series([X[c].value_counts().index[0] for c in X], \n",
    "                                       index = X.columns)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.fillna(self.most_frequent)\n",
    "\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "        (\"Select\", DataFrameSelector([\"Age\", \"Fare\", \"SibSp\", \"Parch\"])), # Selects Fields from dataframe\n",
    "        (\"Imputer\", SimpleImputer(strategy=\"median\")),   # Fills in NaN w/ median value for its column\n",
    "        (\"Scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "categories_pipe = Pipeline([\n",
    "        (\"Select\", DataFrameSelector([\"Pclass\", \"Sex\"])), # Selects Fields from dataframe\n",
    "        (\"MostFreqImp\", MostFrequentImputer()), # Fill in NaN with most frequent\n",
    "        (\"OneHot\", OneHotEncoder(sparse=False, categories='auto')), # Onehot encode\n",
    "    ])\n",
    "\n",
    "preprocessing_pipe = FeatureUnion(transformer_list = [\n",
    "        (\"numeric pipeline\", numeric_pipe), \n",
    "        (\"categories pipeline\", categories_pipe)\n",
    "     ]) \n",
    "\n",
    "# Process Input Data Using Pipleines\n",
    "X_data = preprocessing_pipe.fit_transform(train_df)\n",
    "y_data = train_df[\"Survived\"].values.reshape(-1,1)\n",
    "\n",
    "# Process the output data.\n",
    "feature_names = [\"Age\", \"Fare\", \"SibSp\", \"Parch\", \"Class0\", \"class1\",\"Sex0\", \"Sex1\"]\n",
    "\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596, 9) (596, 1) (295, 9) (295, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.33)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the TF.Estimator.DNNClassifier (formerly of TFLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\richa\\AppData\\Local\\Temp\\tmp4m0cs18a\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\richa\\\\AppData\\\\Local\\\\Temp\\\\tmp4m0cs18a', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025CECCB3898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:809: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\richa\\AppData\\Local\\Temp\\tmp4m0cs18a\\model.ckpt.\n",
      "INFO:tensorflow:loss = 34.350418, step = 1\n",
      "INFO:tensorflow:global_step/sec: 421.314\n",
      "INFO:tensorflow:loss = 21.448517, step = 101 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 668.444\n",
      "INFO:tensorflow:loss = 19.001314, step = 201 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 655.348\n",
      "INFO:tensorflow:loss = 18.797441, step = 301 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.578\n",
      "INFO:tensorflow:loss = 16.094637, step = 401 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 808.606\n",
      "INFO:tensorflow:loss = 19.970844, step = 501 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 895.236\n",
      "INFO:tensorflow:loss = 14.678972, step = 601 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 706.115\n",
      "INFO:tensorflow:loss = 21.641918, step = 701 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 759.598\n",
      "INFO:tensorflow:loss = 14.848111, step = 801 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.536\n",
      "INFO:tensorflow:loss = 20.112179, step = 901 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.323\n",
      "INFO:tensorflow:loss = 14.559097, step = 1001 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 871.886\n",
      "INFO:tensorflow:loss = 15.06392, step = 1101 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 682.088\n",
      "INFO:tensorflow:loss = 20.208746, step = 1201 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 748.267\n",
      "INFO:tensorflow:loss = 14.994849, step = 1301 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.321\n",
      "INFO:tensorflow:loss = 20.912, step = 1401 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 821.86\n",
      "INFO:tensorflow:loss = 17.83878, step = 1501 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.717\n",
      "INFO:tensorflow:loss = 10.590329, step = 1601 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 716.203\n",
      "INFO:tensorflow:loss = 21.922007, step = 1701 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 706.107\n",
      "INFO:tensorflow:loss = 21.151783, step = 1801 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 903.3\n",
      "INFO:tensorflow:loss = 16.670744, step = 1901 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 835.569\n",
      "INFO:tensorflow:loss = 22.252539, step = 2001 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 911.52\n",
      "INFO:tensorflow:loss = 16.738781, step = 2101 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 731.878\n",
      "INFO:tensorflow:loss = 14.523939, step = 2201 (0.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 706.106\n",
      "INFO:tensorflow:loss = 16.619556, step = 2301 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.541\n",
      "INFO:tensorflow:loss = 18.007967, step = 2401 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.53\n",
      "INFO:tensorflow:loss = 14.111547, step = 2501 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 928.408\n",
      "INFO:tensorflow:loss = 20.920988, step = 2601 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 737.25\n",
      "INFO:tensorflow:loss = 15.946479, step = 2701 (0.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 795.78\n",
      "INFO:tensorflow:loss = 13.190104, step = 2801 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.582\n",
      "INFO:tensorflow:loss = 13.565728, step = 2901 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.317\n",
      "INFO:tensorflow:loss = 19.345976, step = 3001 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 795.777\n",
      "INFO:tensorflow:loss = 11.7875, step = 3101 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 716.189\n",
      "INFO:tensorflow:loss = 9.482568, step = 3201 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 686.766\n",
      "INFO:tensorflow:loss = 21.045786, step = 3301 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 903.308\n",
      "INFO:tensorflow:loss = 22.506355, step = 3401 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 911.524\n",
      "INFO:tensorflow:loss = 13.147151, step = 3501 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 903.308\n",
      "INFO:tensorflow:loss = 12.101383, step = 3601 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 721.348\n",
      "INFO:tensorflow:loss = 22.774078, step = 3701 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.321\n",
      "INFO:tensorflow:loss = 21.236715, step = 3801 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.649\n",
      "INFO:tensorflow:loss = 17.473143, step = 3901 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 726.575\n",
      "INFO:tensorflow:loss = 14.418717, step = 4001 (0.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 835.565\n",
      "INFO:tensorflow:loss = 11.217694, step = 4101 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 895.241\n",
      "INFO:tensorflow:loss = 18.153568, step = 4201 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 928.399\n",
      "INFO:tensorflow:loss = 17.480757, step = 4301 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 895.241\n",
      "INFO:tensorflow:loss = 15.888954, step = 4401 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 731.88\n",
      "INFO:tensorflow:loss = 15.1963415, step = 4501 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 789.509\n",
      "INFO:tensorflow:loss = 19.211641, step = 4601 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.31\n",
      "INFO:tensorflow:loss = 12.926632, step = 4701 (0.113 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4768 into C:\\Users\\richa\\AppData\\Local\\Temp\\tmp4m0cs18a\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 14.806521.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-08T13:15:56Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\richa\\AppData\\Local\\Temp\\tmp4m0cs18a\\model.ckpt-4768\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-08-13:15:57\n",
      "INFO:tensorflow:Saving dict for global step 4768: accuracy = 0.77627116, accuracy_baseline = 0.61355937, auc = 0.8430988, auc_precision_recall = 0.8034408, average_loss = 0.53733635, global_step = 4768, label/mean = 0.38644066, loss = 52.838074, precision = 0.7790698, prediction/mean = 0.3744611, recall = 0.5877193\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4768: C:\\Users\\richa\\AppData\\Local\\Temp\\tmp4m0cs18a\\model.ckpt-4768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.77627116,\n",
       " 'accuracy_baseline': 0.61355937,\n",
       " 'auc': 0.8430988,\n",
       " 'auc_precision_recall': 0.8034408,\n",
       " 'average_loss': 0.53733635,\n",
       " 'label/mean': 0.38644066,\n",
       " 'loss': 52.838074,\n",
       " 'precision': 0.7790698,\n",
       " 'prediction/mean': 0.3744611,\n",
       " 'recall': 0.5877193,\n",
       " 'global_step': 4768}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construction Phase\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "feature_cols = [tf.feature_column.numeric_column(\"X\", shape=[X_data.shape[1]])]\n",
    "\n",
    "dnn_clf = tf.estimator.DNNClassifier(hidden_units=[20,20], n_classes=2,\n",
    "                                     feature_columns=feature_cols)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_train}, y=y_train, batch_size=50, num_epochs=400, shuffle=True)\n",
    "dnn_clf.train(input_fn=train_input_fn)\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_test}, y=y_test, shuffle=False)\n",
    "eval_results = dnn_clf.evaluate(input_fn=test_input_fn)\n",
    "                                \n",
    "eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use plain TensorFlow to implement a neural network using the tf.layers.dense class to define fully-connected (dense) layers of RELU and a softmax of the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Train: 0.6304348111152649 Test:0.6305084824562073\n",
      "1-Train: 0.695652186870575 Test:0.6542372703552246\n",
      "2-Train: 0.695652186870575 Test:0.68813556432724\n",
      "3-Train: 0.739130437374115 Test:0.7491525411605835\n",
      "4-Train: 0.804347813129425 Test:0.8169491291046143\n",
      "5-Train: 0.804347813129425 Test:0.8237287998199463\n",
      "6-Train: 0.8260869383811951 Test:0.8135592937469482\n",
      "7-Train: 0.8260869383811951 Test:0.8135592937469482\n",
      "8-Train: 0.8478260636329651 Test:0.8169491291046143\n",
      "9-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "10-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "11-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "12-Train: 0.8695651888847351 Test:0.8203389644622803\n",
      "13-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "14-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "15-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "16-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "17-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "18-Train: 0.8695651888847351 Test:0.8169491291046143\n",
      "19-Train: 0.9130434989929199 Test:0.8135592937469482\n",
      "20-Train: 0.9130434989929199 Test:0.8169491291046143\n",
      "21-Train: 0.9130434989929199 Test:0.8169491291046143\n",
      "22-Train: 0.9130434989929199 Test:0.8169491291046143\n",
      "23-Train: 0.9130434989929199 Test:0.8203389644622803\n",
      "24-Train: 0.9130434989929199 Test:0.8135592937469482\n",
      "25-Train: 0.9130434989929199 Test:0.8203389644622803\n",
      "26-Train: 0.9130434989929199 Test:0.8203389644622803\n",
      "27-Train: 0.9130434989929199 Test:0.8203389644622803\n",
      "28-Train: 0.9130434989929199 Test:0.8135592937469482\n",
      "29-Train: 0.8913043737411499 Test:0.8169491291046143\n",
      "30-Train: 0.8913043737411499 Test:0.8169491291046143\n",
      "31-Train: 0.8913043737411499 Test:0.8135592937469482\n",
      "32-Train: 0.8913043737411499 Test:0.810169517993927\n",
      "33-Train: 0.8913043737411499 Test:0.810169517993927\n",
      "34-Train: 0.8913043737411499 Test:0.810169517993927\n",
      "35-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "36-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "37-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "38-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "39-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "40-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "41-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "42-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "43-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "44-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "45-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "46-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "47-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "48-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "49-Train: 0.8913043737411499 Test:0.803389847278595\n",
      "50-Train: 0.8913043737411499 Test:0.800000011920929\n",
      "51-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "52-Train: 0.8913043737411499 Test:0.800000011920929\n",
      "53-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "54-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "55-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "56-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "57-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "58-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "59-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "60-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "61-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "62-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "63-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "64-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "65-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "66-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "67-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "68-Train: 0.8695651888847351 Test:0.806779682636261\n",
      "69-Train: 0.8913043737411499 Test:0.810169517993927\n",
      "70-Train: 0.8913043737411499 Test:0.810169517993927\n",
      "71-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "72-Train: 0.8913043737411499 Test:0.810169517993927\n",
      "73-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "74-Train: 0.8913043737411499 Test:0.810169517993927\n",
      "75-Train: 0.8913043737411499 Test:0.810169517993927\n",
      "76-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "77-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "78-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "79-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "80-Train: 0.8695651888847351 Test:0.806779682636261\n",
      "81-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "82-Train: 0.8695651888847351 Test:0.8135592937469482\n",
      "83-Train: 0.8695651888847351 Test:0.806779682636261\n",
      "84-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "85-Train: 0.8913043737411499 Test:0.8135592937469482\n",
      "86-Train: 0.8695651888847351 Test:0.8135592937469482\n",
      "87-Train: 0.8695651888847351 Test:0.806779682636261\n",
      "88-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "89-Train: 0.8913043737411499 Test:0.8135592937469482\n",
      "90-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "91-Train: 0.8695651888847351 Test:0.806779682636261\n",
      "92-Train: 0.8695651888847351 Test:0.806779682636261\n",
      "93-Train: 0.9130434989929199 Test:0.8135592937469482\n",
      "94-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "95-Train: 0.8695651888847351 Test:0.810169517993927\n",
      "96-Train: 0.8913043737411499 Test:0.806779682636261\n",
      "97-Train: 0.8695651888847351 Test:0.8135592937469482\n",
      "98-Train: 0.8695651888847351 Test:0.806779682636261\n",
      "99-Train: 0.8695651888847351 Test:0.810169517993927\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "def get_batch(X, iter, size):\n",
    "    return X[(iter*batch_size) : ((iter+1)*batch_size)]\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_instances = X_train.shape[0]\n",
    "\n",
    "# Construction\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_features), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Titanic_MLP\"):\n",
    "    hidden1 = tf.layers.dense(X, 20, name=\"Hidden-1\", activation = tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, 10, name=\"Hidden-2\", activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, 5, name=\"Hidden-3\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden3, 2, name=\"Survived\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('Loss', loss)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.name_scope(\"train\"): \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    step=0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for iteration in range(num_instances // batch_size + 1):\n",
    "            step+=1\n",
    "            X_batch = get_batch(X_train, iteration, batch_size)\n",
    "            y_batch = get_batch(y_train, iteration, batch_size)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={X: X_batch,\n",
    "                                            y: y_batch.reshape(y_batch.shape[0])})\n",
    "        acc_train, loss_summary_str, acc_summary_str = sess.run([accuracy, loss_summary, accuracy_summary],feed_dict={X: X_batch, y: y_batch.reshape(y_batch.shape[0])})\n",
    "        file_writer.add_summary(loss_summary_str,step)\n",
    "        file_writer.add_summary(acc_summary_str,step)\n",
    "        acc_val = accuracy.eval(feed_dict={X:X_test, y: y_test.reshape(y_test.shape[0])})\n",
    "        \n",
    "        print(\"{}-Train: {} Test:{}\".format(epoch,\n",
    "                                           acc_train,\n",
    "                                           acc_val))\n",
    "        \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 596 samples, validate on 295 samples\n",
      "Epoch 1/400\n",
      "596/596 [==============================] - 0s 465us/sample - loss: 0.7282 - acc: 0.4648 - val_loss: 0.6858 - val_acc: 0.6203\n",
      "Epoch 2/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.6772 - acc: 0.6141 - val_loss: 0.6643 - val_acc: 0.6237\n",
      "Epoch 3/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.6609 - acc: 0.6326 - val_loss: 0.6526 - val_acc: 0.6237\n",
      "Epoch 4/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.6507 - acc: 0.6376 - val_loss: 0.6426 - val_acc: 0.6271\n",
      "Epoch 5/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.6412 - acc: 0.6359 - val_loss: 0.6311 - val_acc: 0.6339\n",
      "Epoch 6/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.6293 - acc: 0.6359 - val_loss: 0.6176 - val_acc: 0.6339\n",
      "Epoch 7/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.6156 - acc: 0.6493 - val_loss: 0.6006 - val_acc: 0.6475\n",
      "Epoch 8/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5978 - acc: 0.6644 - val_loss: 0.5759 - val_acc: 0.6847\n",
      "Epoch 9/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5716 - acc: 0.7349 - val_loss: 0.5483 - val_acc: 0.7864\n",
      "Epoch 10/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5439 - acc: 0.7768 - val_loss: 0.5233 - val_acc: 0.8000\n",
      "Epoch 11/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5210 - acc: 0.7903 - val_loss: 0.5033 - val_acc: 0.8000\n",
      "Epoch 12/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5044 - acc: 0.8003 - val_loss: 0.4884 - val_acc: 0.8000\n",
      "Epoch 13/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4893 - acc: 0.7970 - val_loss: 0.4768 - val_acc: 0.8000\n",
      "Epoch 14/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4779 - acc: 0.8003 - val_loss: 0.4698 - val_acc: 0.8000\n",
      "Epoch 15/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4711 - acc: 0.8037 - val_loss: 0.4616 - val_acc: 0.8000\n",
      "Epoch 16/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4626 - acc: 0.8070 - val_loss: 0.4570 - val_acc: 0.7966\n",
      "Epoch 17/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4560 - acc: 0.8070 - val_loss: 0.4513 - val_acc: 0.8034\n",
      "Epoch 18/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4535 - acc: 0.8070 - val_loss: 0.4501 - val_acc: 0.8034\n",
      "Epoch 19/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4486 - acc: 0.8070 - val_loss: 0.4437 - val_acc: 0.8068\n",
      "Epoch 20/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4449 - acc: 0.8087 - val_loss: 0.4413 - val_acc: 0.8102\n",
      "Epoch 21/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4407 - acc: 0.8104 - val_loss: 0.4378 - val_acc: 0.8068\n",
      "Epoch 22/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4389 - acc: 0.8138 - val_loss: 0.4357 - val_acc: 0.8102\n",
      "Epoch 23/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4352 - acc: 0.8138 - val_loss: 0.4394 - val_acc: 0.8068\n",
      "Epoch 24/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4331 - acc: 0.8138 - val_loss: 0.4328 - val_acc: 0.8169\n",
      "Epoch 25/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4307 - acc: 0.8205 - val_loss: 0.4354 - val_acc: 0.8102\n",
      "Epoch 26/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4346 - acc: 0.8154 - val_loss: 0.4308 - val_acc: 0.8169\n",
      "Epoch 27/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4270 - acc: 0.8188 - val_loss: 0.4281 - val_acc: 0.8136\n",
      "Epoch 28/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4267 - acc: 0.8205 - val_loss: 0.4336 - val_acc: 0.8237\n",
      "Epoch 29/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4229 - acc: 0.8188 - val_loss: 0.4256 - val_acc: 0.8136\n",
      "Epoch 30/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4228 - acc: 0.8221 - val_loss: 0.4236 - val_acc: 0.8136\n",
      "Epoch 31/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4205 - acc: 0.8289 - val_loss: 0.4232 - val_acc: 0.8169\n",
      "Epoch 32/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4189 - acc: 0.8255 - val_loss: 0.4329 - val_acc: 0.8203\n",
      "Epoch 33/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4197 - acc: 0.8205 - val_loss: 0.4212 - val_acc: 0.8169\n",
      "Epoch 34/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4177 - acc: 0.8238 - val_loss: 0.4211 - val_acc: 0.8169\n",
      "Epoch 35/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4185 - acc: 0.8238 - val_loss: 0.4184 - val_acc: 0.8169\n",
      "Epoch 36/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4184 - acc: 0.8289 - val_loss: 0.4158 - val_acc: 0.8203\n",
      "Epoch 37/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4163 - acc: 0.8238 - val_loss: 0.4157 - val_acc: 0.8203\n",
      "Epoch 38/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.4134 - acc: 0.8255 - val_loss: 0.4132 - val_acc: 0.8203\n",
      "Epoch 39/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4120 - acc: 0.8289 - val_loss: 0.4143 - val_acc: 0.8203\n",
      "Epoch 40/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4128 - acc: 0.8255 - val_loss: 0.4117 - val_acc: 0.8203\n",
      "Epoch 41/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4147 - acc: 0.8289 - val_loss: 0.4129 - val_acc: 0.8169\n",
      "Epoch 42/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4102 - acc: 0.8272 - val_loss: 0.4122 - val_acc: 0.8169\n",
      "Epoch 43/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.4141 - acc: 0.820 - 0s 25us/sample - loss: 0.4096 - acc: 0.8272 - val_loss: 0.4131 - val_acc: 0.8203\n",
      "Epoch 44/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4083 - acc: 0.8272 - val_loss: 0.4105 - val_acc: 0.8237\n",
      "Epoch 45/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4082 - acc: 0.8305 - val_loss: 0.4122 - val_acc: 0.8203\n",
      "Epoch 46/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4119 - acc: 0.8255 - val_loss: 0.4215 - val_acc: 0.8271\n",
      "Epoch 47/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4081 - acc: 0.8289 - val_loss: 0.4092 - val_acc: 0.8169\n",
      "Epoch 48/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4054 - acc: 0.8322 - val_loss: 0.4102 - val_acc: 0.8169\n",
      "Epoch 49/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.4055 - acc: 0.8221 - val_loss: 0.4085 - val_acc: 0.8169\n",
      "Epoch 50/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4068 - acc: 0.8289 - val_loss: 0.4081 - val_acc: 0.8305\n",
      "Epoch 51/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4048 - acc: 0.8322 - val_loss: 0.4072 - val_acc: 0.8271\n",
      "Epoch 52/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4043 - acc: 0.8272 - val_loss: 0.4102 - val_acc: 0.8237\n",
      "Epoch 53/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4039 - acc: 0.8272 - val_loss: 0.4067 - val_acc: 0.8136\n",
      "Epoch 54/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4012 - acc: 0.8238 - val_loss: 0.4071 - val_acc: 0.8271\n",
      "Epoch 55/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4063 - acc: 0.8305 - val_loss: 0.4064 - val_acc: 0.8237\n",
      "Epoch 56/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4058 - acc: 0.8238 - val_loss: 0.4087 - val_acc: 0.8203\n",
      "Epoch 57/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4010 - acc: 0.8238 - val_loss: 0.4065 - val_acc: 0.8271\n",
      "Epoch 58/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4017 - acc: 0.8289 - val_loss: 0.4075 - val_acc: 0.8136\n",
      "Epoch 59/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4004 - acc: 0.8322 - val_loss: 0.4056 - val_acc: 0.8271\n",
      "Epoch 60/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3985 - acc: 0.8289 - val_loss: 0.4081 - val_acc: 0.8169\n",
      "Epoch 61/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3989 - acc: 0.8289 - val_loss: 0.4087 - val_acc: 0.8203\n",
      "Epoch 62/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3977 - acc: 0.8305 - val_loss: 0.4073 - val_acc: 0.8237\n",
      "Epoch 63/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.4007 - acc: 0.8339 - val_loss: 0.4043 - val_acc: 0.8237\n",
      "Epoch 64/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3997 - acc: 0.8272 - val_loss: 0.4055 - val_acc: 0.8237\n",
      "Epoch 65/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4012 - acc: 0.8221 - val_loss: 0.4097 - val_acc: 0.8237\n",
      "Epoch 66/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3976 - acc: 0.8272 - val_loss: 0.4036 - val_acc: 0.8237\n",
      "Epoch 67/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3968 - acc: 0.8305 - val_loss: 0.4046 - val_acc: 0.8203\n",
      "Epoch 68/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3959 - acc: 0.8322 - val_loss: 0.4069 - val_acc: 0.8203\n",
      "Epoch 69/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3968 - acc: 0.8305 - val_loss: 0.4092 - val_acc: 0.8203\n",
      "Epoch 70/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3942 - acc: 0.8356 - val_loss: 0.4056 - val_acc: 0.8203\n",
      "Epoch 71/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3988 - acc: 0.8289 - val_loss: 0.4047 - val_acc: 0.8237\n",
      "Epoch 72/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3984 - acc: 0.8272 - val_loss: 0.4078 - val_acc: 0.8237\n",
      "Epoch 73/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3952 - acc: 0.8356 - val_loss: 0.4045 - val_acc: 0.8203\n",
      "Epoch 74/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3935 - acc: 0.8356 - val_loss: 0.4056 - val_acc: 0.8169\n",
      "Epoch 75/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3931 - acc: 0.8406 - val_loss: 0.4036 - val_acc: 0.8271\n",
      "Epoch 76/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3941 - acc: 0.8238 - val_loss: 0.4053 - val_acc: 0.8203\n",
      "Epoch 77/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3963 - acc: 0.8289 - val_loss: 0.4218 - val_acc: 0.8237\n",
      "Epoch 78/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3973 - acc: 0.8339 - val_loss: 0.4037 - val_acc: 0.8271\n",
      "Epoch 79/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3918 - acc: 0.8305 - val_loss: 0.4105 - val_acc: 0.8237\n",
      "Epoch 80/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.3936 - acc: 0.8322 - val_loss: 0.4146 - val_acc: 0.8237\n",
      "Epoch 81/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3939 - acc: 0.8356 - val_loss: 0.4083 - val_acc: 0.8237\n",
      "Epoch 82/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3918 - acc: 0.8339 - val_loss: 0.4037 - val_acc: 0.8237\n",
      "Epoch 83/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3909 - acc: 0.8389 - val_loss: 0.4115 - val_acc: 0.8271\n",
      "Epoch 84/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.4193 - acc: 0.840 - 0s 23us/sample - loss: 0.3918 - acc: 0.8356 - val_loss: 0.4057 - val_acc: 0.8237\n",
      "Epoch 85/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3914 - acc: 0.8372 - val_loss: 0.4269 - val_acc: 0.7898\n",
      "Epoch 86/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3971 - acc: 0.8356 - val_loss: 0.4014 - val_acc: 0.8237\n",
      "Epoch 87/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3916 - acc: 0.8406 - val_loss: 0.4078 - val_acc: 0.8237\n",
      "Epoch 88/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3917 - acc: 0.8372 - val_loss: 0.4064 - val_acc: 0.8237\n",
      "Epoch 89/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3896 - acc: 0.8339 - val_loss: 0.4120 - val_acc: 0.8237\n",
      "Epoch 90/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3946 - acc: 0.8389 - val_loss: 0.4111 - val_acc: 0.8203\n",
      "Epoch 91/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3912 - acc: 0.8389 - val_loss: 0.4128 - val_acc: 0.8237\n",
      "Epoch 92/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3936 - acc: 0.8356 - val_loss: 0.4090 - val_acc: 0.8203\n",
      "Epoch 93/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3914 - acc: 0.8339 - val_loss: 0.4061 - val_acc: 0.8271\n",
      "Epoch 94/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3894 - acc: 0.8406 - val_loss: 0.4054 - val_acc: 0.8237\n",
      "Epoch 95/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3899 - acc: 0.8372 - val_loss: 0.4041 - val_acc: 0.8237\n",
      "Epoch 96/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3896 - acc: 0.8372 - val_loss: 0.4063 - val_acc: 0.8271\n",
      "Epoch 97/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3915 - acc: 0.8372 - val_loss: 0.4067 - val_acc: 0.8271\n",
      "Epoch 98/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3888 - acc: 0.8305 - val_loss: 0.4082 - val_acc: 0.8203\n",
      "Epoch 99/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3873 - acc: 0.8339 - val_loss: 0.4093 - val_acc: 0.8136\n",
      "Epoch 100/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3870 - acc: 0.8440 - val_loss: 0.4035 - val_acc: 0.8271\n",
      "Epoch 101/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3886 - acc: 0.8372 - val_loss: 0.4084 - val_acc: 0.8237\n",
      "Epoch 102/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3867 - acc: 0.8389 - val_loss: 0.4159 - val_acc: 0.8271\n",
      "Epoch 103/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3868 - acc: 0.8372 - val_loss: 0.4096 - val_acc: 0.7932\n",
      "Epoch 104/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3903 - acc: 0.8322 - val_loss: 0.4155 - val_acc: 0.8237\n",
      "Epoch 105/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3927 - acc: 0.8322 - val_loss: 0.4064 - val_acc: 0.8271\n",
      "Epoch 106/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3916 - acc: 0.8322 - val_loss: 0.4062 - val_acc: 0.8271\n",
      "Epoch 107/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3869 - acc: 0.8406 - val_loss: 0.4105 - val_acc: 0.8237\n",
      "Epoch 108/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3848 - acc: 0.8440 - val_loss: 0.4078 - val_acc: 0.8271\n",
      "Epoch 109/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3860 - acc: 0.8356 - val_loss: 0.4111 - val_acc: 0.8237\n",
      "Epoch 110/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3891 - acc: 0.8289 - val_loss: 0.4259 - val_acc: 0.8068\n",
      "Epoch 111/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3926 - acc: 0.8423 - val_loss: 0.4152 - val_acc: 0.8305\n",
      "Epoch 112/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3849 - acc: 0.8507 - val_loss: 0.4131 - val_acc: 0.8271\n",
      "Epoch 113/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3845 - acc: 0.8406 - val_loss: 0.4117 - val_acc: 0.8203\n",
      "Epoch 114/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3879 - acc: 0.8389 - val_loss: 0.4149 - val_acc: 0.8237\n",
      "Epoch 115/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.3850 - acc: 0.8372 - val_loss: 0.4148 - val_acc: 0.8271\n",
      "Epoch 116/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3836 - acc: 0.8423 - val_loss: 0.4064 - val_acc: 0.8203\n",
      "Epoch 117/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3837 - acc: 0.8389 - val_loss: 0.4099 - val_acc: 0.8237\n",
      "Epoch 118/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3910 - acc: 0.8406 - val_loss: 0.4320 - val_acc: 0.8169\n",
      "Epoch 119/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3876 - acc: 0.8322 - val_loss: 0.4106 - val_acc: 0.8305\n",
      "Epoch 120/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3805 - acc: 0.8423 - val_loss: 0.4094 - val_acc: 0.8305\n",
      "Epoch 121/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.4082 - acc: 0.830 - 0s 30us/sample - loss: 0.3824 - acc: 0.8372 - val_loss: 0.4122 - val_acc: 0.8203\n",
      "Epoch 122/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3874 - acc: 0.8440 - val_loss: 0.4106 - val_acc: 0.8271\n",
      "Epoch 123/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3827 - acc: 0.8406 - val_loss: 0.4139 - val_acc: 0.8271\n",
      "Epoch 124/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3805 - acc: 0.8406 - val_loss: 0.4100 - val_acc: 0.8271\n",
      "Epoch 125/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3839 - acc: 0.8423 - val_loss: 0.4110 - val_acc: 0.8271\n",
      "Epoch 126/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3839 - acc: 0.8389 - val_loss: 0.4106 - val_acc: 0.8169\n",
      "Epoch 127/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3857 - acc: 0.8372 - val_loss: 0.4078 - val_acc: 0.8271\n",
      "Epoch 128/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3808 - acc: 0.8423 - val_loss: 0.4085 - val_acc: 0.8237\n",
      "Epoch 129/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3808 - acc: 0.8406 - val_loss: 0.4122 - val_acc: 0.8237\n",
      "Epoch 130/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3810 - acc: 0.8389 - val_loss: 0.4224 - val_acc: 0.8237\n",
      "Epoch 131/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3799 - acc: 0.8389 - val_loss: 0.4122 - val_acc: 0.8136\n",
      "Epoch 132/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3793 - acc: 0.8490 - val_loss: 0.4145 - val_acc: 0.8237\n",
      "Epoch 133/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3806 - acc: 0.8389 - val_loss: 0.4161 - val_acc: 0.8237\n",
      "Epoch 134/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3864 - acc: 0.8339 - val_loss: 0.4110 - val_acc: 0.8305\n",
      "Epoch 135/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3802 - acc: 0.8356 - val_loss: 0.4131 - val_acc: 0.8237\n",
      "Epoch 136/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3812 - acc: 0.8389 - val_loss: 0.4090 - val_acc: 0.8169\n",
      "Epoch 137/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3785 - acc: 0.8423 - val_loss: 0.4108 - val_acc: 0.8136\n",
      "Epoch 138/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3854 - acc: 0.8456 - val_loss: 0.4109 - val_acc: 0.8136\n",
      "Epoch 139/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3796 - acc: 0.8339 - val_loss: 0.4131 - val_acc: 0.8136\n",
      "Epoch 140/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3779 - acc: 0.8507 - val_loss: 0.4130 - val_acc: 0.8237\n",
      "Epoch 141/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3772 - acc: 0.8372 - val_loss: 0.4175 - val_acc: 0.8136\n",
      "Epoch 142/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3761 - acc: 0.8406 - val_loss: 0.4269 - val_acc: 0.8237\n",
      "Epoch 143/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3781 - acc: 0.8440 - val_loss: 0.4155 - val_acc: 0.8102\n",
      "Epoch 144/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3825 - acc: 0.8423 - val_loss: 0.4181 - val_acc: 0.8237\n",
      "Epoch 145/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3776 - acc: 0.8389 - val_loss: 0.4141 - val_acc: 0.8102\n",
      "Epoch 146/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3821 - acc: 0.8372 - val_loss: 0.4145 - val_acc: 0.8237\n",
      "Epoch 147/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3806 - acc: 0.8389 - val_loss: 0.4173 - val_acc: 0.8136\n",
      "Epoch 148/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3777 - acc: 0.8456 - val_loss: 0.4192 - val_acc: 0.8034\n",
      "Epoch 149/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3763 - acc: 0.8339 - val_loss: 0.4128 - val_acc: 0.8203\n",
      "Epoch 150/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3816 - acc: 0.8372 - val_loss: 0.4225 - val_acc: 0.8169\n",
      "Epoch 151/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3818 - acc: 0.8423 - val_loss: 0.4181 - val_acc: 0.8102\n",
      "Epoch 152/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3767 - acc: 0.8456 - val_loss: 0.4148 - val_acc: 0.8000\n",
      "Epoch 153/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3769 - acc: 0.8473 - val_loss: 0.4146 - val_acc: 0.8169\n",
      "Epoch 154/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3771 - acc: 0.8389 - val_loss: 0.4187 - val_acc: 0.8169\n",
      "Epoch 155/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3763 - acc: 0.8406 - val_loss: 0.4197 - val_acc: 0.8271\n",
      "Epoch 156/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3763 - acc: 0.8389 - val_loss: 0.4142 - val_acc: 0.8102\n",
      "Epoch 157/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3815 - acc: 0.8406 - val_loss: 0.4164 - val_acc: 0.8169\n",
      "Epoch 158/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3760 - acc: 0.8406 - val_loss: 0.4134 - val_acc: 0.8169\n",
      "Epoch 159/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3812 - acc: 0.8372 - val_loss: 0.4146 - val_acc: 0.8136\n",
      "Epoch 160/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3738 - acc: 0.8540 - val_loss: 0.4276 - val_acc: 0.7932\n",
      "Epoch 161/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3801 - acc: 0.8372 - val_loss: 0.4162 - val_acc: 0.8271\n",
      "Epoch 162/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3743 - acc: 0.8406 - val_loss: 0.4160 - val_acc: 0.8136\n",
      "Epoch 163/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3730 - acc: 0.8440 - val_loss: 0.4268 - val_acc: 0.8136\n",
      "Epoch 164/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3755 - acc: 0.8389 - val_loss: 0.4216 - val_acc: 0.8169\n",
      "Epoch 165/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.3736 - acc: 0.8389 - val_loss: 0.4224 - val_acc: 0.8237\n",
      "Epoch 166/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3769 - acc: 0.8406 - val_loss: 0.4197 - val_acc: 0.8136\n",
      "Epoch 167/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3700 - acc: 0.8406 - val_loss: 0.4174 - val_acc: 0.8169\n",
      "Epoch 168/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3745 - acc: 0.8440 - val_loss: 0.4190 - val_acc: 0.8169\n",
      "Epoch 169/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.3749 - acc: 0.8423 - val_loss: 0.4291 - val_acc: 0.8203\n",
      "Epoch 170/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3742 - acc: 0.8440 - val_loss: 0.4191 - val_acc: 0.8102\n",
      "Epoch 171/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3745 - acc: 0.8423 - val_loss: 0.4215 - val_acc: 0.8136\n",
      "Epoch 172/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3758 - acc: 0.8389 - val_loss: 0.4197 - val_acc: 0.8068\n",
      "Epoch 173/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3686 - acc: 0.8473 - val_loss: 0.4403 - val_acc: 0.7864\n",
      "Epoch 174/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3735 - acc: 0.8473 - val_loss: 0.4272 - val_acc: 0.8068\n",
      "Epoch 175/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3765 - acc: 0.8423 - val_loss: 0.4224 - val_acc: 0.8102\n",
      "Epoch 176/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3693 - acc: 0.8406 - val_loss: 0.4203 - val_acc: 0.8136\n",
      "Epoch 177/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3715 - acc: 0.8507 - val_loss: 0.4325 - val_acc: 0.8102\n",
      "Epoch 178/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3764 - acc: 0.8473 - val_loss: 0.4327 - val_acc: 0.8000\n",
      "Epoch 179/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3686 - acc: 0.8423 - val_loss: 0.4248 - val_acc: 0.8136\n",
      "Epoch 180/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3728 - acc: 0.8356 - val_loss: 0.4265 - val_acc: 0.8136\n",
      "Epoch 181/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3182 - acc: 0.890 - 0s 37us/sample - loss: 0.3720 - acc: 0.8406 - val_loss: 0.4259 - val_acc: 0.8102\n",
      "Epoch 182/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.3687 - acc: 0.8440 - val_loss: 0.4257 - val_acc: 0.8000\n",
      "Epoch 183/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3758 - acc: 0.8456 - val_loss: 0.4291 - val_acc: 0.8000\n",
      "Epoch 184/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.3691 - acc: 0.8473 - val_loss: 0.4238 - val_acc: 0.8169\n",
      "Epoch 185/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3780 - acc: 0.8356 - val_loss: 0.4314 - val_acc: 0.8136\n",
      "Epoch 186/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3691 - acc: 0.8389 - val_loss: 0.4460 - val_acc: 0.8068\n",
      "Epoch 187/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3736 - acc: 0.8473 - val_loss: 0.4218 - val_acc: 0.8136\n",
      "Epoch 188/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.3663 - acc: 0.8423 - val_loss: 0.4267 - val_acc: 0.8034\n",
      "Epoch 189/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3744 - acc: 0.8305 - val_loss: 0.4268 - val_acc: 0.8068\n",
      "Epoch 190/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3674 - acc: 0.8523 - val_loss: 0.4309 - val_acc: 0.7864\n",
      "Epoch 191/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3675 - acc: 0.8406 - val_loss: 0.4220 - val_acc: 0.8102\n",
      "Epoch 192/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3666 - acc: 0.8507 - val_loss: 0.4256 - val_acc: 0.8169\n",
      "Epoch 193/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3651 - acc: 0.8490 - val_loss: 0.4233 - val_acc: 0.8136\n",
      "Epoch 194/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3683 - acc: 0.8406 - val_loss: 0.4242 - val_acc: 0.8068\n",
      "Epoch 195/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3646 - acc: 0.8473 - val_loss: 0.4420 - val_acc: 0.7864\n",
      "Epoch 196/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3657 - acc: 0.8456 - val_loss: 0.4287 - val_acc: 0.8136\n",
      "Epoch 197/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3755 - acc: 0.8406 - val_loss: 0.4254 - val_acc: 0.8136\n",
      "Epoch 198/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3677 - acc: 0.8523 - val_loss: 0.4222 - val_acc: 0.8136\n",
      "Epoch 199/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3704 - acc: 0.8372 - val_loss: 0.4203 - val_acc: 0.8136\n",
      "Epoch 200/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3663 - acc: 0.8423 - val_loss: 0.4272 - val_acc: 0.7932\n",
      "Epoch 201/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3656 - acc: 0.8456 - val_loss: 0.4366 - val_acc: 0.8102\n",
      "Epoch 202/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3636 - acc: 0.8440 - val_loss: 0.4493 - val_acc: 0.7932\n",
      "Epoch 203/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3658 - acc: 0.8440 - val_loss: 0.4240 - val_acc: 0.8102\n",
      "Epoch 204/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3655 - acc: 0.8473 - val_loss: 0.4275 - val_acc: 0.8068\n",
      "Epoch 205/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3630 - acc: 0.8423 - val_loss: 0.4253 - val_acc: 0.8068\n",
      "Epoch 206/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3656 - acc: 0.8423 - val_loss: 0.4215 - val_acc: 0.8136\n",
      "Epoch 207/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3614 - acc: 0.8440 - val_loss: 0.4301 - val_acc: 0.8068\n",
      "Epoch 208/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3625 - acc: 0.8523 - val_loss: 0.4353 - val_acc: 0.7966\n",
      "Epoch 209/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3673 - acc: 0.8490 - val_loss: 0.4343 - val_acc: 0.7864\n",
      "Epoch 210/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3666 - acc: 0.8339 - val_loss: 0.4254 - val_acc: 0.8136\n",
      "Epoch 211/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3664 - acc: 0.8507 - val_loss: 0.4495 - val_acc: 0.7966\n",
      "Epoch 212/400\n",
      "596/596 [==============================] - 0s 17us/sample - loss: 0.3754 - acc: 0.8456 - val_loss: 0.4348 - val_acc: 0.8136\n",
      "Epoch 213/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3597 - acc: 0.8507 - val_loss: 0.4452 - val_acc: 0.7966\n",
      "Epoch 214/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3639 - acc: 0.8540 - val_loss: 0.4448 - val_acc: 0.7864\n",
      "Epoch 215/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3642 - acc: 0.8456 - val_loss: 0.4343 - val_acc: 0.8068\n",
      "Epoch 216/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3695 - acc: 0.8440 - val_loss: 0.4323 - val_acc: 0.7932\n",
      "Epoch 217/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3683 - acc: 0.8406 - val_loss: 0.4304 - val_acc: 0.8000\n",
      "Epoch 218/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3644 - acc: 0.8507 - val_loss: 0.4354 - val_acc: 0.8169\n",
      "Epoch 219/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.3645 - acc: 0.8440 - val_loss: 0.4441 - val_acc: 0.7864\n",
      "Epoch 220/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3635 - acc: 0.8557 - val_loss: 0.4413 - val_acc: 0.8000\n",
      "Epoch 221/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3696 - acc: 0.8440 - val_loss: 0.4357 - val_acc: 0.8068\n",
      "Epoch 222/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3639 - acc: 0.8507 - val_loss: 0.4368 - val_acc: 0.8068\n",
      "Epoch 223/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3623 - acc: 0.8507 - val_loss: 0.4421 - val_acc: 0.7898\n",
      "Epoch 224/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3604 - acc: 0.8591 - val_loss: 0.4315 - val_acc: 0.8102\n",
      "Epoch 225/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3630 - acc: 0.8473 - val_loss: 0.4256 - val_acc: 0.8102\n",
      "Epoch 226/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3645 - acc: 0.8473 - val_loss: 0.4294 - val_acc: 0.8102\n",
      "Epoch 227/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3602 - acc: 0.8540 - val_loss: 0.4428 - val_acc: 0.8034\n",
      "Epoch 228/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3733 - acc: 0.8255 - val_loss: 0.4676 - val_acc: 0.7831\n",
      "Epoch 229/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3695 - acc: 0.8456 - val_loss: 0.4481 - val_acc: 0.7864\n",
      "Epoch 230/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3609 - acc: 0.8574 - val_loss: 0.4382 - val_acc: 0.8034\n",
      "Epoch 231/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3588 - acc: 0.8540 - val_loss: 0.4461 - val_acc: 0.7932\n",
      "Epoch 232/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3623 - acc: 0.8473 - val_loss: 0.4550 - val_acc: 0.7763\n",
      "Epoch 233/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3619 - acc: 0.8490 - val_loss: 0.4440 - val_acc: 0.8102\n",
      "Epoch 234/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3659 - acc: 0.8372 - val_loss: 0.4436 - val_acc: 0.7932\n",
      "Epoch 235/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3662 - acc: 0.8339 - val_loss: 0.4462 - val_acc: 0.8000\n",
      "Epoch 236/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3578 - acc: 0.8557 - val_loss: 0.4479 - val_acc: 0.7864\n",
      "Epoch 237/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3560 - acc: 0.8490 - val_loss: 0.4527 - val_acc: 0.8034\n",
      "Epoch 238/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3594 - acc: 0.8540 - val_loss: 0.4477 - val_acc: 0.7864\n",
      "Epoch 239/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3573 - acc: 0.8540 - val_loss: 0.4437 - val_acc: 0.8068\n",
      "Epoch 240/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3573 - acc: 0.8591 - val_loss: 0.4539 - val_acc: 0.7797\n",
      "Epoch 241/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3580 - acc: 0.8440 - val_loss: 0.4491 - val_acc: 0.7831\n",
      "Epoch 242/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3657 - acc: 0.8289 - val_loss: 0.4554 - val_acc: 0.7932\n",
      "Epoch 243/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3586 - acc: 0.8473 - val_loss: 0.4607 - val_acc: 0.7797\n",
      "Epoch 244/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3556 - acc: 0.8490 - val_loss: 0.4404 - val_acc: 0.8000\n",
      "Epoch 245/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3548 - acc: 0.8591 - val_loss: 0.4440 - val_acc: 0.8034\n",
      "Epoch 246/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3567 - acc: 0.8574 - val_loss: 0.4501 - val_acc: 0.7966\n",
      "Epoch 247/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3557 - acc: 0.8540 - val_loss: 0.4583 - val_acc: 0.7763\n",
      "Epoch 248/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3561 - acc: 0.8557 - val_loss: 0.4494 - val_acc: 0.7932\n",
      "Epoch 249/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3543 - acc: 0.8456 - val_loss: 0.4548 - val_acc: 0.7831\n",
      "Epoch 250/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3558 - acc: 0.8440 - val_loss: 0.4597 - val_acc: 0.7898\n",
      "Epoch 251/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3546 - acc: 0.8490 - val_loss: 0.4549 - val_acc: 0.7797\n",
      "Epoch 252/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3593 - acc: 0.8574 - val_loss: 0.4465 - val_acc: 0.7932\n",
      "Epoch 253/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3510 - acc: 0.8406 - val_loss: 0.4558 - val_acc: 0.7864\n",
      "Epoch 254/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3534 - acc: 0.8607 - val_loss: 0.4519 - val_acc: 0.8102\n",
      "Epoch 255/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3564 - acc: 0.8490 - val_loss: 0.4565 - val_acc: 0.7831\n",
      "Epoch 256/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3535 - acc: 0.8523 - val_loss: 0.4586 - val_acc: 0.7864\n",
      "Epoch 257/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3569 - acc: 0.8574 - val_loss: 0.4611 - val_acc: 0.7898\n",
      "Epoch 258/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3539 - acc: 0.8557 - val_loss: 0.4553 - val_acc: 0.7932\n",
      "Epoch 259/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3569 - acc: 0.8591 - val_loss: 0.4534 - val_acc: 0.7864\n",
      "Epoch 260/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3619 - acc: 0.8490 - val_loss: 0.4588 - val_acc: 0.7797\n",
      "Epoch 261/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3560 - acc: 0.8507 - val_loss: 0.4646 - val_acc: 0.7797\n",
      "Epoch 262/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3609 - acc: 0.8356 - val_loss: 0.4443 - val_acc: 0.8068\n",
      "Epoch 263/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3520 - acc: 0.8557 - val_loss: 0.4602 - val_acc: 0.7797\n",
      "Epoch 264/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3525 - acc: 0.8523 - val_loss: 0.4568 - val_acc: 0.8000\n",
      "Epoch 265/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3528 - acc: 0.8473 - val_loss: 0.4481 - val_acc: 0.8068\n",
      "Epoch 266/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3509 - acc: 0.8557 - val_loss: 0.4553 - val_acc: 0.7932\n",
      "Epoch 267/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3510 - acc: 0.8574 - val_loss: 0.4660 - val_acc: 0.7797\n",
      "Epoch 268/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3637 - acc: 0.8406 - val_loss: 0.4596 - val_acc: 0.8068\n",
      "Epoch 269/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3538 - acc: 0.8456 - val_loss: 0.4768 - val_acc: 0.7932\n",
      "Epoch 270/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3566 - acc: 0.8490 - val_loss: 0.4576 - val_acc: 0.7831\n",
      "Epoch 271/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3541 - acc: 0.8574 - val_loss: 0.4503 - val_acc: 0.8034\n",
      "Epoch 272/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3536 - acc: 0.8557 - val_loss: 0.4517 - val_acc: 0.8068\n",
      "Epoch 273/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3535 - acc: 0.8607 - val_loss: 0.4598 - val_acc: 0.7932\n",
      "Epoch 274/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3601 - acc: 0.8406 - val_loss: 0.4848 - val_acc: 0.7729\n",
      "Epoch 275/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3526 - acc: 0.8658 - val_loss: 0.4716 - val_acc: 0.7831\n",
      "Epoch 276/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3520 - acc: 0.8624 - val_loss: 0.4562 - val_acc: 0.8000\n",
      "Epoch 277/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3572 - acc: 0.8490 - val_loss: 0.4650 - val_acc: 0.8000\n",
      "Epoch 278/400\n",
      "596/596 [==============================] - 0s 44us/sample - loss: 0.3503 - acc: 0.8591 - val_loss: 0.4571 - val_acc: 0.8000\n",
      "Epoch 279/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3110 - acc: 0.880 - 0s 25us/sample - loss: 0.3543 - acc: 0.8507 - val_loss: 0.4615 - val_acc: 0.7932\n",
      "Epoch 280/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3531 - acc: 0.8574 - val_loss: 0.4598 - val_acc: 0.7966\n",
      "Epoch 281/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3658 - acc: 0.8322 - val_loss: 0.4571 - val_acc: 0.8068\n",
      "Epoch 282/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3524 - acc: 0.8523 - val_loss: 0.4852 - val_acc: 0.7898\n",
      "Epoch 283/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3534 - acc: 0.8523 - val_loss: 0.4741 - val_acc: 0.7763\n",
      "Epoch 284/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3554 - acc: 0.8389 - val_loss: 0.4728 - val_acc: 0.7797\n",
      "Epoch 285/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3595 - acc: 0.8456 - val_loss: 0.4654 - val_acc: 0.7864\n",
      "Epoch 286/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3615 - acc: 0.8490 - val_loss: 0.4602 - val_acc: 0.8000\n",
      "Epoch 287/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3496 - acc: 0.8607 - val_loss: 0.4781 - val_acc: 0.7695\n",
      "Epoch 288/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3435 - acc: 0.8540 - val_loss: 0.4682 - val_acc: 0.8000\n",
      "Epoch 289/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3485 - acc: 0.8540 - val_loss: 0.4693 - val_acc: 0.7898\n",
      "Epoch 290/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3470 - acc: 0.8574 - val_loss: 0.4629 - val_acc: 0.8034\n",
      "Epoch 291/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3446 - acc: 0.8557 - val_loss: 0.4656 - val_acc: 0.7932\n",
      "Epoch 292/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.3461 - acc: 0.8540 - val_loss: 0.4608 - val_acc: 0.8068\n",
      "Epoch 293/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3481 - acc: 0.8540 - val_loss: 0.4600 - val_acc: 0.7864\n",
      "Epoch 294/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3427 - acc: 0.8691 - val_loss: 0.4628 - val_acc: 0.8000\n",
      "Epoch 295/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3522 - acc: 0.8557 - val_loss: 0.4669 - val_acc: 0.7864\n",
      "Epoch 296/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3525 - acc: 0.8423 - val_loss: 0.5046 - val_acc: 0.7898\n",
      "Epoch 297/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3523 - acc: 0.8624 - val_loss: 0.4688 - val_acc: 0.7966\n",
      "Epoch 298/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3426 - acc: 0.8591 - val_loss: 0.4640 - val_acc: 0.8000\n",
      "Epoch 299/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3419 - acc: 0.8624 - val_loss: 0.4760 - val_acc: 0.7831\n",
      "Epoch 300/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3441 - acc: 0.8540 - val_loss: 0.4701 - val_acc: 0.7797\n",
      "Epoch 301/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3484 - acc: 0.8641 - val_loss: 0.4796 - val_acc: 0.7729\n",
      "Epoch 302/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3580 - acc: 0.8507 - val_loss: 0.4828 - val_acc: 0.7932\n",
      "Epoch 303/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3454 - acc: 0.8658 - val_loss: 0.4749 - val_acc: 0.7898\n",
      "Epoch 304/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3484 - acc: 0.8641 - val_loss: 0.4732 - val_acc: 0.7831\n",
      "Epoch 305/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3413 - acc: 0.8624 - val_loss: 0.4754 - val_acc: 0.7797\n",
      "Epoch 306/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3453 - acc: 0.8641 - val_loss: 0.4822 - val_acc: 0.7864\n",
      "Epoch 307/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3489 - acc: 0.8607 - val_loss: 0.4706 - val_acc: 0.7831\n",
      "Epoch 308/400\n",
      "596/596 [==============================] - 0s 17us/sample - loss: 0.3753 - acc: 0.8389 - val_loss: 0.4860 - val_acc: 0.7898\n",
      "Epoch 309/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3447 - acc: 0.8658 - val_loss: 0.4904 - val_acc: 0.7831\n",
      "Epoch 310/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3444 - acc: 0.8607 - val_loss: 0.4797 - val_acc: 0.7831\n",
      "Epoch 311/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3400 - acc: 0.8574 - val_loss: 0.4740 - val_acc: 0.8000\n",
      "Epoch 312/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3555 - acc: 0.8372 - val_loss: 0.4624 - val_acc: 0.8068\n",
      "Epoch 313/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3506 - acc: 0.8574 - val_loss: 0.4811 - val_acc: 0.7831\n",
      "Epoch 314/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3454 - acc: 0.8557 - val_loss: 0.4731 - val_acc: 0.7966\n",
      "Epoch 315/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3419 - acc: 0.8607 - val_loss: 0.4717 - val_acc: 0.8000\n",
      "Epoch 316/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3555 - acc: 0.8523 - val_loss: 0.4765 - val_acc: 0.7932\n",
      "Epoch 317/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3389 - acc: 0.8591 - val_loss: 0.4867 - val_acc: 0.7864\n",
      "Epoch 318/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3424 - acc: 0.8591 - val_loss: 0.4750 - val_acc: 0.7831\n",
      "Epoch 319/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3405 - acc: 0.8691 - val_loss: 0.4739 - val_acc: 0.7864\n",
      "Epoch 320/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3393 - acc: 0.8708 - val_loss: 0.4851 - val_acc: 0.7932\n",
      "Epoch 321/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3496 - acc: 0.8473 - val_loss: 0.4960 - val_acc: 0.7898\n",
      "Epoch 322/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3468 - acc: 0.8540 - val_loss: 0.4906 - val_acc: 0.7797\n",
      "Epoch 323/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3709 - acc: 0.8389 - val_loss: 0.4860 - val_acc: 0.7831\n",
      "Epoch 324/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3394 - acc: 0.8557 - val_loss: 0.4890 - val_acc: 0.7831\n",
      "Epoch 325/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3478 - acc: 0.8540 - val_loss: 0.4842 - val_acc: 0.7966\n",
      "Epoch 326/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3434 - acc: 0.8658 - val_loss: 0.4813 - val_acc: 0.7831\n",
      "Epoch 327/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3402 - acc: 0.8507 - val_loss: 0.4765 - val_acc: 0.8000\n",
      "Epoch 328/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3384 - acc: 0.8591 - val_loss: 0.4948 - val_acc: 0.7831\n",
      "Epoch 329/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3712 - acc: 0.8305 - val_loss: 0.5039 - val_acc: 0.7966\n",
      "Epoch 330/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3510 - acc: 0.8607 - val_loss: 0.4901 - val_acc: 0.7864\n",
      "Epoch 331/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3488 - acc: 0.8557 - val_loss: 0.4851 - val_acc: 0.8000\n",
      "Epoch 332/400\n",
      "596/596 [==============================] - 0s 40us/sample - loss: 0.3449 - acc: 0.8540 - val_loss: 0.4978 - val_acc: 0.7593\n",
      "Epoch 333/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3582 - acc: 0.8523 - val_loss: 0.4995 - val_acc: 0.7763\n",
      "Epoch 334/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3524 - acc: 0.8557 - val_loss: 0.4813 - val_acc: 0.7831\n",
      "Epoch 335/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3496 - acc: 0.8473 - val_loss: 0.4825 - val_acc: 0.7831\n",
      "Epoch 336/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3460 - acc: 0.8574 - val_loss: 0.4668 - val_acc: 0.8000\n",
      "Epoch 337/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3427 - acc: 0.8607 - val_loss: 0.4746 - val_acc: 0.7831\n",
      "Epoch 338/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3348 - acc: 0.8691 - val_loss: 0.4757 - val_acc: 0.7763\n",
      "Epoch 339/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3382 - acc: 0.8607 - val_loss: 0.4916 - val_acc: 0.7898\n",
      "Epoch 340/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3385 - acc: 0.8591 - val_loss: 0.5100 - val_acc: 0.7898\n",
      "Epoch 341/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3556 - acc: 0.8423 - val_loss: 0.4842 - val_acc: 0.7932\n",
      "Epoch 342/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3430 - acc: 0.8523 - val_loss: 0.4796 - val_acc: 0.7831\n",
      "Epoch 343/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3358 - acc: 0.8658 - val_loss: 0.4864 - val_acc: 0.7729\n",
      "Epoch 344/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3404 - acc: 0.8574 - val_loss: 0.4891 - val_acc: 0.7864\n",
      "Epoch 345/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3431 - acc: 0.8708 - val_loss: 0.4830 - val_acc: 0.8000\n",
      "Epoch 346/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3449 - acc: 0.8490 - val_loss: 0.4761 - val_acc: 0.8068\n",
      "Epoch 347/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3373 - acc: 0.8658 - val_loss: 0.4956 - val_acc: 0.7797\n",
      "Epoch 348/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3435 - acc: 0.8523 - val_loss: 0.4886 - val_acc: 0.7797\n",
      "Epoch 349/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3352 - acc: 0.8607 - val_loss: 0.4805 - val_acc: 0.7763\n",
      "Epoch 350/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3487 - acc: 0.8540 - val_loss: 0.4947 - val_acc: 0.7898\n",
      "Epoch 351/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3390 - acc: 0.8490 - val_loss: 0.4901 - val_acc: 0.7763\n",
      "Epoch 352/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3395 - acc: 0.8624 - val_loss: 0.4958 - val_acc: 0.7898\n",
      "Epoch 353/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3409 - acc: 0.8540 - val_loss: 0.5029 - val_acc: 0.7932\n",
      "Epoch 354/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3356 - acc: 0.8691 - val_loss: 0.5093 - val_acc: 0.7864\n",
      "Epoch 355/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3374 - acc: 0.8607 - val_loss: 0.4849 - val_acc: 0.7864\n",
      "Epoch 356/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3509 - acc: 0.8691 - val_loss: 0.4762 - val_acc: 0.8068\n",
      "Epoch 357/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3408 - acc: 0.8523 - val_loss: 0.4945 - val_acc: 0.8000\n",
      "Epoch 358/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3448 - acc: 0.8574 - val_loss: 0.4713 - val_acc: 0.7932\n",
      "Epoch 359/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3320 - acc: 0.870 - 0s 30us/sample - loss: 0.3353 - acc: 0.8624 - val_loss: 0.4895 - val_acc: 0.7966\n",
      "Epoch 360/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3361 - acc: 0.8540 - val_loss: 0.5081 - val_acc: 0.7797\n",
      "Epoch 361/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3379 - acc: 0.8490 - val_loss: 0.4893 - val_acc: 0.8000\n",
      "Epoch 362/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3379 - acc: 0.8607 - val_loss: 0.5047 - val_acc: 0.7831\n",
      "Epoch 363/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3395 - acc: 0.8641 - val_loss: 0.4874 - val_acc: 0.7898\n",
      "Epoch 364/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3324 - acc: 0.8624 - val_loss: 0.4856 - val_acc: 0.7831\n",
      "Epoch 365/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3422 - acc: 0.8624 - val_loss: 0.4892 - val_acc: 0.7932\n",
      "Epoch 366/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3485 - acc: 0.8473 - val_loss: 0.4862 - val_acc: 0.8102\n",
      "Epoch 367/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3428 - acc: 0.8540 - val_loss: 0.4808 - val_acc: 0.7932\n",
      "Epoch 368/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3375 - acc: 0.8624 - val_loss: 0.4778 - val_acc: 0.8068\n",
      "Epoch 369/400\n",
      "596/596 [==============================] - 0s 40us/sample - loss: 0.3320 - acc: 0.8708 - val_loss: 0.5014 - val_acc: 0.7763\n",
      "Epoch 370/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3381 - acc: 0.8574 - val_loss: 0.4888 - val_acc: 0.7898\n",
      "Epoch 371/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3327 - acc: 0.8607 - val_loss: 0.4865 - val_acc: 0.7831\n",
      "Epoch 372/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3328 - acc: 0.8708 - val_loss: 0.4981 - val_acc: 0.7898\n",
      "Epoch 373/400\n",
      "596/596 [==============================] - 0s 17us/sample - loss: 0.3338 - acc: 0.8658 - val_loss: 0.4819 - val_acc: 0.7864\n",
      "Epoch 374/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3372 - acc: 0.8523 - val_loss: 0.5005 - val_acc: 0.7966\n",
      "Epoch 375/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3343 - acc: 0.8574 - val_loss: 0.4845 - val_acc: 0.7831\n",
      "Epoch 376/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3535 - acc: 0.8473 - val_loss: 0.4887 - val_acc: 0.7797\n",
      "Epoch 377/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3388 - acc: 0.8658 - val_loss: 0.4821 - val_acc: 0.7831\n",
      "Epoch 378/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3396 - acc: 0.8557 - val_loss: 0.4848 - val_acc: 0.7932\n",
      "Epoch 379/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3353 - acc: 0.8641 - val_loss: 0.4921 - val_acc: 0.7966\n",
      "Epoch 380/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3394 - acc: 0.8624 - val_loss: 0.4918 - val_acc: 0.8068\n",
      "Epoch 381/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3322 - acc: 0.8758 - val_loss: 0.5019 - val_acc: 0.7831\n",
      "Epoch 382/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3323 - acc: 0.8624 - val_loss: 0.4888 - val_acc: 0.7831\n",
      "Epoch 383/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3440 - acc: 0.8624 - val_loss: 0.4847 - val_acc: 0.7661\n",
      "Epoch 384/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.2632 - acc: 0.930 - 0s 32us/sample - loss: 0.3412 - acc: 0.8624 - val_loss: 0.4897 - val_acc: 0.7627\n",
      "Epoch 385/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3312 - acc: 0.8658 - val_loss: 0.4988 - val_acc: 0.7831\n",
      "Epoch 386/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3339 - acc: 0.8725 - val_loss: 0.4976 - val_acc: 0.7763\n",
      "Epoch 387/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3274 - acc: 0.8658 - val_loss: 0.4907 - val_acc: 0.7932\n",
      "Epoch 388/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3486 - acc: 0.8423 - val_loss: 0.4976 - val_acc: 0.7898\n",
      "Epoch 389/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3355 - acc: 0.8674 - val_loss: 0.4804 - val_acc: 0.8068\n",
      "Epoch 390/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3334 - acc: 0.8708 - val_loss: 0.5642 - val_acc: 0.7695\n",
      "Epoch 391/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3506 - acc: 0.8523 - val_loss: 0.5720 - val_acc: 0.7797\n",
      "Epoch 392/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3606 - acc: 0.8557 - val_loss: 0.5063 - val_acc: 0.8102\n",
      "Epoch 393/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3389 - acc: 0.8641 - val_loss: 0.5144 - val_acc: 0.7864\n",
      "Epoch 394/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3472 - acc: 0.8507 - val_loss: 0.5002 - val_acc: 0.7898\n",
      "Epoch 395/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3352 - acc: 0.8607 - val_loss: 0.4987 - val_acc: 0.8000\n",
      "Epoch 396/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3399 - acc: 0.8540 - val_loss: 0.4911 - val_acc: 0.8000\n",
      "Epoch 397/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.2914 - acc: 0.890 - 0s 33us/sample - loss: 0.3337 - acc: 0.8591 - val_loss: 0.5319 - val_acc: 0.7797\n",
      "Epoch 398/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3431 - acc: 0.8574 - val_loss: 0.5149 - val_acc: 0.7763\n",
      "Epoch 399/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3349 - acc: 0.8607 - val_loss: 0.5025 - val_acc: 0.8000\n",
      "Epoch 400/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.3385 - acc: 0.8624 - val_loss: 0.4857 - val_acc: 0.8034\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "#label_train = encoder.fit_transform(y_train)\n",
    "label_test = encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(20, activation='relu', input_shape=(9,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = tf.train.GradientDescentOptimizer(0.1),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    model.fit(X_train, y_train, epochs=400, batch_size=100, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing Cost Estimator\n",
    "\n",
    "Building off the classifier examples above, this section shows ensemble regressors using bagging and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "from sklearn import datasets\n",
    "boston_housing_data = datasets.load_boston()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bouston_housing_data_instances = scaler.fit_transform(boston_housing_data.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,) (167, 13) (167,)\n",
      "0-Train: 202.22035217285156 Test:294.7690124511719\n",
      "1-Train: 89.31452941894531 Test:146.78369140625\n",
      "2-Train: 77.20492553710938 Test:125.2916030883789\n",
      "3-Train: 75.1042251586914 Test:119.77290344238281\n",
      "4-Train: 74.22882080078125 Test:117.44296264648438\n",
      "5-Train: 73.56881713867188 Test:116.1061019897461\n",
      "6-Train: 73.07276916503906 Test:115.19467163085938\n",
      "7-Train: 72.70021057128906 Test:114.51588439941406\n",
      "8-Train: 72.38704681396484 Test:113.9810791015625\n",
      "9-Train: 72.13630676269531 Test:113.55441284179688\n",
      "10-Train: 71.9155044555664 Test:113.18358612060547\n",
      "11-Train: 71.70333099365234 Test:112.83824920654297\n",
      "12-Train: 71.49002075195312 Test:112.5195083618164\n",
      "13-Train: 71.29476928710938 Test:112.23749542236328\n",
      "14-Train: 71.10044860839844 Test:111.96229553222656\n",
      "15-Train: 70.93395233154297 Test:111.70030975341797\n",
      "16-Train: 70.753173828125 Test:111.4442367553711\n",
      "17-Train: 70.631591796875 Test:111.23519897460938\n",
      "18-Train: 70.54571533203125 Test:111.05506134033203\n",
      "19-Train: 70.41896057128906 Test:110.84944152832031\n",
      "20-Train: 70.33995056152344 Test:110.6793212890625\n",
      "21-Train: 70.26090240478516 Test:110.51737213134766\n",
      "22-Train: 70.18046569824219 Test:110.36407470703125\n",
      "23-Train: 70.10319519042969 Test:110.22305297851562\n",
      "24-Train: 70.04659271240234 Test:110.10082244873047\n",
      "25-Train: 70.00260925292969 Test:109.99031829833984\n",
      "26-Train: 69.95266723632812 Test:109.88030242919922\n",
      "27-Train: 69.90447235107422 Test:109.77193450927734\n",
      "28-Train: 69.8674087524414 Test:109.68754577636719\n",
      "29-Train: 69.83245086669922 Test:109.60649108886719\n",
      "30-Train: 69.82889556884766 Test:109.5418472290039\n",
      "31-Train: 69.80777740478516 Test:109.47500610351562\n",
      "32-Train: 69.79676055908203 Test:109.41032409667969\n",
      "33-Train: 69.78157806396484 Test:109.35369110107422\n",
      "34-Train: 69.75936126708984 Test:109.28929138183594\n",
      "35-Train: 69.7359848022461 Test:109.2327880859375\n",
      "36-Train: 69.71733856201172 Test:109.18851470947266\n",
      "37-Train: 69.688720703125 Test:109.14173889160156\n",
      "38-Train: 69.68451690673828 Test:109.11019897460938\n",
      "39-Train: 69.66851806640625 Test:109.0821304321289\n",
      "40-Train: 69.67594146728516 Test:109.05083465576172\n",
      "41-Train: 69.65817260742188 Test:109.02571105957031\n",
      "42-Train: 69.6582260131836 Test:108.99365234375\n",
      "43-Train: 69.65483856201172 Test:108.97383117675781\n",
      "44-Train: 69.65167999267578 Test:108.94603729248047\n",
      "45-Train: 69.64998626708984 Test:108.93051147460938\n",
      "46-Train: 69.66268157958984 Test:108.91001892089844\n",
      "47-Train: 69.6544189453125 Test:108.89553833007812\n",
      "48-Train: 69.65914154052734 Test:108.86211395263672\n",
      "49-Train: 69.6620101928711 Test:108.85698699951172\n",
      "50-Train: 69.66291809082031 Test:108.85242462158203\n",
      "51-Train: 69.66291046142578 Test:108.84285736083984\n",
      "52-Train: 69.67955780029297 Test:108.83090209960938\n",
      "53-Train: 69.67670440673828 Test:108.8223648071289\n",
      "54-Train: 69.6837158203125 Test:108.81758880615234\n",
      "55-Train: 69.67620086669922 Test:108.81532287597656\n",
      "56-Train: 69.68196868896484 Test:108.8077163696289\n",
      "57-Train: 69.68485260009766 Test:108.80440521240234\n",
      "58-Train: 69.68914794921875 Test:108.79625701904297\n",
      "59-Train: 69.68729400634766 Test:108.79161834716797\n",
      "60-Train: 69.69197082519531 Test:108.78862762451172\n",
      "61-Train: 69.69970703125 Test:108.7901840209961\n",
      "62-Train: 69.688232421875 Test:108.787353515625\n",
      "63-Train: 69.69446563720703 Test:108.77983856201172\n",
      "64-Train: 69.69535064697266 Test:108.76151275634766\n",
      "65-Train: 69.69278717041016 Test:108.76588439941406\n",
      "66-Train: 69.69151306152344 Test:108.7509765625\n",
      "67-Train: 69.6960220336914 Test:108.7497787475586\n",
      "68-Train: 69.68814849853516 Test:108.75015258789062\n",
      "69-Train: 69.68824005126953 Test:108.7439193725586\n",
      "70-Train: 69.6873550415039 Test:108.72981262207031\n",
      "71-Train: 69.68556213378906 Test:108.72909545898438\n",
      "72-Train: 69.67830657958984 Test:108.72236633300781\n",
      "73-Train: 69.68119049072266 Test:108.72134399414062\n",
      "74-Train: 69.67628479003906 Test:108.70952606201172\n",
      "75-Train: 69.67549133300781 Test:108.71302032470703\n",
      "76-Train: 69.67529296875 Test:108.71187591552734\n",
      "77-Train: 69.66869354248047 Test:108.69674682617188\n",
      "78-Train: 69.67876434326172 Test:108.69881439208984\n",
      "79-Train: 69.68494415283203 Test:108.69049835205078\n",
      "80-Train: 69.68463897705078 Test:108.69279479980469\n",
      "81-Train: 69.69441986083984 Test:108.69020080566406\n",
      "82-Train: 69.67550659179688 Test:108.68482208251953\n",
      "83-Train: 69.68113708496094 Test:108.68263244628906\n",
      "84-Train: 69.65929412841797 Test:108.67915344238281\n",
      "85-Train: 69.66541290283203 Test:108.67247772216797\n",
      "86-Train: 69.6722183227539 Test:108.6697769165039\n",
      "87-Train: 69.66529083251953 Test:108.67338562011719\n",
      "88-Train: 69.65997314453125 Test:108.67251586914062\n",
      "89-Train: 69.65419006347656 Test:108.66724395751953\n",
      "90-Train: 69.64239501953125 Test:108.67334747314453\n",
      "91-Train: 69.64508819580078 Test:108.65262603759766\n",
      "92-Train: 69.64939880371094 Test:108.66478729248047\n",
      "93-Train: 69.6445541381836 Test:108.66726684570312\n",
      "94-Train: 69.64859771728516 Test:108.65762329101562\n",
      "95-Train: 69.63768768310547 Test:108.66387939453125\n",
      "96-Train: 69.64592742919922 Test:108.66320037841797\n",
      "97-Train: 69.64591979980469 Test:108.66898345947266\n",
      "98-Train: 69.63379669189453 Test:108.66191101074219\n",
      "99-Train: 69.64381408691406 Test:108.66045379638672\n",
      "100-Train: 69.63578796386719 Test:108.66626739501953\n",
      "101-Train: 69.63775634765625 Test:108.657958984375\n",
      "102-Train: 69.64057159423828 Test:108.65460968017578\n",
      "103-Train: 69.63919067382812 Test:108.65812683105469\n",
      "104-Train: 69.63845825195312 Test:108.65699768066406\n",
      "105-Train: 69.63317108154297 Test:108.65518951416016\n",
      "106-Train: 69.63409423828125 Test:108.64973449707031\n",
      "107-Train: 69.6280517578125 Test:108.65416717529297\n",
      "108-Train: 69.63214111328125 Test:108.65345764160156\n",
      "109-Train: 69.630859375 Test:108.64697265625\n",
      "110-Train: 69.62506866455078 Test:108.64749908447266\n",
      "111-Train: 69.62732696533203 Test:108.6402816772461\n",
      "112-Train: 69.62803649902344 Test:108.6327133178711\n",
      "113-Train: 69.62883758544922 Test:108.64269256591797\n",
      "114-Train: 69.62486267089844 Test:108.63946533203125\n",
      "115-Train: 69.63965606689453 Test:108.63301086425781\n",
      "116-Train: 69.64035034179688 Test:108.6321792602539\n",
      "117-Train: 69.64200592041016 Test:108.61918640136719\n",
      "118-Train: 69.65066528320312 Test:108.61288452148438\n",
      "119-Train: 69.64959716796875 Test:108.61360931396484\n",
      "120-Train: 69.65512084960938 Test:108.6143798828125\n",
      "121-Train: 69.65351104736328 Test:108.60800170898438\n",
      "122-Train: 69.64508056640625 Test:108.60627746582031\n",
      "123-Train: 69.64689636230469 Test:108.61206817626953\n",
      "124-Train: 69.6512451171875 Test:108.60136413574219\n",
      "125-Train: 69.63871002197266 Test:108.61158752441406\n",
      "126-Train: 69.63890075683594 Test:108.60767364501953\n",
      "127-Train: 69.64612579345703 Test:108.61414337158203\n",
      "128-Train: 69.63800048828125 Test:108.60440826416016\n",
      "129-Train: 69.6419677734375 Test:108.60248565673828\n",
      "130-Train: 69.64077758789062 Test:108.61302185058594\n",
      "131-Train: 69.65046691894531 Test:108.61466979980469\n",
      "132-Train: 69.63490295410156 Test:108.60873413085938\n",
      "133-Train: 69.6455078125 Test:108.61214447021484\n",
      "134-Train: 69.63565063476562 Test:108.61702728271484\n",
      "135-Train: 69.645263671875 Test:108.61186218261719\n",
      "136-Train: 69.63763427734375 Test:108.62153625488281\n",
      "137-Train: 69.64616394042969 Test:108.6158218383789\n",
      "138-Train: 69.64600372314453 Test:108.60865020751953\n",
      "139-Train: 69.65372467041016 Test:108.61113739013672\n",
      "140-Train: 69.64690399169922 Test:108.61537170410156\n",
      "141-Train: 69.65434265136719 Test:108.61125946044922\n",
      "142-Train: 69.64695739746094 Test:108.61441802978516\n",
      "143-Train: 69.65873718261719 Test:108.60662841796875\n",
      "144-Train: 69.66340637207031 Test:108.60697937011719\n",
      "145-Train: 69.65571594238281 Test:108.61617279052734\n",
      "146-Train: 69.66314697265625 Test:108.59864807128906\n",
      "147-Train: 69.67183685302734 Test:108.61299896240234\n",
      "148-Train: 69.6607437133789 Test:108.61103057861328\n",
      "149-Train: 69.66869354248047 Test:108.60395050048828\n",
      "150-Train: 69.66255950927734 Test:108.61266326904297\n",
      "151-Train: 69.66887664794922 Test:108.60355377197266\n",
      "152-Train: 69.67941284179688 Test:108.60468292236328\n",
      "153-Train: 69.67635345458984 Test:108.62381744384766\n",
      "154-Train: 69.67708587646484 Test:108.59910583496094\n",
      "155-Train: 69.68559265136719 Test:108.6096420288086\n",
      "156-Train: 69.67774963378906 Test:108.61790466308594\n",
      "157-Train: 69.68462371826172 Test:108.61585235595703\n",
      "158-Train: 69.69119262695312 Test:108.60623931884766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159-Train: 69.67720031738281 Test:108.61474609375\n",
      "160-Train: 69.69322967529297 Test:108.61048889160156\n",
      "161-Train: 69.69268798828125 Test:108.61351776123047\n",
      "162-Train: 69.6895980834961 Test:108.61801147460938\n",
      "163-Train: 69.68814849853516 Test:108.60235595703125\n",
      "164-Train: 69.69866943359375 Test:108.61370086669922\n",
      "165-Train: 69.6900634765625 Test:108.61006164550781\n",
      "166-Train: 69.69401550292969 Test:108.61486053466797\n",
      "167-Train: 69.705322265625 Test:108.5992660522461\n",
      "168-Train: 69.6942367553711 Test:108.61450958251953\n",
      "169-Train: 69.69859313964844 Test:108.61644744873047\n",
      "170-Train: 69.7023696899414 Test:108.61133575439453\n",
      "171-Train: 69.69644927978516 Test:108.60735321044922\n",
      "172-Train: 69.70138549804688 Test:108.61390686035156\n",
      "173-Train: 69.70396423339844 Test:108.60600280761719\n",
      "174-Train: 69.6936264038086 Test:108.61036682128906\n",
      "175-Train: 69.70879364013672 Test:108.60887145996094\n",
      "176-Train: 69.70238494873047 Test:108.61422729492188\n",
      "177-Train: 69.70930480957031 Test:108.60213470458984\n",
      "178-Train: 69.70303344726562 Test:108.61251831054688\n",
      "179-Train: 69.70115661621094 Test:108.6139144897461\n",
      "180-Train: 69.70647430419922 Test:108.60986328125\n",
      "181-Train: 69.70342254638672 Test:108.6169662475586\n",
      "182-Train: 69.71269989013672 Test:108.61375427246094\n",
      "183-Train: 69.7178726196289 Test:108.59693908691406\n",
      "184-Train: 69.71700286865234 Test:108.61436462402344\n",
      "185-Train: 69.72196960449219 Test:108.61602783203125\n",
      "186-Train: 69.71452331542969 Test:108.61506652832031\n",
      "187-Train: 69.71879577636719 Test:108.61396026611328\n",
      "188-Train: 69.73278045654297 Test:108.62052917480469\n",
      "189-Train: 69.7240219116211 Test:108.61590576171875\n",
      "190-Train: 69.71774291992188 Test:108.62364959716797\n",
      "191-Train: 69.73159790039062 Test:108.6067886352539\n",
      "192-Train: 69.72603607177734 Test:108.61870574951172\n",
      "193-Train: 69.72969818115234 Test:108.61727142333984\n",
      "194-Train: 69.7421646118164 Test:108.6195068359375\n",
      "195-Train: 69.73087310791016 Test:108.62134552001953\n",
      "196-Train: 69.74762725830078 Test:108.6089859008789\n",
      "197-Train: 69.74691009521484 Test:108.61463928222656\n",
      "198-Train: 69.75041198730469 Test:108.60887908935547\n",
      "199-Train: 69.74124145507812 Test:108.61619567871094\n",
      "200-Train: 69.74425506591797 Test:108.63729095458984\n",
      "201-Train: 69.74139404296875 Test:108.62139129638672\n",
      "202-Train: 69.75379943847656 Test:108.6180191040039\n",
      "203-Train: 69.75581359863281 Test:108.62232208251953\n",
      "204-Train: 69.739501953125 Test:108.62167358398438\n",
      "205-Train: 69.7554702758789 Test:108.63722229003906\n",
      "206-Train: 69.75627899169922 Test:108.6287612915039\n",
      "207-Train: 69.75509643554688 Test:108.622314453125\n",
      "208-Train: 69.76776123046875 Test:108.63310241699219\n",
      "209-Train: 69.75160217285156 Test:108.6302490234375\n",
      "210-Train: 69.74569702148438 Test:108.63479614257812\n",
      "211-Train: 69.74137115478516 Test:108.62946319580078\n",
      "212-Train: 69.76420593261719 Test:108.6387710571289\n",
      "213-Train: 69.75958251953125 Test:108.62763214111328\n",
      "214-Train: 69.76242065429688 Test:108.6279525756836\n",
      "215-Train: 69.75881958007812 Test:108.63566589355469\n",
      "216-Train: 69.76765441894531 Test:108.64128112792969\n",
      "217-Train: 69.7558822631836 Test:108.63751983642578\n",
      "218-Train: 69.7621841430664 Test:108.65636444091797\n",
      "219-Train: 69.75576782226562 Test:108.64151000976562\n",
      "220-Train: 69.75593566894531 Test:108.63507080078125\n",
      "221-Train: 69.761962890625 Test:108.65006256103516\n",
      "222-Train: 69.75836181640625 Test:108.65516662597656\n",
      "223-Train: 69.74935913085938 Test:108.64527130126953\n",
      "224-Train: 69.75193786621094 Test:108.65460968017578\n",
      "225-Train: 69.76336669921875 Test:108.66156005859375\n",
      "226-Train: 69.75975036621094 Test:108.65567016601562\n",
      "227-Train: 69.75801849365234 Test:108.65937805175781\n",
      "228-Train: 69.75975036621094 Test:108.68041229248047\n",
      "229-Train: 69.75944519042969 Test:108.66221618652344\n",
      "230-Train: 69.76360321044922 Test:108.67322540283203\n",
      "231-Train: 69.76701354980469 Test:108.68912506103516\n",
      "232-Train: 69.75167846679688 Test:108.68184661865234\n",
      "233-Train: 69.7574234008789 Test:108.68659973144531\n",
      "234-Train: 69.75462341308594 Test:108.68268585205078\n",
      "235-Train: 69.7693099975586 Test:108.69669342041016\n",
      "236-Train: 69.76276397705078 Test:108.68965911865234\n",
      "237-Train: 69.75814056396484 Test:108.68824005126953\n",
      "238-Train: 69.76277160644531 Test:108.71390533447266\n",
      "239-Train: 69.76528930664062 Test:108.69139862060547\n",
      "240-Train: 69.75640106201172 Test:108.6993408203125\n",
      "241-Train: 69.7651596069336 Test:108.71771240234375\n",
      "242-Train: 69.7592544555664 Test:108.69182586669922\n",
      "243-Train: 69.75846099853516 Test:108.70721435546875\n",
      "244-Train: 69.76715850830078 Test:108.72478485107422\n",
      "245-Train: 69.75870513916016 Test:108.70228576660156\n",
      "246-Train: 69.7593765258789 Test:108.71802520751953\n",
      "247-Train: 69.76378631591797 Test:108.72708129882812\n",
      "248-Train: 69.75529479980469 Test:108.72212982177734\n",
      "249-Train: 69.75536346435547 Test:108.72391510009766\n",
      "250-Train: 69.76813507080078 Test:108.7301254272461\n",
      "251-Train: 69.76090240478516 Test:108.73272705078125\n",
      "252-Train: 69.76763916015625 Test:108.74018096923828\n",
      "253-Train: 69.7538833618164 Test:108.7383804321289\n",
      "254-Train: 69.7502670288086 Test:108.74140167236328\n",
      "255-Train: 69.76031494140625 Test:108.75160217285156\n",
      "256-Train: 69.7530746459961 Test:108.74327087402344\n",
      "257-Train: 69.74799346923828 Test:108.75845336914062\n",
      "258-Train: 69.75071716308594 Test:108.74235534667969\n",
      "259-Train: 69.75628662109375 Test:108.74457550048828\n",
      "260-Train: 69.7561264038086 Test:108.77363586425781\n",
      "261-Train: 69.73645782470703 Test:108.75358581542969\n",
      "262-Train: 69.73540496826172 Test:108.75538635253906\n",
      "263-Train: 69.7389907836914 Test:108.7681884765625\n",
      "264-Train: 69.72957611083984 Test:108.76505279541016\n",
      "265-Train: 69.72501373291016 Test:108.7734375\n",
      "266-Train: 69.73857116699219 Test:108.7733154296875\n",
      "267-Train: 69.73806762695312 Test:108.7676773071289\n",
      "268-Train: 69.74299621582031 Test:108.78545379638672\n",
      "269-Train: 69.74073791503906 Test:108.77394104003906\n",
      "270-Train: 69.73753356933594 Test:108.79745483398438\n",
      "271-Train: 69.72903442382812 Test:108.79024505615234\n",
      "272-Train: 69.73551177978516 Test:108.7970199584961\n",
      "273-Train: 69.72562408447266 Test:108.7969970703125\n",
      "274-Train: 69.73829650878906 Test:108.81108856201172\n",
      "275-Train: 69.7305679321289 Test:108.81134033203125\n",
      "276-Train: 69.73389434814453 Test:108.8108901977539\n",
      "277-Train: 69.7357406616211 Test:108.80264282226562\n",
      "278-Train: 69.73380279541016 Test:108.83328247070312\n",
      "279-Train: 69.72579193115234 Test:108.81690216064453\n",
      "280-Train: 69.73954010009766 Test:108.8377914428711\n",
      "281-Train: 69.71662139892578 Test:108.82815551757812\n",
      "282-Train: 69.73197937011719 Test:108.85212707519531\n",
      "283-Train: 69.71936798095703 Test:108.826416015625\n",
      "284-Train: 69.7217788696289 Test:108.86068725585938\n",
      "285-Train: 69.7243881225586 Test:108.83422088623047\n",
      "286-Train: 69.72603607177734 Test:108.86014556884766\n",
      "287-Train: 69.72344970703125 Test:108.8528060913086\n",
      "288-Train: 69.71781158447266 Test:108.87680053710938\n",
      "289-Train: 69.71707916259766 Test:108.85427856445312\n",
      "290-Train: 69.71674346923828 Test:108.87857055664062\n",
      "291-Train: 69.71251678466797 Test:108.87369537353516\n",
      "292-Train: 69.71075439453125 Test:108.89160919189453\n",
      "293-Train: 69.7125473022461 Test:108.87686920166016\n",
      "294-Train: 69.70851135253906 Test:108.90116882324219\n",
      "295-Train: 69.70579528808594 Test:108.88612365722656\n",
      "296-Train: 69.70394134521484 Test:108.91413879394531\n",
      "297-Train: 69.72327423095703 Test:108.90709686279297\n",
      "298-Train: 69.6972427368164 Test:108.91016387939453\n",
      "299-Train: 69.70909881591797 Test:108.9157943725586\n",
      "300-Train: 69.70619201660156 Test:108.90170288085938\n",
      "301-Train: 69.6999282836914 Test:108.92669677734375\n",
      "302-Train: 69.69888305664062 Test:108.92897033691406\n",
      "303-Train: 69.69452667236328 Test:108.93909454345703\n",
      "304-Train: 69.70803833007812 Test:108.93931579589844\n",
      "305-Train: 69.70063781738281 Test:108.93537139892578\n",
      "306-Train: 69.69432067871094 Test:108.94782257080078\n",
      "307-Train: 69.6942367553711 Test:108.94512176513672\n",
      "308-Train: 69.69855499267578 Test:108.95962524414062\n",
      "309-Train: 69.6972885131836 Test:108.95874786376953\n",
      "310-Train: 69.7000503540039 Test:108.96751403808594\n",
      "311-Train: 69.69107055664062 Test:108.9813003540039\n",
      "312-Train: 69.69673156738281 Test:108.97171783447266\n",
      "313-Train: 69.69290161132812 Test:108.99076080322266\n",
      "314-Train: 69.69892883300781 Test:108.99888610839844\n",
      "315-Train: 69.68433380126953 Test:108.98180389404297\n",
      "316-Train: 69.69001007080078 Test:109.00919342041016\n",
      "317-Train: 69.68860626220703 Test:108.9993896484375\n",
      "318-Train: 69.69245910644531 Test:109.01899719238281\n",
      "319-Train: 69.68525695800781 Test:109.01358795166016\n",
      "320-Train: 69.6895980834961 Test:109.02664184570312\n",
      "321-Train: 69.6859359741211 Test:109.02527618408203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322-Train: 69.69769287109375 Test:109.02758026123047\n",
      "323-Train: 69.70519256591797 Test:109.05591583251953\n",
      "324-Train: 69.69580078125 Test:109.04165649414062\n",
      "325-Train: 69.7052993774414 Test:109.0633773803711\n",
      "326-Train: 69.69535827636719 Test:109.04940795898438\n",
      "327-Train: 69.70071411132812 Test:109.08019256591797\n",
      "328-Train: 69.71040344238281 Test:109.07901763916016\n",
      "329-Train: 69.69285583496094 Test:109.0782470703125\n",
      "330-Train: 69.69881439208984 Test:109.08158874511719\n",
      "331-Train: 69.70426940917969 Test:109.07660675048828\n",
      "332-Train: 69.70391082763672 Test:109.10965728759766\n",
      "333-Train: 69.70984649658203 Test:109.09734344482422\n",
      "334-Train: 69.6978759765625 Test:109.1030502319336\n",
      "335-Train: 69.7125244140625 Test:109.12173461914062\n",
      "336-Train: 69.69200897216797 Test:109.12173461914062\n",
      "337-Train: 69.7139663696289 Test:109.12718200683594\n",
      "338-Train: 69.70613861083984 Test:109.14491271972656\n",
      "339-Train: 69.7054443359375 Test:109.14014434814453\n",
      "340-Train: 69.70897674560547 Test:109.15225219726562\n",
      "341-Train: 69.71625518798828 Test:109.15946960449219\n",
      "342-Train: 69.70561218261719 Test:109.15070343017578\n",
      "343-Train: 69.71302795410156 Test:109.17610931396484\n",
      "344-Train: 69.70892333984375 Test:109.18524932861328\n",
      "345-Train: 69.70564270019531 Test:109.17362213134766\n",
      "346-Train: 69.7113265991211 Test:109.18191528320312\n",
      "347-Train: 69.70855712890625 Test:109.20230102539062\n",
      "348-Train: 69.69780731201172 Test:109.2020263671875\n",
      "349-Train: 69.71090698242188 Test:109.20410919189453\n",
      "350-Train: 69.71613311767578 Test:109.21906280517578\n",
      "351-Train: 69.70578002929688 Test:109.21572875976562\n",
      "352-Train: 69.70702362060547 Test:109.2384262084961\n",
      "353-Train: 69.70703125 Test:109.23507690429688\n",
      "354-Train: 69.70890808105469 Test:109.2267074584961\n",
      "355-Train: 69.705810546875 Test:109.24808502197266\n",
      "356-Train: 69.71210479736328 Test:109.26861572265625\n",
      "357-Train: 69.70626831054688 Test:109.24214172363281\n",
      "358-Train: 69.70951080322266 Test:109.2712631225586\n",
      "359-Train: 69.71315002441406 Test:109.27849578857422\n",
      "360-Train: 69.6954116821289 Test:109.28340911865234\n",
      "361-Train: 69.70268249511719 Test:109.30262756347656\n",
      "362-Train: 69.71217346191406 Test:109.29631042480469\n",
      "363-Train: 69.69416046142578 Test:109.2942886352539\n",
      "364-Train: 69.7124252319336 Test:109.32464599609375\n",
      "365-Train: 69.70681762695312 Test:109.32659912109375\n",
      "366-Train: 69.69725036621094 Test:109.32015991210938\n",
      "367-Train: 69.70354461669922 Test:109.35103607177734\n",
      "368-Train: 69.7049789428711 Test:109.34549713134766\n",
      "369-Train: 69.71741485595703 Test:109.34628295898438\n",
      "370-Train: 69.70079803466797 Test:109.35012817382812\n",
      "371-Train: 69.70104217529297 Test:109.378662109375\n",
      "372-Train: 69.70683288574219 Test:109.38423919677734\n",
      "373-Train: 69.70721435546875 Test:109.3787841796875\n",
      "374-Train: 69.7016830444336 Test:109.38445281982422\n",
      "375-Train: 69.6925277709961 Test:109.39563751220703\n",
      "376-Train: 69.6947250366211 Test:109.41027069091797\n",
      "377-Train: 69.69761657714844 Test:109.4121322631836\n",
      "378-Train: 69.70288848876953 Test:109.43292236328125\n",
      "379-Train: 69.70465087890625 Test:109.43294525146484\n",
      "380-Train: 69.69098663330078 Test:109.44060516357422\n",
      "381-Train: 69.68804168701172 Test:109.44374084472656\n",
      "382-Train: 69.69677734375 Test:109.46511840820312\n",
      "383-Train: 69.69630432128906 Test:109.45718383789062\n",
      "384-Train: 69.69330596923828 Test:109.47803497314453\n",
      "385-Train: 69.70384216308594 Test:109.50056457519531\n",
      "386-Train: 69.70193481445312 Test:109.50160217285156\n",
      "387-Train: 69.70549774169922 Test:109.50823974609375\n",
      "388-Train: 69.68753814697266 Test:109.50415802001953\n",
      "389-Train: 69.68645477294922 Test:109.5225601196289\n",
      "390-Train: 69.70687103271484 Test:109.54058837890625\n",
      "391-Train: 69.69891357421875 Test:109.54873657226562\n",
      "392-Train: 69.6783218383789 Test:109.53321838378906\n",
      "393-Train: 69.68743896484375 Test:109.56182861328125\n",
      "394-Train: 69.6871566772461 Test:109.56526947021484\n",
      "395-Train: 69.69973754882812 Test:109.5806655883789\n",
      "396-Train: 69.6874008178711 Test:109.59571838378906\n",
      "397-Train: 69.6855697631836 Test:109.60272979736328\n",
      "398-Train: 69.67398071289062 Test:109.60582733154297\n",
      "399-Train: 69.68285369873047 Test:109.6006851196289\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(bouston_housing_data_instances,\n",
    "                                                   boston_housing_data.target,\n",
    "                                                   test_size=0.33)\n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "def get_batch(X, iter, size):\n",
    "    return X[(iter*batch_size) : ((iter+1)*batch_size)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_features = train_X.shape[1]\n",
    "num_instances = train_y.shape[0]\n",
    "\n",
    "# Construction\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_features), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Boston-MLP\"):\n",
    "    hidden1 = tf.layers.dense(X, 10, name=\"Hidden-1\", activation = tf.nn.relu)\n",
    "    output = tf.layers.dense(hidden1, 1, name=\"Price\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.square(y-output))\n",
    "    \n",
    "with tf.name_scope(\"train\"): \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 400\n",
    "batch_size = 10\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for iteration in range(num_instances // batch_size + 1):\n",
    "            X_batch = get_batch(train_X, iteration, batch_size)\n",
    "            y_batch = get_batch(train_y, iteration, batch_size)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={X: X_batch,\n",
    "                                            y: y_batch})\n",
    "            \n",
    "        mse_train = loss.eval(feed_dict={X: X_batch,\n",
    "                                            y: y_batch})\n",
    "        mse_val = loss.eval(feed_dict={X:test_X, y: test_y})\n",
    "        \n",
    "        print(\"{}-Train: {} Test:{}\".format(epoch,\n",
    "                                           mse_train,\n",
    "                                           mse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example, you will see that the model converges early. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 339 samples, validate on 167 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x0000025CE9E0BE18>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n",
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x0000025CE9E0BE18>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "339/339 [==============================] - 0s 832us/sample - loss: 507.6292 - mean_squared_error: 507.6292 - val_loss: 312.0483 - val_mean_squared_error: 312.0484\n",
      "Epoch 2/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 157.9132 - mean_squared_error: 157.9132 - val_loss: 71.1901 - val_mean_squared_error: 71.1901\n",
      "Epoch 3/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 49.0046 - mean_squared_error: 49.0046 - val_loss: 39.3386 - val_mean_squared_error: 39.3386\n",
      "Epoch 4/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 32.4291 - mean_squared_error: 32.4291 - val_loss: 30.7759 - val_mean_squared_error: 30.7759\n",
      "Epoch 5/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 27.3918 - mean_squared_error: 27.3918 - val_loss: 27.5107 - val_mean_squared_error: 27.5107\n",
      "Epoch 6/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 24.7225 - mean_squared_error: 24.7225 - val_loss: 24.9626 - val_mean_squared_error: 24.9626\n",
      "Epoch 7/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 23.1381 - mean_squared_error: 23.1381 - val_loss: 23.4687 - val_mean_squared_error: 23.4687\n",
      "Epoch 8/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 21.7141 - mean_squared_error: 21.7141 - val_loss: 22.1466 - val_mean_squared_error: 22.1466\n",
      "Epoch 9/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 20.6940 - mean_squared_error: 20.6940 - val_loss: 20.8961 - val_mean_squared_error: 20.8961\n",
      "Epoch 10/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 19.9983 - mean_squared_error: 19.9983 - val_loss: 20.3985 - val_mean_squared_error: 20.3985\n",
      "Epoch 11/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 19.3223 - mean_squared_error: 19.3223 - val_loss: 20.0118 - val_mean_squared_error: 20.0118\n",
      "Epoch 12/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 18.6040 - mean_squared_error: 18.6040 - val_loss: 19.4045 - val_mean_squared_error: 19.4045\n",
      "Epoch 13/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 17.9346 - mean_squared_error: 17.9346 - val_loss: 18.5199 - val_mean_squared_error: 18.5199\n",
      "Epoch 14/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 17.6748 - mean_squared_error: 17.6748 - val_loss: 18.8736 - val_mean_squared_error: 18.8736\n",
      "Epoch 15/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 17.3822 - mean_squared_error: 17.3822 - val_loss: 17.9319 - val_mean_squared_error: 17.9319\n",
      "Epoch 16/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 16.8558 - mean_squared_error: 16.8558 - val_loss: 17.7814 - val_mean_squared_error: 17.7814\n",
      "Epoch 17/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 16.4917 - mean_squared_error: 16.4917 - val_loss: 17.1569 - val_mean_squared_error: 17.1569\n",
      "Epoch 18/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 16.0992 - mean_squared_error: 16.0992 - val_loss: 16.7262 - val_mean_squared_error: 16.7262\n",
      "Epoch 19/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 15.8336 - mean_squared_error: 15.8336 - val_loss: 16.3310 - val_mean_squared_error: 16.3310\n",
      "Epoch 20/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 15.6907 - mean_squared_error: 15.6907 - val_loss: 16.2633 - val_mean_squared_error: 16.2633\n",
      "Epoch 21/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 15.2696 - mean_squared_error: 15.2696 - val_loss: 16.5199 - val_mean_squared_error: 16.5199\n",
      "Epoch 22/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 14.9659 - mean_squared_error: 14.9659 - val_loss: 16.4678 - val_mean_squared_error: 16.4678\n",
      "Epoch 23/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 14.5475 - mean_squared_error: 14.5475 - val_loss: 15.7228 - val_mean_squared_error: 15.7228\n",
      "Epoch 24/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 14.2786 - mean_squared_error: 14.2786 - val_loss: 14.8891 - val_mean_squared_error: 14.8891\n",
      "Epoch 25/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 14.1120 - mean_squared_error: 14.1120 - val_loss: 14.5457 - val_mean_squared_error: 14.5457\n",
      "Epoch 26/400\n",
      "339/339 [==============================] - 0s 126us/sample - loss: 13.8764 - mean_squared_error: 13.8764 - val_loss: 14.8698 - val_mean_squared_error: 14.8698\n",
      "Epoch 27/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: 13.5240 - mean_squared_error: 13.5239 - val_loss: 14.6423 - val_mean_squared_error: 14.6423\n",
      "Epoch 28/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 13.1896 - mean_squared_error: 13.1896 - val_loss: 15.0360 - val_mean_squared_error: 15.0360\n",
      "Epoch 29/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 12.8464 - mean_squared_error: 12.8464 - val_loss: 14.3223 - val_mean_squared_error: 14.3223\n",
      "Epoch 30/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: 12.9387 - mean_squared_error: 12.9387 - val_loss: 13.7962 - val_mean_squared_error: 13.7962\n",
      "Epoch 31/400\n",
      "339/339 [==============================] - 0s 182us/sample - loss: 12.6624 - mean_squared_error: 12.6624 - val_loss: 13.8787 - val_mean_squared_error: 13.8787\n",
      "Epoch 32/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 12.5277 - mean_squared_error: 12.5277 - val_loss: 13.8606 - val_mean_squared_error: 13.8606\n",
      "Epoch 33/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 12.3266 - mean_squared_error: 12.3266 - val_loss: 14.1815 - val_mean_squared_error: 14.1815\n",
      "Epoch 34/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 12.2262 - mean_squared_error: 12.2262 - val_loss: 14.1842 - val_mean_squared_error: 14.1842\n",
      "Epoch 35/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 11.9706 - mean_squared_error: 11.9706 - val_loss: 14.0971 - val_mean_squared_error: 14.0971\n",
      "Epoch 36/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 12.0067 - mean_squared_error: 12.0067 - val_loss: 13.7157 - val_mean_squared_error: 13.7157\n",
      "Epoch 37/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: 11.8844 - mean_squared_error: 11.8844 - val_loss: 14.5691 - val_mean_squared_error: 14.5691\n",
      "Epoch 38/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 11.8080 - mean_squared_error: 11.8080 - val_loss: 14.9529 - val_mean_squared_error: 14.9529\n",
      "Epoch 39/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 11.6528 - mean_squared_error: 11.6528 - val_loss: 14.3992 - val_mean_squared_error: 14.3992\n",
      "Epoch 40/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: 11.7194 - mean_squared_error: 11.7194 - val_loss: 13.6975 - val_mean_squared_error: 13.6975\n",
      "Epoch 41/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 11.3647 - mean_squared_error: 11.3647 - val_loss: 15.1982 - val_mean_squared_error: 15.1982\n",
      "Epoch 42/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 11.4991 - mean_squared_error: 11.4991 - val_loss: 14.7516 - val_mean_squared_error: 14.7516\n",
      "Epoch 43/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 11.2490 - mean_squared_error: 11.2490 - val_loss: 14.1430 - val_mean_squared_error: 14.1430\n",
      "Epoch 44/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 11.2030 - mean_squared_error: 11.2030 - val_loss: 14.3135 - val_mean_squared_error: 14.3135\n",
      "Epoch 45/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 11.1247 - mean_squared_error: 11.1247 - val_loss: 14.2407 - val_mean_squared_error: 14.2407\n",
      "Epoch 46/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 11.0320 - mean_squared_error: 11.0320 - val_loss: 14.1886 - val_mean_squared_error: 14.1886\n",
      "Epoch 47/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 10.9723 - mean_squared_error: 10.9723 - val_loss: 14.0907 - val_mean_squared_error: 14.0907\n",
      "Epoch 48/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 10.6923 - mean_squared_error: 10.6923 - val_loss: 14.1347 - val_mean_squared_error: 14.1347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 10.6789 - mean_squared_error: 10.6789 - val_loss: 14.3287 - val_mean_squared_error: 14.3287\n",
      "Epoch 50/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 10.6451 - mean_squared_error: 10.6451 - val_loss: 14.2759 - val_mean_squared_error: 14.2759\n",
      "Epoch 51/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 10.6512 - mean_squared_error: 10.6512 - val_loss: 13.9540 - val_mean_squared_error: 13.9540\n",
      "Epoch 52/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 10.6853 - mean_squared_error: 10.6853 - val_loss: 13.9788 - val_mean_squared_error: 13.9788\n",
      "Epoch 53/400\n",
      "339/339 [==============================] - 0s 197us/sample - loss: 10.4073 - mean_squared_error: 10.4073 - val_loss: 14.0947 - val_mean_squared_error: 14.0947\n",
      "Epoch 54/400\n",
      "339/339 [==============================] - 0s 247us/sample - loss: 10.3699 - mean_squared_error: 10.3699 - val_loss: 14.5047 - val_mean_squared_error: 14.5047\n",
      "Epoch 55/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 10.3835 - mean_squared_error: 10.3835 - val_loss: 14.4863 - val_mean_squared_error: 14.4863\n",
      "Epoch 56/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 10.1976 - mean_squared_error: 10.1976 - val_loss: 15.0174 - val_mean_squared_error: 15.0174\n",
      "Epoch 57/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 10.3482 - mean_squared_error: 10.3482 - val_loss: 14.9421 - val_mean_squared_error: 14.9421\n",
      "Epoch 58/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 10.3946 - mean_squared_error: 10.3946 - val_loss: 14.4174 - val_mean_squared_error: 14.4174\n",
      "Epoch 59/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 10.0334 - mean_squared_error: 10.0334 - val_loss: 14.6632 - val_mean_squared_error: 14.6632\n",
      "Epoch 60/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 10.1028 - mean_squared_error: 10.1028 - val_loss: 14.5011 - val_mean_squared_error: 14.5011\n",
      "Epoch 61/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 10.1515 - mean_squared_error: 10.1515 - val_loss: 14.2435 - val_mean_squared_error: 14.2435\n",
      "Epoch 62/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 9.9864 - mean_squared_error: 9.9864 - val_loss: 14.7222 - val_mean_squared_error: 14.7222\n",
      "Epoch 63/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 10.0316 - mean_squared_error: 10.0316 - val_loss: 15.2292 - val_mean_squared_error: 15.2292\n",
      "Epoch 64/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 9.9180 - mean_squared_error: 9.9180 - val_loss: 15.1523 - val_mean_squared_error: 15.1523\n",
      "Epoch 65/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 9.7804 - mean_squared_error: 9.7804 - val_loss: 15.1718 - val_mean_squared_error: 15.1718\n",
      "Epoch 66/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 9.7988 - mean_squared_error: 9.7988 - val_loss: 14.4943 - val_mean_squared_error: 14.4943\n",
      "Epoch 67/400\n",
      "339/339 [==============================] - 0s 297us/sample - loss: 9.6874 - mean_squared_error: 9.6874 - val_loss: 14.9288 - val_mean_squared_error: 14.9288\n",
      "Epoch 68/400\n",
      "339/339 [==============================] - 0s 279us/sample - loss: 9.5800 - mean_squared_error: 9.5800 - val_loss: 14.4737 - val_mean_squared_error: 14.4737\n",
      "Epoch 69/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 9.7432 - mean_squared_error: 9.7432 - val_loss: 14.5417 - val_mean_squared_error: 14.5417\n",
      "Epoch 70/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 9.5241 - mean_squared_error: 9.5241 - val_loss: 14.1284 - val_mean_squared_error: 14.1284\n",
      "Epoch 71/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 9.6191 - mean_squared_error: 9.6191 - val_loss: 14.4736 - val_mean_squared_error: 14.4736\n",
      "Epoch 72/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 9.5401 - mean_squared_error: 9.5401 - val_loss: 15.2714 - val_mean_squared_error: 15.2714\n",
      "Epoch 73/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 9.3786 - mean_squared_error: 9.3786 - val_loss: 16.0308 - val_mean_squared_error: 16.0308\n",
      "Epoch 74/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 9.6787 - mean_squared_error: 9.6787 - val_loss: 15.1588 - val_mean_squared_error: 15.1588\n",
      "Epoch 75/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 9.5887 - mean_squared_error: 9.5887 - val_loss: 14.9809 - val_mean_squared_error: 14.9809\n",
      "Epoch 76/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 9.3962 - mean_squared_error: 9.3962 - val_loss: 14.5639 - val_mean_squared_error: 14.5639\n",
      "Epoch 77/400\n",
      "339/339 [==============================] - 0s 191us/sample - loss: 9.3648 - mean_squared_error: 9.3648 - val_loss: 14.5613 - val_mean_squared_error: 14.5613\n",
      "Epoch 78/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 9.3996 - mean_squared_error: 9.3996 - val_loss: 14.8950 - val_mean_squared_error: 14.8950\n",
      "Epoch 79/400\n",
      "339/339 [==============================] - 0s 203us/sample - loss: 9.2534 - mean_squared_error: 9.2534 - val_loss: 14.9535 - val_mean_squared_error: 14.9535\n",
      "Epoch 80/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 9.4156 - mean_squared_error: 9.4156 - val_loss: 15.1284 - val_mean_squared_error: 15.1284\n",
      "Epoch 81/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 9.3016 - mean_squared_error: 9.3016 - val_loss: 14.7797 - val_mean_squared_error: 14.7797\n",
      "Epoch 82/400\n",
      "339/339 [==============================] - ETA: 0s - loss: 9.5101 - mean_squared_error: 9.51 - 0s 179us/sample - loss: 9.3666 - mean_squared_error: 9.3666 - val_loss: 14.9396 - val_mean_squared_error: 14.9396\n",
      "Epoch 83/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 9.2413 - mean_squared_error: 9.2413 - val_loss: 15.0290 - val_mean_squared_error: 15.0290\n",
      "Epoch 84/400\n",
      "339/339 [==============================] - 0s 341us/sample - loss: 9.2269 - mean_squared_error: 9.2269 - val_loss: 14.5608 - val_mean_squared_error: 14.5608\n",
      "Epoch 85/400\n",
      "339/339 [==============================] - 0s 297us/sample - loss: 9.1166 - mean_squared_error: 9.1166 - val_loss: 14.5687 - val_mean_squared_error: 14.5687\n",
      "Epoch 86/400\n",
      "339/339 [==============================] - 0s 277us/sample - loss: 9.1986 - mean_squared_error: 9.1986 - val_loss: 14.2710 - val_mean_squared_error: 14.2710\n",
      "Epoch 87/400\n",
      "339/339 [==============================] - 0s 229us/sample - loss: 9.1052 - mean_squared_error: 9.1052 - val_loss: 14.4198 - val_mean_squared_error: 14.4198\n",
      "Epoch 88/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: 9.2707 - mean_squared_error: 9.2707 - val_loss: 14.5655 - val_mean_squared_error: 14.5655\n",
      "Epoch 89/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 9.1827 - mean_squared_error: 9.1827 - val_loss: 14.5806 - val_mean_squared_error: 14.5806\n",
      "Epoch 90/400\n",
      "339/339 [==============================] - 0s 191us/sample - loss: 9.0215 - mean_squared_error: 9.0215 - val_loss: 15.0135 - val_mean_squared_error: 15.0135\n",
      "Epoch 91/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 8.9816 - mean_squared_error: 8.9816 - val_loss: 14.8311 - val_mean_squared_error: 14.8311\n",
      "Epoch 92/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 9.0709 - mean_squared_error: 9.0709 - val_loss: 14.6905 - val_mean_squared_error: 14.6905\n",
      "Epoch 93/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 9.0201 - mean_squared_error: 9.0201 - val_loss: 14.7165 - val_mean_squared_error: 14.7165\n",
      "Epoch 94/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 8.9706 - mean_squared_error: 8.9706 - val_loss: 15.0610 - val_mean_squared_error: 15.0610\n",
      "Epoch 95/400\n",
      "339/339 [==============================] - 0s 271us/sample - loss: 8.8366 - mean_squared_error: 8.8366 - val_loss: 14.6793 - val_mean_squared_error: 14.6793\n",
      "Epoch 96/400\n",
      "339/339 [==============================] - 0s 279us/sample - loss: 8.9082 - mean_squared_error: 8.9082 - val_loss: 15.0848 - val_mean_squared_error: 15.0848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/400\n",
      "339/339 [==============================] - 0s 256us/sample - loss: 8.8071 - mean_squared_error: 8.8071 - val_loss: 15.4638 - val_mean_squared_error: 15.4638\n",
      "Epoch 98/400\n",
      "339/339 [==============================] - 0s 271us/sample - loss: 8.9782 - mean_squared_error: 8.9782 - val_loss: 14.3543 - val_mean_squared_error: 14.3543\n",
      "Epoch 99/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 9.0042 - mean_squared_error: 9.0042 - val_loss: 14.6682 - val_mean_squared_error: 14.6682\n",
      "Epoch 100/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 8.9110 - mean_squared_error: 8.9110 - val_loss: 14.9517 - val_mean_squared_error: 14.9516\n",
      "Epoch 101/400\n",
      "339/339 [==============================] - 0s 191us/sample - loss: 8.8626 - mean_squared_error: 8.8626 - val_loss: 14.8254 - val_mean_squared_error: 14.8254\n",
      "Epoch 102/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: 8.7286 - mean_squared_error: 8.7286 - val_loss: 15.0070 - val_mean_squared_error: 15.0070\n",
      "Epoch 103/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 8.8136 - mean_squared_error: 8.8136 - val_loss: 15.2604 - val_mean_squared_error: 15.2604\n",
      "Epoch 104/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 8.6773 - mean_squared_error: 8.6773 - val_loss: 15.3394 - val_mean_squared_error: 15.3394\n",
      "Epoch 105/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 8.8575 - mean_squared_error: 8.8575 - val_loss: 15.2292 - val_mean_squared_error: 15.2292\n",
      "Epoch 106/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 8.6551 - mean_squared_error: 8.6551 - val_loss: 14.7929 - val_mean_squared_error: 14.7929\n",
      "Epoch 107/400\n",
      "339/339 [==============================] - 0s 241us/sample - loss: 8.5424 - mean_squared_error: 8.5424 - val_loss: 14.9770 - val_mean_squared_error: 14.9770\n",
      "Epoch 108/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 8.5689 - mean_squared_error: 8.5689 - val_loss: 14.9765 - val_mean_squared_error: 14.9765\n",
      "Epoch 109/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 8.7778 - mean_squared_error: 8.7778 - val_loss: 15.1169 - val_mean_squared_error: 15.1169\n",
      "Epoch 110/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 8.5013 - mean_squared_error: 8.5013 - val_loss: 16.7513 - val_mean_squared_error: 16.7513\n",
      "Epoch 111/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 8.6472 - mean_squared_error: 8.6472 - val_loss: 15.2867 - val_mean_squared_error: 15.2867\n",
      "Epoch 112/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 8.6506 - mean_squared_error: 8.6506 - val_loss: 14.4349 - val_mean_squared_error: 14.4349\n",
      "Epoch 113/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 8.5783 - mean_squared_error: 8.5783 - val_loss: 15.2159 - val_mean_squared_error: 15.2159\n",
      "Epoch 114/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 8.6251 - mean_squared_error: 8.6251 - val_loss: 15.6288 - val_mean_squared_error: 15.6288\n",
      "Epoch 115/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 8.4983 - mean_squared_error: 8.4983 - val_loss: 14.7506 - val_mean_squared_error: 14.7506\n",
      "Epoch 116/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 8.5342 - mean_squared_error: 8.5342 - val_loss: 14.8699 - val_mean_squared_error: 14.8699\n",
      "Epoch 117/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 8.3105 - mean_squared_error: 8.3105 - val_loss: 14.7603 - val_mean_squared_error: 14.7603\n",
      "Epoch 118/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 8.4710 - mean_squared_error: 8.4710 - val_loss: 15.1598 - val_mean_squared_error: 15.1598\n",
      "Epoch 119/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 8.4636 - mean_squared_error: 8.4636 - val_loss: 14.9173 - val_mean_squared_error: 14.9173\n",
      "Epoch 120/400\n",
      "339/339 [==============================] - 0s 265us/sample - loss: 8.4493 - mean_squared_error: 8.4493 - val_loss: 14.7780 - val_mean_squared_error: 14.7780\n",
      "Epoch 121/400\n",
      "339/339 [==============================] - 0s 197us/sample - loss: 8.3718 - mean_squared_error: 8.3718 - val_loss: 15.3196 - val_mean_squared_error: 15.3195\n",
      "Epoch 122/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 8.4085 - mean_squared_error: 8.4085 - val_loss: 15.1449 - val_mean_squared_error: 15.1449\n",
      "Epoch 123/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 8.2679 - mean_squared_error: 8.2679 - val_loss: 15.2634 - val_mean_squared_error: 15.2634\n",
      "Epoch 124/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 8.2280 - mean_squared_error: 8.2280 - val_loss: 15.3737 - val_mean_squared_error: 15.3737\n",
      "Epoch 125/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: 8.3281 - mean_squared_error: 8.3281 - val_loss: 14.9636 - val_mean_squared_error: 14.9636\n",
      "Epoch 126/400\n",
      "339/339 [==============================] - 0s 182us/sample - loss: 8.1849 - mean_squared_error: 8.1849 - val_loss: 15.0768 - val_mean_squared_error: 15.0768\n",
      "Epoch 127/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 8.2494 - mean_squared_error: 8.2494 - val_loss: 15.7314 - val_mean_squared_error: 15.7314\n",
      "Epoch 128/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 8.0978 - mean_squared_error: 8.0978 - val_loss: 15.2570 - val_mean_squared_error: 15.2570\n",
      "Epoch 129/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 8.1140 - mean_squared_error: 8.1140 - val_loss: 15.6096 - val_mean_squared_error: 15.6096\n",
      "Epoch 130/400\n",
      "339/339 [==============================] - 0s 227us/sample - loss: 8.1577 - mean_squared_error: 8.1577 - val_loss: 15.9627 - val_mean_squared_error: 15.9627\n",
      "Epoch 131/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 8.2428 - mean_squared_error: 8.2428 - val_loss: 16.0640 - val_mean_squared_error: 16.0640\n",
      "Epoch 132/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 8.1208 - mean_squared_error: 8.1208 - val_loss: 15.3687 - val_mean_squared_error: 15.3687\n",
      "Epoch 133/400\n",
      "339/339 [==============================] - 0s 268us/sample - loss: 8.0691 - mean_squared_error: 8.0691 - val_loss: 15.2230 - val_mean_squared_error: 15.2230\n",
      "Epoch 134/400\n",
      "339/339 [==============================] - 0s 265us/sample - loss: 8.0768 - mean_squared_error: 8.0768 - val_loss: 15.2167 - val_mean_squared_error: 15.2167\n",
      "Epoch 135/400\n",
      "339/339 [==============================] - 0s 194us/sample - loss: 8.1501 - mean_squared_error: 8.1501 - val_loss: 14.9952 - val_mean_squared_error: 14.9952\n",
      "Epoch 136/400\n",
      "339/339 [==============================] - 0s 297us/sample - loss: 8.0570 - mean_squared_error: 8.0570 - val_loss: 15.3709 - val_mean_squared_error: 15.3709\n",
      "Epoch 137/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 8.1118 - mean_squared_error: 8.1118 - val_loss: 15.9492 - val_mean_squared_error: 15.9492\n",
      "Epoch 138/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 8.0113 - mean_squared_error: 8.0113 - val_loss: 15.1544 - val_mean_squared_error: 15.1545\n",
      "Epoch 139/400\n",
      "339/339 [==============================] - 0s 209us/sample - loss: 8.0254 - mean_squared_error: 8.0254 - val_loss: 15.8797 - val_mean_squared_error: 15.8797\n",
      "Epoch 140/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 8.0185 - mean_squared_error: 8.0185 - val_loss: 15.7720 - val_mean_squared_error: 15.7720\n",
      "Epoch 141/400\n",
      "339/339 [==============================] - 0s 244us/sample - loss: 7.9905 - mean_squared_error: 7.9905 - val_loss: 15.7016 - val_mean_squared_error: 15.7016\n",
      "Epoch 142/400\n",
      "339/339 [==============================] - 0s 241us/sample - loss: 7.8089 - mean_squared_error: 7.8089 - val_loss: 15.2134 - val_mean_squared_error: 15.2134\n",
      "Epoch 143/400\n",
      "339/339 [==============================] - 0s 282us/sample - loss: 7.9375 - mean_squared_error: 7.9374 - val_loss: 15.0769 - val_mean_squared_error: 15.0769\n",
      "Epoch 144/400\n",
      "339/339 [==============================] - 0s 247us/sample - loss: 7.7981 - mean_squared_error: 7.7981 - val_loss: 15.9768 - val_mean_squared_error: 15.9768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/400\n",
      "339/339 [==============================] - 0s 250us/sample - loss: 7.8938 - mean_squared_error: 7.8938 - val_loss: 15.2905 - val_mean_squared_error: 15.2905\n",
      "Epoch 146/400\n",
      "339/339 [==============================] - 0s 262us/sample - loss: 7.8621 - mean_squared_error: 7.8621 - val_loss: 15.1030 - val_mean_squared_error: 15.1030\n",
      "Epoch 147/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 7.8671 - mean_squared_error: 7.8671 - val_loss: 15.1286 - val_mean_squared_error: 15.1286\n",
      "Epoch 148/400\n",
      "339/339 [==============================] - 0s 171us/sample - loss: 7.7459 - mean_squared_error: 7.7459 - val_loss: 15.4399 - val_mean_squared_error: 15.4399\n",
      "Epoch 149/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 7.8455 - mean_squared_error: 7.8455 - val_loss: 15.6362 - val_mean_squared_error: 15.6362\n",
      "Epoch 150/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 7.8780 - mean_squared_error: 7.8780 - val_loss: 15.7615 - val_mean_squared_error: 15.7615\n",
      "Epoch 151/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 7.7735 - mean_squared_error: 7.7735 - val_loss: 15.7827 - val_mean_squared_error: 15.7827\n",
      "Epoch 152/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 7.7875 - mean_squared_error: 7.7875 - val_loss: 15.0820 - val_mean_squared_error: 15.0820\n",
      "Epoch 153/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 7.7678 - mean_squared_error: 7.7678 - val_loss: 15.6494 - val_mean_squared_error: 15.6494\n",
      "Epoch 154/400\n",
      "339/339 [==============================] - 0s 126us/sample - loss: 7.7345 - mean_squared_error: 7.7345 - val_loss: 15.3067 - val_mean_squared_error: 15.3067\n",
      "Epoch 155/400\n",
      "339/339 [==============================] - 0s 126us/sample - loss: 7.7615 - mean_squared_error: 7.7615 - val_loss: 15.1034 - val_mean_squared_error: 15.1034\n",
      "Epoch 156/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 7.6903 - mean_squared_error: 7.6903 - val_loss: 15.3040 - val_mean_squared_error: 15.3040\n",
      "Epoch 157/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 7.7599 - mean_squared_error: 7.7599 - val_loss: 15.6227 - val_mean_squared_error: 15.6227\n",
      "Epoch 158/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 7.5079 - mean_squared_error: 7.5079 - val_loss: 15.1497 - val_mean_squared_error: 15.1497\n",
      "Epoch 159/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 7.6265 - mean_squared_error: 7.6265 - val_loss: 15.8845 - val_mean_squared_error: 15.8845\n",
      "Epoch 160/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 7.5226 - mean_squared_error: 7.5226 - val_loss: 15.4566 - val_mean_squared_error: 15.4566\n",
      "Epoch 161/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 7.5049 - mean_squared_error: 7.5049 - val_loss: 14.7948 - val_mean_squared_error: 14.7948\n",
      "Epoch 162/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 7.6540 - mean_squared_error: 7.6540 - val_loss: 15.7480 - val_mean_squared_error: 15.7480\n",
      "Epoch 163/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 7.5729 - mean_squared_error: 7.5729 - val_loss: 15.3400 - val_mean_squared_error: 15.3400\n",
      "Epoch 164/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 7.5378 - mean_squared_error: 7.5378 - val_loss: 15.7621 - val_mean_squared_error: 15.7621\n",
      "Epoch 165/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 7.5853 - mean_squared_error: 7.5853 - val_loss: 15.7493 - val_mean_squared_error: 15.7493\n",
      "Epoch 166/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 7.4659 - mean_squared_error: 7.4659 - val_loss: 16.0021 - val_mean_squared_error: 16.0021\n",
      "Epoch 167/400\n",
      "339/339 [==============================] - 0s 259us/sample - loss: 7.5658 - mean_squared_error: 7.5658 - val_loss: 15.2821 - val_mean_squared_error: 15.2821\n",
      "Epoch 168/400\n",
      "339/339 [==============================] - 0s 241us/sample - loss: 7.4431 - mean_squared_error: 7.4431 - val_loss: 15.2955 - val_mean_squared_error: 15.2955\n",
      "Epoch 169/400\n",
      "339/339 [==============================] - 0s 277us/sample - loss: 7.4529 - mean_squared_error: 7.4529 - val_loss: 15.2882 - val_mean_squared_error: 15.2882\n",
      "Epoch 170/400\n",
      "339/339 [==============================] - 0s 235us/sample - loss: 7.3719 - mean_squared_error: 7.3719 - val_loss: 15.3916 - val_mean_squared_error: 15.3916\n",
      "Epoch 171/400\n",
      "339/339 [==============================] - 0s 224us/sample - loss: 7.4064 - mean_squared_error: 7.4064 - val_loss: 16.0582 - val_mean_squared_error: 16.0582\n",
      "Epoch 172/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 7.2998 - mean_squared_error: 7.2998 - val_loss: 15.9362 - val_mean_squared_error: 15.9362\n",
      "Epoch 173/400\n",
      "339/339 [==============================] - 0s 238us/sample - loss: 7.4159 - mean_squared_error: 7.4159 - val_loss: 15.2420 - val_mean_squared_error: 15.2420\n",
      "Epoch 174/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 7.3382 - mean_squared_error: 7.3382 - val_loss: 15.1892 - val_mean_squared_error: 15.1892\n",
      "Epoch 175/400\n",
      "339/339 [==============================] - 0s 253us/sample - loss: 7.2869 - mean_squared_error: 7.2869 - val_loss: 15.7083 - val_mean_squared_error: 15.7083\n",
      "Epoch 176/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 7.2832 - mean_squared_error: 7.2832 - val_loss: 15.4376 - val_mean_squared_error: 15.4376\n",
      "Epoch 177/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 7.2441 - mean_squared_error: 7.2441 - val_loss: 16.5129 - val_mean_squared_error: 16.5129\n",
      "Epoch 178/400\n",
      "339/339 [==============================] - 0s 244us/sample - loss: 7.2551 - mean_squared_error: 7.2551 - val_loss: 15.2383 - val_mean_squared_error: 15.2383\n",
      "Epoch 179/400\n",
      "339/339 [==============================] - 0s 259us/sample - loss: 7.2566 - mean_squared_error: 7.2566 - val_loss: 15.6560 - val_mean_squared_error: 15.6560\n",
      "Epoch 180/400\n",
      "339/339 [==============================] - 0s 250us/sample - loss: 7.2232 - mean_squared_error: 7.2232 - val_loss: 15.5721 - val_mean_squared_error: 15.5721\n",
      "Epoch 181/400\n",
      "339/339 [==============================] - 0s 250us/sample - loss: 7.1338 - mean_squared_error: 7.1338 - val_loss: 15.3135 - val_mean_squared_error: 15.3135\n",
      "Epoch 182/400\n",
      "339/339 [==============================] - 0s 212us/sample - loss: 7.2997 - mean_squared_error: 7.2997 - val_loss: 15.5407 - val_mean_squared_error: 15.5407\n",
      "Epoch 183/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 7.2594 - mean_squared_error: 7.2594 - val_loss: 16.0228 - val_mean_squared_error: 16.0228\n",
      "Epoch 184/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 7.2632 - mean_squared_error: 7.2632 - val_loss: 15.7188 - val_mean_squared_error: 15.7188\n",
      "Epoch 185/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 7.1445 - mean_squared_error: 7.1445 - val_loss: 16.0701 - val_mean_squared_error: 16.0701\n",
      "Epoch 186/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 7.1559 - mean_squared_error: 7.1559 - val_loss: 15.7265 - val_mean_squared_error: 15.7265\n",
      "Epoch 187/400\n",
      "339/339 [==============================] - 0s 268us/sample - loss: 7.1452 - mean_squared_error: 7.1452 - val_loss: 15.2084 - val_mean_squared_error: 15.2084\n",
      "Epoch 188/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: 7.1432 - mean_squared_error: 7.1432 - val_loss: 15.5496 - val_mean_squared_error: 15.5496\n",
      "Epoch 189/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 7.1046 - mean_squared_error: 7.1046 - val_loss: 15.3753 - val_mean_squared_error: 15.3753\n",
      "Epoch 190/400\n",
      "339/339 [==============================] - 0s 256us/sample - loss: 6.9988 - mean_squared_error: 6.9988 - val_loss: 15.6296 - val_mean_squared_error: 15.6296\n",
      "Epoch 191/400\n",
      "339/339 [==============================] - ETA: 0s - loss: 6.6947 - mean_squared_error: 6.69 - 0s 271us/sample - loss: 7.1345 - mean_squared_error: 7.1345 - val_loss: 15.2964 - val_mean_squared_error: 15.2964\n",
      "Epoch 192/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 174us/sample - loss: 7.0364 - mean_squared_error: 7.0364 - val_loss: 15.2249 - val_mean_squared_error: 15.2249\n",
      "Epoch 193/400\n",
      "339/339 [==============================] - 0s 262us/sample - loss: 7.1458 - mean_squared_error: 7.1458 - val_loss: 15.3979 - val_mean_squared_error: 15.3979\n",
      "Epoch 194/400\n",
      "339/339 [==============================] - 0s 221us/sample - loss: 6.9578 - mean_squared_error: 6.9578 - val_loss: 15.4545 - val_mean_squared_error: 15.4545\n",
      "Epoch 195/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 7.0647 - mean_squared_error: 7.0647 - val_loss: 15.0985 - val_mean_squared_error: 15.0985\n",
      "Epoch 196/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 7.0419 - mean_squared_error: 7.0419 - val_loss: 15.9515 - val_mean_squared_error: 15.9515\n",
      "Epoch 197/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 6.9213 - mean_squared_error: 6.9213 - val_loss: 15.8552 - val_mean_squared_error: 15.8552\n",
      "Epoch 198/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 6.9898 - mean_squared_error: 6.9898 - val_loss: 15.3790 - val_mean_squared_error: 15.3790\n",
      "Epoch 199/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 6.9815 - mean_squared_error: 6.9815 - val_loss: 15.7219 - val_mean_squared_error: 15.7219\n",
      "Epoch 200/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 6.9533 - mean_squared_error: 6.9533 - val_loss: 15.4547 - val_mean_squared_error: 15.4547\n",
      "Epoch 201/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 6.9288 - mean_squared_error: 6.9288 - val_loss: 15.4690 - val_mean_squared_error: 15.4690\n",
      "Epoch 202/400\n",
      "339/339 [==============================] - 0s 194us/sample - loss: 6.9418 - mean_squared_error: 6.9418 - val_loss: 15.2575 - val_mean_squared_error: 15.2575\n",
      "Epoch 203/400\n",
      "339/339 [==============================] - 0s 250us/sample - loss: 6.9058 - mean_squared_error: 6.9058 - val_loss: 15.4654 - val_mean_squared_error: 15.4654\n",
      "Epoch 204/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 6.8640 - mean_squared_error: 6.8640 - val_loss: 15.5510 - val_mean_squared_error: 15.5510\n",
      "Epoch 205/400\n",
      "339/339 [==============================] - 0s 197us/sample - loss: 6.9953 - mean_squared_error: 6.9953 - val_loss: 15.3657 - val_mean_squared_error: 15.3657\n",
      "Epoch 206/400\n",
      "339/339 [==============================] - 0s 268us/sample - loss: 6.9098 - mean_squared_error: 6.9098 - val_loss: 15.5713 - val_mean_squared_error: 15.5713\n",
      "Epoch 207/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 6.8654 - mean_squared_error: 6.8654 - val_loss: 16.1892 - val_mean_squared_error: 16.1892\n",
      "Epoch 208/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 6.8303 - mean_squared_error: 6.8303 - val_loss: 15.2436 - val_mean_squared_error: 15.2436\n",
      "Epoch 209/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: 6.8302 - mean_squared_error: 6.8302 - val_loss: 15.2599 - val_mean_squared_error: 15.2599\n",
      "Epoch 210/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 6.9516 - mean_squared_error: 6.9516 - val_loss: 15.8402 - val_mean_squared_error: 15.8402\n",
      "Epoch 211/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 6.8239 - mean_squared_error: 6.8239 - val_loss: 15.3381 - val_mean_squared_error: 15.3381\n",
      "Epoch 212/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 6.8529 - mean_squared_error: 6.8529 - val_loss: 15.0552 - val_mean_squared_error: 15.0552\n",
      "Epoch 213/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 6.8336 - mean_squared_error: 6.8336 - val_loss: 15.7017 - val_mean_squared_error: 15.7017\n",
      "Epoch 214/400\n",
      "339/339 [==============================] - 0s 206us/sample - loss: 6.8210 - mean_squared_error: 6.8210 - val_loss: 16.1249 - val_mean_squared_error: 16.1249\n",
      "Epoch 215/400\n",
      "339/339 [==============================] - 0s 241us/sample - loss: 6.8184 - mean_squared_error: 6.8184 - val_loss: 15.3683 - val_mean_squared_error: 15.3683\n",
      "Epoch 216/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 6.7665 - mean_squared_error: 6.7665 - val_loss: 15.5788 - val_mean_squared_error: 15.5788\n",
      "Epoch 217/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 6.9371 - mean_squared_error: 6.9371 - val_loss: 15.9453 - val_mean_squared_error: 15.9453\n",
      "Epoch 218/400\n",
      "339/339 [==============================] - 0s 171us/sample - loss: 6.7509 - mean_squared_error: 6.7509 - val_loss: 15.7216 - val_mean_squared_error: 15.7216\n",
      "Epoch 219/400\n",
      "339/339 [==============================] - 0s 232us/sample - loss: 6.7774 - mean_squared_error: 6.7775 - val_loss: 15.5249 - val_mean_squared_error: 15.5249\n",
      "Epoch 220/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 6.7557 - mean_squared_error: 6.7557 - val_loss: 15.0599 - val_mean_squared_error: 15.0599\n",
      "Epoch 221/400\n",
      "339/339 [==============================] - 0s 182us/sample - loss: 6.7743 - mean_squared_error: 6.7743 - val_loss: 15.9518 - val_mean_squared_error: 15.9518\n",
      "Epoch 222/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 6.6403 - mean_squared_error: 6.6403 - val_loss: 15.2794 - val_mean_squared_error: 15.2794\n",
      "Epoch 223/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: 6.7751 - mean_squared_error: 6.7751 - val_loss: 16.4749 - val_mean_squared_error: 16.4749\n",
      "Epoch 224/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 6.7311 - mean_squared_error: 6.7311 - val_loss: 15.5764 - val_mean_squared_error: 15.5764\n",
      "Epoch 225/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 6.7192 - mean_squared_error: 6.7192 - val_loss: 15.7868 - val_mean_squared_error: 15.7868\n",
      "Epoch 226/400\n",
      "339/339 [==============================] - 0s 279us/sample - loss: 6.7116 - mean_squared_error: 6.7116 - val_loss: 15.0444 - val_mean_squared_error: 15.0444\n",
      "Epoch 227/400\n",
      "339/339 [==============================] - 0s 250us/sample - loss: 6.7676 - mean_squared_error: 6.7676 - val_loss: 15.2434 - val_mean_squared_error: 15.2434\n",
      "Epoch 228/400\n",
      "339/339 [==============================] - 0s 203us/sample - loss: 6.6749 - mean_squared_error: 6.6749 - val_loss: 15.7388 - val_mean_squared_error: 15.7388\n",
      "Epoch 229/400\n",
      "339/339 [==============================] - 0s 238us/sample - loss: 6.6397 - mean_squared_error: 6.6397 - val_loss: 16.1679 - val_mean_squared_error: 16.1679\n",
      "Epoch 230/400\n",
      "339/339 [==============================] - 0s 232us/sample - loss: 6.6041 - mean_squared_error: 6.6041 - val_loss: 15.7021 - val_mean_squared_error: 15.7021\n",
      "Epoch 231/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 6.6852 - mean_squared_error: 6.6852 - val_loss: 15.2328 - val_mean_squared_error: 15.2328\n",
      "Epoch 232/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 6.5551 - mean_squared_error: 6.5551 - val_loss: 15.2970 - val_mean_squared_error: 15.2970\n",
      "Epoch 233/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 6.6997 - mean_squared_error: 6.6997 - val_loss: 15.6458 - val_mean_squared_error: 15.6458\n",
      "Epoch 234/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 6.6230 - mean_squared_error: 6.6230 - val_loss: 16.0227 - val_mean_squared_error: 16.0227\n",
      "Epoch 235/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 6.4829 - mean_squared_error: 6.4829 - val_loss: 15.4283 - val_mean_squared_error: 15.4283\n",
      "Epoch 236/400\n",
      "339/339 [==============================] - 0s 232us/sample - loss: 6.6509 - mean_squared_error: 6.6509 - val_loss: 15.5113 - val_mean_squared_error: 15.5113\n",
      "Epoch 237/400\n",
      "339/339 [==============================] - 0s 224us/sample - loss: 6.5989 - mean_squared_error: 6.5989 - val_loss: 15.3818 - val_mean_squared_error: 15.3818\n",
      "Epoch 238/400\n",
      "339/339 [==============================] - 0s 274us/sample - loss: 6.6103 - mean_squared_error: 6.6103 - val_loss: 16.4617 - val_mean_squared_error: 16.4617\n",
      "Epoch 239/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 6.5751 - mean_squared_error: 6.5751 - val_loss: 15.7392 - val_mean_squared_error: 15.7392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/400\n",
      "339/339 [==============================] - 0s 247us/sample - loss: 6.4864 - mean_squared_error: 6.4864 - val_loss: 16.3365 - val_mean_squared_error: 16.3365\n",
      "Epoch 241/400\n",
      "339/339 [==============================] - 0s 294us/sample - loss: 6.5665 - mean_squared_error: 6.5665 - val_loss: 16.6527 - val_mean_squared_error: 16.6527\n",
      "Epoch 242/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 6.5303 - mean_squared_error: 6.5303 - val_loss: 16.4519 - val_mean_squared_error: 16.4519\n",
      "Epoch 243/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 6.4707 - mean_squared_error: 6.4707 - val_loss: 16.0724 - val_mean_squared_error: 16.0724\n",
      "Epoch 244/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 6.3673 - mean_squared_error: 6.3673 - val_loss: 15.5737 - val_mean_squared_error: 15.5737\n",
      "Epoch 245/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 6.4870 - mean_squared_error: 6.4870 - val_loss: 15.5661 - val_mean_squared_error: 15.5661\n",
      "Epoch 246/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 6.5807 - mean_squared_error: 6.5807 - val_loss: 15.6635 - val_mean_squared_error: 15.6635\n",
      "Epoch 247/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 6.5455 - mean_squared_error: 6.5455 - val_loss: 15.7098 - val_mean_squared_error: 15.7098\n",
      "Epoch 248/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 6.5284 - mean_squared_error: 6.5284 - val_loss: 15.8551 - val_mean_squared_error: 15.8551\n",
      "Epoch 249/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 6.4129 - mean_squared_error: 6.4129 - val_loss: 15.5956 - val_mean_squared_error: 15.5956\n",
      "Epoch 250/400\n",
      "339/339 [==============================] - 0s 282us/sample - loss: 6.2882 - mean_squared_error: 6.2882 - val_loss: 16.5709 - val_mean_squared_error: 16.5709\n",
      "Epoch 251/400\n",
      "339/339 [==============================] - 0s 230us/sample - loss: 6.5717 - mean_squared_error: 6.5717 - val_loss: 15.6403 - val_mean_squared_error: 15.6403\n",
      "Epoch 252/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 6.3913 - mean_squared_error: 6.3913 - val_loss: 15.4101 - val_mean_squared_error: 15.4101\n",
      "Epoch 253/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 6.2751 - mean_squared_error: 6.2751 - val_loss: 17.1745 - val_mean_squared_error: 17.1745\n",
      "Epoch 254/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 6.4355 - mean_squared_error: 6.4355 - val_loss: 15.3762 - val_mean_squared_error: 15.3762\n",
      "Epoch 255/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 6.3312 - mean_squared_error: 6.3312 - val_loss: 16.3214 - val_mean_squared_error: 16.3214\n",
      "Epoch 256/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 6.4327 - mean_squared_error: 6.4327 - val_loss: 15.6123 - val_mean_squared_error: 15.6123\n",
      "Epoch 257/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 6.3595 - mean_squared_error: 6.3595 - val_loss: 15.8452 - val_mean_squared_error: 15.8452\n",
      "Epoch 258/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 6.2986 - mean_squared_error: 6.2986 - val_loss: 15.4697 - val_mean_squared_error: 15.4697\n",
      "Epoch 259/400\n",
      "339/339 [==============================] - 0s 126us/sample - loss: 6.3426 - mean_squared_error: 6.3426 - val_loss: 15.7246 - val_mean_squared_error: 15.7246\n",
      "Epoch 260/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 6.2085 - mean_squared_error: 6.2085 - val_loss: 15.6877 - val_mean_squared_error: 15.6877\n",
      "Epoch 261/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 6.2333 - mean_squared_error: 6.2333 - val_loss: 15.4779 - val_mean_squared_error: 15.4779\n",
      "Epoch 262/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 6.3357 - mean_squared_error: 6.3357 - val_loss: 15.8398 - val_mean_squared_error: 15.8398\n",
      "Epoch 263/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 6.2945 - mean_squared_error: 6.2945 - val_loss: 15.8564 - val_mean_squared_error: 15.8564\n",
      "Epoch 264/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 6.2218 - mean_squared_error: 6.2218 - val_loss: 15.6783 - val_mean_squared_error: 15.6783\n",
      "Epoch 265/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 6.2814 - mean_squared_error: 6.2814 - val_loss: 15.9944 - val_mean_squared_error: 15.9944\n",
      "Epoch 266/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 6.1550 - mean_squared_error: 6.1550 - val_loss: 15.9642 - val_mean_squared_error: 15.9642\n",
      "Epoch 267/400\n",
      "339/339 [==============================] - 0s 262us/sample - loss: 6.1244 - mean_squared_error: 6.1244 - val_loss: 17.4593 - val_mean_squared_error: 17.4593\n",
      "Epoch 268/400\n",
      "339/339 [==============================] - 0s 218us/sample - loss: 6.2387 - mean_squared_error: 6.2387 - val_loss: 16.0627 - val_mean_squared_error: 16.0627\n",
      "Epoch 269/400\n",
      "339/339 [==============================] - 0s 218us/sample - loss: 6.2077 - mean_squared_error: 6.2077 - val_loss: 16.0170 - val_mean_squared_error: 16.0170\n",
      "Epoch 270/400\n",
      "339/339 [==============================] - 0s 279us/sample - loss: 6.1911 - mean_squared_error: 6.1911 - val_loss: 16.3280 - val_mean_squared_error: 16.3280\n",
      "Epoch 271/400\n",
      "339/339 [==============================] - 0s 194us/sample - loss: 6.1957 - mean_squared_error: 6.1957 - val_loss: 16.1688 - val_mean_squared_error: 16.1688\n",
      "Epoch 272/400\n",
      "339/339 [==============================] - 0s 253us/sample - loss: 6.2161 - mean_squared_error: 6.2161 - val_loss: 15.4410 - val_mean_squared_error: 15.4410\n",
      "Epoch 273/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 6.2286 - mean_squared_error: 6.2286 - val_loss: 15.7718 - val_mean_squared_error: 15.7718\n",
      "Epoch 274/400\n",
      "339/339 [==============================] - 0s 250us/sample - loss: 6.0880 - mean_squared_error: 6.0880 - val_loss: 17.5785 - val_mean_squared_error: 17.5785\n",
      "Epoch 275/400\n",
      "339/339 [==============================] - 0s 215us/sample - loss: 6.2380 - mean_squared_error: 6.2380 - val_loss: 16.0876 - val_mean_squared_error: 16.0876\n",
      "Epoch 276/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 6.1884 - mean_squared_error: 6.1884 - val_loss: 15.9752 - val_mean_squared_error: 15.9752\n",
      "Epoch 277/400\n",
      "339/339 [==============================] - 0s 241us/sample - loss: 6.2847 - mean_squared_error: 6.2847 - val_loss: 15.8220 - val_mean_squared_error: 15.8220\n",
      "Epoch 278/400\n",
      "339/339 [==============================] - 0s 253us/sample - loss: 6.1370 - mean_squared_error: 6.1370 - val_loss: 15.8037 - val_mean_squared_error: 15.8037\n",
      "Epoch 279/400\n",
      "339/339 [==============================] - 0s 244us/sample - loss: 6.1963 - mean_squared_error: 6.1963 - val_loss: 16.1459 - val_mean_squared_error: 16.1459\n",
      "Epoch 280/400\n",
      "339/339 [==============================] - 0s 244us/sample - loss: 6.1506 - mean_squared_error: 6.1506 - val_loss: 15.5010 - val_mean_squared_error: 15.5010\n",
      "Epoch 281/400\n",
      "339/339 [==============================] - 0s 221us/sample - loss: 6.1401 - mean_squared_error: 6.1401 - val_loss: 15.7577 - val_mean_squared_error: 15.7577\n",
      "Epoch 282/400\n",
      "339/339 [==============================] - 0s 265us/sample - loss: 6.2049 - mean_squared_error: 6.2049 - val_loss: 15.3748 - val_mean_squared_error: 15.3748\n",
      "Epoch 283/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 6.1435 - mean_squared_error: 6.1435 - val_loss: 15.3421 - val_mean_squared_error: 15.3421\n",
      "Epoch 284/400\n",
      "339/339 [==============================] - 0s 227us/sample - loss: 6.1511 - mean_squared_error: 6.1511 - val_loss: 15.8865 - val_mean_squared_error: 15.8865\n",
      "Epoch 285/400\n",
      "339/339 [==============================] - 0s 215us/sample - loss: 6.1381 - mean_squared_error: 6.1381 - val_loss: 15.6962 - val_mean_squared_error: 15.6962\n",
      "Epoch 286/400\n",
      "339/339 [==============================] - 0s 277us/sample - loss: 6.1365 - mean_squared_error: 6.1365 - val_loss: 15.7763 - val_mean_squared_error: 15.7763\n",
      "Epoch 287/400\n",
      "339/339 [==============================] - 0s 241us/sample - loss: 6.1115 - mean_squared_error: 6.1115 - val_loss: 16.2016 - val_mean_squared_error: 16.2016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 6.1723 - mean_squared_error: 6.1723 - val_loss: 15.9058 - val_mean_squared_error: 15.9058\n",
      "Epoch 289/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 6.1109 - mean_squared_error: 6.1109 - val_loss: 15.6573 - val_mean_squared_error: 15.6573\n",
      "Epoch 290/400\n",
      "339/339 [==============================] - 0s 215us/sample - loss: 6.1362 - mean_squared_error: 6.1362 - val_loss: 16.2043 - val_mean_squared_error: 16.2043\n",
      "Epoch 291/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 6.0889 - mean_squared_error: 6.0889 - val_loss: 15.2511 - val_mean_squared_error: 15.2511\n",
      "Epoch 292/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 6.0848 - mean_squared_error: 6.0848 - val_loss: 15.7082 - val_mean_squared_error: 15.7082\n",
      "Epoch 293/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 6.0713 - mean_squared_error: 6.0713 - val_loss: 15.9258 - val_mean_squared_error: 15.9258\n",
      "Epoch 294/400\n",
      "339/339 [==============================] - 0s 212us/sample - loss: 6.0827 - mean_squared_error: 6.0827 - val_loss: 16.1484 - val_mean_squared_error: 16.1484\n",
      "Epoch 295/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: 6.0667 - mean_squared_error: 6.0667 - val_loss: 15.5054 - val_mean_squared_error: 15.5054\n",
      "Epoch 296/400\n",
      "339/339 [==============================] - 0s 197us/sample - loss: 6.0698 - mean_squared_error: 6.0698 - val_loss: 15.4080 - val_mean_squared_error: 15.4080\n",
      "Epoch 297/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 6.1678 - mean_squared_error: 6.1678 - val_loss: 15.7606 - val_mean_squared_error: 15.7606\n",
      "Epoch 298/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 6.0655 - mean_squared_error: 6.0655 - val_loss: 16.1229 - val_mean_squared_error: 16.1229\n",
      "Epoch 299/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 6.0337 - mean_squared_error: 6.0337 - val_loss: 16.0381 - val_mean_squared_error: 16.0381\n",
      "Epoch 300/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 5.9702 - mean_squared_error: 5.9702 - val_loss: 15.8277 - val_mean_squared_error: 15.8277\n",
      "Epoch 301/400\n",
      "339/339 [==============================] - 0s 197us/sample - loss: 6.0728 - mean_squared_error: 6.0728 - val_loss: 16.2149 - val_mean_squared_error: 16.2149\n",
      "Epoch 302/400\n",
      "339/339 [==============================] - 0s 247us/sample - loss: 6.1228 - mean_squared_error: 6.1228 - val_loss: 15.9832 - val_mean_squared_error: 15.9832\n",
      "Epoch 303/400\n",
      "339/339 [==============================] - 0s 235us/sample - loss: 6.0788 - mean_squared_error: 6.0788 - val_loss: 15.4718 - val_mean_squared_error: 15.4718\n",
      "Epoch 304/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 5.9231 - mean_squared_error: 5.9231 - val_loss: 15.9711 - val_mean_squared_error: 15.9711\n",
      "Epoch 305/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: 5.9710 - mean_squared_error: 5.9710 - val_loss: 15.5631 - val_mean_squared_error: 15.5631\n",
      "Epoch 306/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 5.9660 - mean_squared_error: 5.9660 - val_loss: 15.1894 - val_mean_squared_error: 15.1894\n",
      "Epoch 307/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 6.0118 - mean_squared_error: 6.0118 - val_loss: 15.7112 - val_mean_squared_error: 15.7112\n",
      "Epoch 308/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 5.9870 - mean_squared_error: 5.9870 - val_loss: 16.2194 - val_mean_squared_error: 16.2194\n",
      "Epoch 309/400\n",
      "339/339 [==============================] - 0s 182us/sample - loss: 5.9160 - mean_squared_error: 5.9160 - val_loss: 16.0213 - val_mean_squared_error: 16.0213\n",
      "Epoch 310/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 5.9571 - mean_squared_error: 5.9571 - val_loss: 15.4773 - val_mean_squared_error: 15.4773\n",
      "Epoch 311/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 6.0376 - mean_squared_error: 6.0376 - val_loss: 15.8481 - val_mean_squared_error: 15.8481\n",
      "Epoch 312/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 5.9131 - mean_squared_error: 5.9131 - val_loss: 15.2563 - val_mean_squared_error: 15.2563\n",
      "Epoch 313/400\n",
      "339/339 [==============================] - 0s 197us/sample - loss: 5.9885 - mean_squared_error: 5.9885 - val_loss: 16.3687 - val_mean_squared_error: 16.3687\n",
      "Epoch 314/400\n",
      "339/339 [==============================] - 0s 291us/sample - loss: 5.9873 - mean_squared_error: 5.9873 - val_loss: 15.9750 - val_mean_squared_error: 15.9750\n",
      "Epoch 315/400\n",
      "339/339 [==============================] - 0s 253us/sample - loss: 6.0011 - mean_squared_error: 6.0011 - val_loss: 15.6603 - val_mean_squared_error: 15.6603\n",
      "Epoch 316/400\n",
      "339/339 [==============================] - 0s 259us/sample - loss: 5.8851 - mean_squared_error: 5.8851 - val_loss: 15.8722 - val_mean_squared_error: 15.8722\n",
      "Epoch 317/400\n",
      "339/339 [==============================] - 0s 265us/sample - loss: 5.8966 - mean_squared_error: 5.8966 - val_loss: 15.6320 - val_mean_squared_error: 15.6320\n",
      "Epoch 318/400\n",
      "339/339 [==============================] - ETA: 0s - loss: 5.9101 - mean_squared_error: 5.91 - 0s 224us/sample - loss: 5.9089 - mean_squared_error: 5.9089 - val_loss: 15.8585 - val_mean_squared_error: 15.8585\n",
      "Epoch 319/400\n",
      "339/339 [==============================] - 0s 194us/sample - loss: 5.8653 - mean_squared_error: 5.8653 - val_loss: 16.6918 - val_mean_squared_error: 16.6918\n",
      "Epoch 320/400\n",
      "339/339 [==============================] - 0s 215us/sample - loss: 5.9076 - mean_squared_error: 5.9076 - val_loss: 17.1065 - val_mean_squared_error: 17.1065\n",
      "Epoch 321/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 5.9987 - mean_squared_error: 5.9987 - val_loss: 15.5318 - val_mean_squared_error: 15.5318\n",
      "Epoch 322/400\n",
      "339/339 [==============================] - 0s 238us/sample - loss: 5.8555 - mean_squared_error: 5.8555 - val_loss: 15.7121 - val_mean_squared_error: 15.7121\n",
      "Epoch 323/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 5.9036 - mean_squared_error: 5.9036 - val_loss: 15.7301 - val_mean_squared_error: 15.7301\n",
      "Epoch 324/400\n",
      "339/339 [==============================] - 0s 332us/sample - loss: 5.9245 - mean_squared_error: 5.9245 - val_loss: 15.5460 - val_mean_squared_error: 15.5460\n",
      "Epoch 325/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: 5.8667 - mean_squared_error: 5.8667 - val_loss: 16.1784 - val_mean_squared_error: 16.1784\n",
      "Epoch 326/400\n",
      "339/339 [==============================] - 0s 279us/sample - loss: 5.8716 - mean_squared_error: 5.8716 - val_loss: 15.8957 - val_mean_squared_error: 15.8957\n",
      "Epoch 327/400\n",
      "339/339 [==============================] - 0s 229us/sample - loss: 5.8637 - mean_squared_error: 5.8637 - val_loss: 16.0496 - val_mean_squared_error: 16.0496\n",
      "Epoch 328/400\n",
      "339/339 [==============================] - 0s 241us/sample - loss: 5.8184 - mean_squared_error: 5.8184 - val_loss: 15.4069 - val_mean_squared_error: 15.4069\n",
      "Epoch 329/400\n",
      "339/339 [==============================] - 0s 197us/sample - loss: 5.8692 - mean_squared_error: 5.8692 - val_loss: 15.7442 - val_mean_squared_error: 15.7442\n",
      "Epoch 330/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 5.8591 - mean_squared_error: 5.8591 - val_loss: 16.0028 - val_mean_squared_error: 16.0028\n",
      "Epoch 331/400\n",
      "339/339 [==============================] - 0s 212us/sample - loss: 5.8065 - mean_squared_error: 5.8065 - val_loss: 16.0233 - val_mean_squared_error: 16.0233\n",
      "Epoch 332/400\n",
      "339/339 [==============================] - 0s 232us/sample - loss: 5.8872 - mean_squared_error: 5.8872 - val_loss: 15.6442 - val_mean_squared_error: 15.6442\n",
      "Epoch 333/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 5.8701 - mean_squared_error: 5.8701 - val_loss: 15.4156 - val_mean_squared_error: 15.4156\n",
      "Epoch 334/400\n",
      "339/339 [==============================] - 0s 221us/sample - loss: 5.8347 - mean_squared_error: 5.8347 - val_loss: 16.2749 - val_mean_squared_error: 16.2749\n",
      "Epoch 335/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 294us/sample - loss: 5.7967 - mean_squared_error: 5.7967 - val_loss: 15.5218 - val_mean_squared_error: 15.5218\n",
      "Epoch 336/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 5.8567 - mean_squared_error: 5.8567 - val_loss: 15.8726 - val_mean_squared_error: 15.8726\n",
      "Epoch 337/400\n",
      "339/339 [==============================] - 0s 247us/sample - loss: 5.8752 - mean_squared_error: 5.8752 - val_loss: 15.5354 - val_mean_squared_error: 15.5354\n",
      "Epoch 338/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 5.8293 - mean_squared_error: 5.8293 - val_loss: 15.8133 - val_mean_squared_error: 15.8133\n",
      "Epoch 339/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: 5.7939 - mean_squared_error: 5.7939 - val_loss: 15.6419 - val_mean_squared_error: 15.6419\n",
      "Epoch 340/400\n",
      "339/339 [==============================] - 0s 171us/sample - loss: 5.7278 - mean_squared_error: 5.7278 - val_loss: 15.2032 - val_mean_squared_error: 15.2032\n",
      "Epoch 341/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 5.8176 - mean_squared_error: 5.8176 - val_loss: 15.7026 - val_mean_squared_error: 15.7026\n",
      "Epoch 342/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 5.7795 - mean_squared_error: 5.7795 - val_loss: 15.7738 - val_mean_squared_error: 15.7738\n",
      "Epoch 343/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 5.7650 - mean_squared_error: 5.7650 - val_loss: 15.6101 - val_mean_squared_error: 15.6101\n",
      "Epoch 344/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: 5.7124 - mean_squared_error: 5.7124 - val_loss: 15.2257 - val_mean_squared_error: 15.2257\n",
      "Epoch 345/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 5.7738 - mean_squared_error: 5.7738 - val_loss: 16.2113 - val_mean_squared_error: 16.2113\n",
      "Epoch 346/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 5.7541 - mean_squared_error: 5.7541 - val_loss: 15.9051 - val_mean_squared_error: 15.9051\n",
      "Epoch 347/400\n",
      "339/339 [==============================] - 0s 227us/sample - loss: 5.7254 - mean_squared_error: 5.7254 - val_loss: 15.4499 - val_mean_squared_error: 15.4499\n",
      "Epoch 348/400\n",
      "339/339 [==============================] - 0s 212us/sample - loss: 5.7188 - mean_squared_error: 5.7188 - val_loss: 15.0314 - val_mean_squared_error: 15.0314\n",
      "Epoch 349/400\n",
      "339/339 [==============================] - 0s 268us/sample - loss: 5.7164 - mean_squared_error: 5.7164 - val_loss: 15.6326 - val_mean_squared_error: 15.6326\n",
      "Epoch 350/400\n",
      "339/339 [==============================] - 0s 215us/sample - loss: 5.7833 - mean_squared_error: 5.7833 - val_loss: 14.8910 - val_mean_squared_error: 14.8910\n",
      "Epoch 351/400\n",
      "339/339 [==============================] - 0s 191us/sample - loss: 5.6806 - mean_squared_error: 5.6806 - val_loss: 15.7127 - val_mean_squared_error: 15.7127\n",
      "Epoch 352/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 5.7045 - mean_squared_error: 5.7045 - val_loss: 16.0277 - val_mean_squared_error: 16.0277\n",
      "Epoch 353/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 5.7471 - mean_squared_error: 5.7471 - val_loss: 15.0922 - val_mean_squared_error: 15.0922\n",
      "Epoch 354/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 5.8319 - mean_squared_error: 5.8319 - val_loss: 15.1595 - val_mean_squared_error: 15.1595\n",
      "Epoch 355/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 5.7123 - mean_squared_error: 5.7123 - val_loss: 14.9784 - val_mean_squared_error: 14.9784\n",
      "Epoch 356/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 5.7197 - mean_squared_error: 5.7197 - val_loss: 15.2931 - val_mean_squared_error: 15.2931\n",
      "Epoch 357/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 5.7561 - mean_squared_error: 5.7561 - val_loss: 15.3332 - val_mean_squared_error: 15.3332\n",
      "Epoch 358/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 5.6915 - mean_squared_error: 5.6915 - val_loss: 15.4209 - val_mean_squared_error: 15.4209\n",
      "Epoch 359/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 5.6746 - mean_squared_error: 5.6746 - val_loss: 15.4338 - val_mean_squared_error: 15.4338\n",
      "Epoch 360/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: 5.6430 - mean_squared_error: 5.6430 - val_loss: 15.4771 - val_mean_squared_error: 15.4771\n",
      "Epoch 361/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 5.7009 - mean_squared_error: 5.7009 - val_loss: 16.3010 - val_mean_squared_error: 16.3010\n",
      "Epoch 362/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 5.6792 - mean_squared_error: 5.6792 - val_loss: 15.5275 - val_mean_squared_error: 15.5275\n",
      "Epoch 363/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 5.6407 - mean_squared_error: 5.6407 - val_loss: 15.4047 - val_mean_squared_error: 15.4047\n",
      "Epoch 364/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 5.6698 - mean_squared_error: 5.6698 - val_loss: 16.0232 - val_mean_squared_error: 16.0232\n",
      "Epoch 365/400\n",
      "339/339 [==============================] - 0s 244us/sample - loss: 5.6438 - mean_squared_error: 5.6438 - val_loss: 15.8363 - val_mean_squared_error: 15.8363\n",
      "Epoch 366/400\n",
      "339/339 [==============================] - 0s 309us/sample - loss: 5.6805 - mean_squared_error: 5.6805 - val_loss: 15.1028 - val_mean_squared_error: 15.1028\n",
      "Epoch 367/400\n",
      "339/339 [==============================] - 0s 238us/sample - loss: 5.6772 - mean_squared_error: 5.6772 - val_loss: 14.9650 - val_mean_squared_error: 14.9650\n",
      "Epoch 368/400\n",
      "339/339 [==============================] - 0s 200us/sample - loss: 5.7607 - mean_squared_error: 5.7607 - val_loss: 15.3793 - val_mean_squared_error: 15.3793\n",
      "Epoch 369/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 5.6741 - mean_squared_error: 5.6741 - val_loss: 14.9682 - val_mean_squared_error: 14.9682\n",
      "Epoch 370/400\n",
      "339/339 [==============================] - 0s 291us/sample - loss: 5.6374 - mean_squared_error: 5.6374 - val_loss: 15.6616 - val_mean_squared_error: 15.6616\n",
      "Epoch 371/400\n",
      "339/339 [==============================] - 0s 215us/sample - loss: 5.6071 - mean_squared_error: 5.6071 - val_loss: 15.2721 - val_mean_squared_error: 15.2721\n",
      "Epoch 372/400\n",
      "339/339 [==============================] - 0s 335us/sample - loss: 5.6045 - mean_squared_error: 5.6045 - val_loss: 15.0671 - val_mean_squared_error: 15.0671\n",
      "Epoch 373/400\n",
      "339/339 [==============================] - 0s 171us/sample - loss: 5.6104 - mean_squared_error: 5.6104 - val_loss: 15.3110 - val_mean_squared_error: 15.3110\n",
      "Epoch 374/400\n",
      "339/339 [==============================] - 0s 235us/sample - loss: 5.6795 - mean_squared_error: 5.6795 - val_loss: 15.6880 - val_mean_squared_error: 15.6880\n",
      "Epoch 375/400\n",
      "339/339 [==============================] - 0s 212us/sample - loss: 5.6755 - mean_squared_error: 5.6755 - val_loss: 15.4232 - val_mean_squared_error: 15.4232\n",
      "Epoch 376/400\n",
      "339/339 [==============================] - 0s 256us/sample - loss: 5.7044 - mean_squared_error: 5.7044 - val_loss: 15.0030 - val_mean_squared_error: 15.0030\n",
      "Epoch 377/400\n",
      "339/339 [==============================] - 0s 253us/sample - loss: 5.5449 - mean_squared_error: 5.5449 - val_loss: 15.3084 - val_mean_squared_error: 15.3084\n",
      "Epoch 378/400\n",
      "339/339 [==============================] - 0s 238us/sample - loss: 5.5806 - mean_squared_error: 5.5806 - val_loss: 15.4627 - val_mean_squared_error: 15.4627\n",
      "Epoch 379/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 5.6658 - mean_squared_error: 5.6658 - val_loss: 15.5238 - val_mean_squared_error: 15.5238\n",
      "Epoch 380/400\n",
      "339/339 [==============================] - 0s 194us/sample - loss: 5.6591 - mean_squared_error: 5.6591 - val_loss: 15.6911 - val_mean_squared_error: 15.6911\n",
      "Epoch 381/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 5.5500 - mean_squared_error: 5.5500 - val_loss: 16.1734 - val_mean_squared_error: 16.1734\n",
      "Epoch 382/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 5.6726 - mean_squared_error: 5.6726 - val_loss: 15.5522 - val_mean_squared_error: 15.5522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 5.4960 - mean_squared_error: 5.4960 - val_loss: 16.2311 - val_mean_squared_error: 16.2311\n",
      "Epoch 384/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 5.6197 - mean_squared_error: 5.6197 - val_loss: 15.4801 - val_mean_squared_error: 15.4801\n",
      "Epoch 385/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 5.6154 - mean_squared_error: 5.6154 - val_loss: 15.1186 - val_mean_squared_error: 15.1186\n",
      "Epoch 386/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 5.5802 - mean_squared_error: 5.5802 - val_loss: 14.6211 - val_mean_squared_error: 14.6211\n",
      "Epoch 387/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 5.6793 - mean_squared_error: 5.6793 - val_loss: 15.0756 - val_mean_squared_error: 15.0756\n",
      "Epoch 388/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 5.6295 - mean_squared_error: 5.6295 - val_loss: 15.4963 - val_mean_squared_error: 15.4963\n",
      "Epoch 389/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 5.5233 - mean_squared_error: 5.5233 - val_loss: 15.4908 - val_mean_squared_error: 15.4908\n",
      "Epoch 390/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 5.5401 - mean_squared_error: 5.5401 - val_loss: 15.0264 - val_mean_squared_error: 15.0264\n",
      "Epoch 391/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 5.4929 - mean_squared_error: 5.4929 - val_loss: 15.0010 - val_mean_squared_error: 15.0010\n",
      "Epoch 392/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 5.6404 - mean_squared_error: 5.6404 - val_loss: 14.8229 - val_mean_squared_error: 14.8229\n",
      "Epoch 393/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 5.5794 - mean_squared_error: 5.5794 - val_loss: 14.7599 - val_mean_squared_error: 14.7599\n",
      "Epoch 394/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 5.4760 - mean_squared_error: 5.4760 - val_loss: 15.5157 - val_mean_squared_error: 15.5157\n",
      "Epoch 395/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 5.4787 - mean_squared_error: 5.4787 - val_loss: 14.8375 - val_mean_squared_error: 14.8375\n",
      "Epoch 396/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 5.5580 - mean_squared_error: 5.5580 - val_loss: 15.7591 - val_mean_squared_error: 15.7591\n",
      "Epoch 397/400\n",
      "339/339 [==============================] - 0s 241us/sample - loss: 5.5653 - mean_squared_error: 5.5653 - val_loss: 15.5752 - val_mean_squared_error: 15.5752\n",
      "Epoch 398/400\n",
      "339/339 [==============================] - 0s 182us/sample - loss: 5.5411 - mean_squared_error: 5.5411 - val_loss: 15.2525 - val_mean_squared_error: 15.2525\n",
      "Epoch 399/400\n",
      "339/339 [==============================] - 0s 229us/sample - loss: 5.5169 - mean_squared_error: 5.5169 - val_loss: 15.0756 - val_mean_squared_error: 15.0756\n",
      "Epoch 400/400\n",
      "339/339 [==============================] - 0s 229us/sample - loss: 5.4729 - mean_squared_error: 5.4729 - val_loss: 15.1701 - val_mean_squared_error: 15.1701\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(bouston_housing_data_instances,\n",
    "                                                   boston_housing_data.target,\n",
    "                                                   test_size=0.33)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu', input_shape=(13,)))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "\n",
    "model.compile(optimizer = tf.train.GradientDescentOptimizer(0.001),loss='mean_squared_error',metrics=['mse'])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    model.fit(train_X, train_y, epochs=400, batch_size=10, validation_data = (test_X, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
