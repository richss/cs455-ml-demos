{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CS 455/595a: MLP Demo using TensorFlow</center></h1>\n",
    "<center>Richard S. Stansbury</center>\n",
    "\n",
    "This notebook applies the ANN techniques for the Titanic Survivors and Boston Housing Prediction models covered in [1] with the [Titanic](https://www.kaggle.com/c/titanic/) and [Boston Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) data sets for DT-based classification and regression, respectively.\n",
    "\n",
    "Several different approaches to model construction are shown ihe demos below\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1] Aurelen Geron. *Hands on Machine Learning with Scikit-Learn & TensorFlow* O'Reilley Media Inc, 2017.\n",
    "\n",
    "[2] Aurelen Geron. \"ageron/handson-ml: A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow.\" Github.com, online at: https://github.com/ageron/handson-ml [last accessed 2019-03-01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "1. [Titanic Survivor ANN Classifiers](#Titanic-Survivor-Classifier)\n",
    " \n",
    "2. [Boston Housing Cost Ensemble ANN Regressor](#Boston-Housing-Cost-Estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survivor Classifier\n",
    "\n",
    "## Set up - Imports of libraries and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# From: https://github.com/ageron/handson-ml/blob/master/09_up_and_running_with_tensorflow.ipynb    \n",
    "def reset_graph():\n",
    "    tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data and apply pipelines to pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 9)\n",
      "(891, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data from input files into Pandas data frames\n",
    "data_path = os.path.join(\"datasets\",\"titanic\")\n",
    "train_filename = \"train.csv\"\n",
    "test_filename = \"test.csv\"\n",
    "\n",
    "def read_csv(data_path, filename):\n",
    "    joined_path = os.path.join(data_path, filename)\n",
    "    return pd.read_csv(joined_path)\n",
    "\n",
    "# Read CSV file into Pandas Dataframes\n",
    "train_df = read_csv(data_path, train_filename)\n",
    "\n",
    "# Defining Data Pre-Processing Pipelines\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, attributes):\n",
    "        self.attributes = attributes\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attributes]\n",
    "\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent = pd.Series([X[c].value_counts().index[0] for c in X], \n",
    "                                       index = X.columns)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.fillna(self.most_frequent)\n",
    "\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "        (\"Select\", DataFrameSelector([\"Age\", \"Fare\", \"SibSp\", \"Parch\"])), # Selects Fields from dataframe\n",
    "        (\"Imputer\", SimpleImputer(strategy=\"median\")),   # Fills in NaN w/ median value for its column\n",
    "        (\"Scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "categories_pipe = Pipeline([\n",
    "        (\"Select\", DataFrameSelector([\"Pclass\", \"Sex\"])), # Selects Fields from dataframe\n",
    "        (\"MostFreqImp\", MostFrequentImputer()), # Fill in NaN with most frequent\n",
    "        (\"OneHot\", OneHotEncoder(sparse=False, categories='auto')), # Onehot encode\n",
    "    ])\n",
    "\n",
    "preprocessing_pipe = FeatureUnion(transformer_list = [\n",
    "        (\"numeric pipeline\", numeric_pipe), \n",
    "        (\"categories pipeline\", categories_pipe)\n",
    "     ]) \n",
    "\n",
    "# Process Input Data Using Pipleines\n",
    "X_data = preprocessing_pipe.fit_transform(train_df)\n",
    "y_data = train_df[\"Survived\"].values.reshape(-1,1)\n",
    "\n",
    "# Process the output data.\n",
    "feature_names = [\"Age\", \"Fare\", \"SibSp\", \"Parch\", \"Class0\", \"class1\",\"Sex0\", \"Sex1\"]\n",
    "\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596, 9) (596, 1) (295, 9) (295, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.33)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the TF.Estimator.DNNClassifier (formerly of TFLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\richa\\\\AppData\\\\Local\\\\Temp\\\\tmp152d877p', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DD025F89E8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\\model.ckpt.\n",
      "INFO:tensorflow:loss = 38.381626, step = 1\n",
      "INFO:tensorflow:global_step/sec: 447.622\n",
      "INFO:tensorflow:loss = 21.983316, step = 101 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.538\n",
      "INFO:tensorflow:loss = 21.217855, step = 201 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.538\n",
      "INFO:tensorflow:loss = 20.855549, step = 301 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.719\n",
      "INFO:tensorflow:loss = 18.536669, step = 401 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 871.886\n",
      "INFO:tensorflow:loss = 17.773914, step = 501 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 911.524\n",
      "INFO:tensorflow:loss = 25.729843, step = 601 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 864.377\n",
      "INFO:tensorflow:loss = 21.810678, step = 701 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.54\n",
      "INFO:tensorflow:loss = 20.115366, step = 801 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 856.977\n",
      "INFO:tensorflow:loss = 12.671253, step = 901 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 895.245\n",
      "INFO:tensorflow:loss = 18.845486, step = 1001 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.321\n",
      "INFO:tensorflow:loss = 16.177044, step = 1101 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.325\n",
      "INFO:tensorflow:loss = 21.307896, step = 1201 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 864.374\n",
      "INFO:tensorflow:loss = 20.480532, step = 1301 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 600.403\n",
      "INFO:tensorflow:loss = 18.540726, step = 1401 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 642.736\n",
      "INFO:tensorflow:loss = 13.092168, step = 1501 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 672.934\n",
      "INFO:tensorflow:loss = 13.699125, step = 1601 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 783.342\n",
      "INFO:tensorflow:loss = 19.529427, step = 1701 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.578\n",
      "INFO:tensorflow:loss = 20.49198, step = 1801 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 742.72\n",
      "INFO:tensorflow:loss = 18.131166, step = 1901 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.337\n",
      "INFO:tensorflow:loss = 16.043087, step = 2001 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.672\n",
      "INFO:tensorflow:loss = 12.737453, step = 2101 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 691.538\n",
      "INFO:tensorflow:loss = 26.872074, step = 2201 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.587\n",
      "INFO:tensorflow:loss = 20.14505, step = 2301 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 871.888\n",
      "INFO:tensorflow:loss = 17.428999, step = 2401 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.658\n",
      "INFO:tensorflow:loss = 23.612007, step = 2501 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.582\n",
      "INFO:tensorflow:loss = 11.4780035, step = 2601 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.72\n",
      "INFO:tensorflow:loss = 13.60433, step = 2701 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 771.287\n",
      "INFO:tensorflow:loss = 14.203247, step = 2801 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.583\n",
      "INFO:tensorflow:loss = 13.395087, step = 2901 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.655\n",
      "INFO:tensorflow:loss = 12.900935, step = 3001 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 765.399\n",
      "INFO:tensorflow:loss = 12.678525, step = 3101 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 821.86\n",
      "INFO:tensorflow:loss = 13.676514, step = 3201 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 777.26\n",
      "INFO:tensorflow:loss = 22.814781, step = 3301 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 579.584\n",
      "INFO:tensorflow:loss = 16.14307, step = 3401 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.137\n",
      "INFO:tensorflow:loss = 16.56072, step = 3501 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 742.72\n",
      "INFO:tensorflow:loss = 18.072622, step = 3601 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.582\n",
      "INFO:tensorflow:loss = 16.15227, step = 3701 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 802.139\n",
      "INFO:tensorflow:loss = 15.808844, step = 3801 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 835.557\n",
      "INFO:tensorflow:loss = 11.586376, step = 3901 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 821.866\n",
      "INFO:tensorflow:loss = 17.958904, step = 4001 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.578\n",
      "INFO:tensorflow:loss = 25.965998, step = 4101 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.65\n",
      "INFO:tensorflow:loss = 20.961824, step = 4201 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 748.268\n",
      "INFO:tensorflow:loss = 12.407464, step = 4301 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 864.356\n",
      "INFO:tensorflow:loss = 14.689327, step = 4401 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.739\n",
      "INFO:tensorflow:loss = 16.462431, step = 4501 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.606\n",
      "INFO:tensorflow:loss = 17.770634, step = 4601 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.706\n",
      "INFO:tensorflow:loss = 15.8100815, step = 4701 (0.120 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4768 into C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 21.607399.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07T17:15:46Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\\model.ckpt-4768\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-17:15:46\n",
      "INFO:tensorflow:Saving dict for global step 4768: accuracy = 0.8305085, accuracy_baseline = 0.64406776, auc = 0.85167915, auc_precision_recall = 0.82776606, average_loss = 0.4634297, global_step = 4768, label/mean = 0.3559322, loss = 45.570587, precision = 0.7522936, prediction/mean = 0.38629514, recall = 0.7809524\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4768: C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\\model.ckpt-4768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8305085,\n",
       " 'accuracy_baseline': 0.64406776,\n",
       " 'auc': 0.85167915,\n",
       " 'auc_precision_recall': 0.82776606,\n",
       " 'average_loss': 0.4634297,\n",
       " 'label/mean': 0.3559322,\n",
       " 'loss': 45.570587,\n",
       " 'precision': 0.7522936,\n",
       " 'prediction/mean': 0.38629514,\n",
       " 'recall': 0.7809524,\n",
       " 'global_step': 4768}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construction Phase\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "\n",
    "feature_cols = [tf.feature_column.numeric_column(\"X\", shape=[X_data.shape[1]])]\n",
    "\n",
    "dnn_clf = tf.estimator.DNNClassifier(hidden_units=[20,20], n_classes=2,\n",
    "                                     feature_columns=feature_cols)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_train}, y=y_train, batch_size=50, num_epochs=400, shuffle=True)\n",
    "dnn_clf.train(input_fn=train_input_fn)\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_test}, y=y_test, shuffle=False)\n",
    "eval_results = dnn_clf.evaluate(input_fn=test_input_fn)\n",
    "                                \n",
    "eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Train: 0.6304348111152649 Test:0.6847457885742188\n",
      "1-Train: 0.695652186870575 Test:0.7593220472335815\n",
      "2-Train: 0.760869562625885 Test:0.7762711644172668\n",
      "3-Train: 0.782608687877655 Test:0.806779682636261\n",
      "4-Train: 0.804347813129425 Test:0.806779682636261\n",
      "5-Train: 0.804347813129425 Test:0.8237287998199463\n",
      "6-Train: 0.8478260636329651 Test:0.8305084705352783\n",
      "7-Train: 0.8478260636329651 Test:0.8338983058929443\n",
      "8-Train: 0.8260869383811951 Test:0.8338983058929443\n",
      "9-Train: 0.8478260636329651 Test:0.8406779766082764\n",
      "10-Train: 0.8478260636329651 Test:0.8406779766082764\n",
      "11-Train: 0.8478260636329651 Test:0.8406779766082764\n",
      "12-Train: 0.8478260636329651 Test:0.8406779766082764\n",
      "13-Train: 0.8260869383811951 Test:0.8406779766082764\n",
      "14-Train: 0.8260869383811951 Test:0.8338983058929443\n",
      "15-Train: 0.804347813129425 Test:0.8305084705352783\n",
      "16-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "17-Train: 0.804347813129425 Test:0.8305084705352783\n",
      "18-Train: 0.804347813129425 Test:0.8305084705352783\n",
      "19-Train: 0.804347813129425 Test:0.8271186351776123\n",
      "20-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "21-Train: 0.804347813129425 Test:0.8237287998199463\n",
      "22-Train: 0.804347813129425 Test:0.8271186351776123\n",
      "23-Train: 0.8260869383811951 Test:0.8338983058929443\n",
      "24-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "25-Train: 0.8478260636329651 Test:0.8305084705352783\n",
      "26-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "27-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "28-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "29-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "30-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "31-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "32-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "33-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "34-Train: 0.8260869383811951 Test:0.8169491291046143\n",
      "35-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "36-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "37-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "38-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "39-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "40-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "41-Train: 0.8260869383811951 Test:0.8237287998199463\n",
      "42-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "43-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "44-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "45-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "46-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "47-Train: 0.804347813129425 Test:0.8203389644622803\n",
      "48-Train: 0.8260869383811951 Test:0.8169491291046143\n",
      "49-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "50-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "51-Train: 0.8260869383811951 Test:0.8169491291046143\n",
      "52-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "53-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "54-Train: 0.8260869383811951 Test:0.8237287998199463\n",
      "55-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "56-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "57-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "58-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "59-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "60-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "61-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "62-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "63-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "64-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "65-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "66-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "67-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "68-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "69-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "70-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "71-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "72-Train: 0.8695651888847351 Test:0.8305084705352783\n",
      "73-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "74-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "75-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "76-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "77-Train: 0.8478260636329651 Test:0.8305084705352783\n",
      "78-Train: 0.8695651888847351 Test:0.8338983058929443\n",
      "79-Train: 0.8695651888847351 Test:0.8338983058929443\n",
      "80-Train: 0.8478260636329651 Test:0.8271186351776123\n",
      "81-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "82-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "83-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "84-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "85-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "86-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "87-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "88-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "89-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "90-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "91-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "92-Train: 0.8695651888847351 Test:0.8305084705352783\n",
      "93-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "94-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "95-Train: 0.8695651888847351 Test:0.8305084705352783\n",
      "96-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "97-Train: 0.8695651888847351 Test:0.8338983058929443\n",
      "98-Train: 0.8695651888847351 Test:0.8305084705352783\n",
      "99-Train: 0.8695651888847351 Test:0.8305084705352783\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "def get_batch(X, iter, size):\n",
    "    return X[(iter*batch_size) : ((iter+1)*batch_size)]\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_instances = X_train.shape[0]\n",
    "\n",
    "# Construction\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_features), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Titanic_MLP\"):\n",
    "    hidden1 = tf.layers.dense(X, 20, name=\"Hidden-1\", activation = tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, 10, name=\"Hidden-2\", activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, 5, name=\"Hidden-3\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden3, 2, name=\"Survived\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"): \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for iteration in range(num_instances // batch_size + 1):\n",
    "            X_batch = get_batch(X_train, iteration, batch_size)\n",
    "            y_batch = get_batch(y_train, iteration, batch_size)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={X: X_batch,\n",
    "                                            y: y_batch.reshape(y_batch.shape[0])})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch,\n",
    "                                            y: y_batch.reshape(y_batch.shape[0])})\n",
    "        acc_val = accuracy.eval(feed_dict={X:X_test, y: y_test.reshape(y_test.shape[0])})\n",
    "        \n",
    "        print(\"{}-Train: {} Test:{}\".format(epoch,\n",
    "                                           acc_train,\n",
    "                                           acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 596 samples, validate on 295 samples\n",
      "Epoch 1/400\n",
      "596/596 [==============================] - 0s 423us/sample - loss: 0.6953 - acc: 0.5134 - val_loss: 0.6740 - val_acc: 0.5932\n",
      "Epoch 2/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.6713 - acc: 0.5839 - val_loss: 0.6451 - val_acc: 0.6576\n",
      "Epoch 3/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.6527 - acc: 0.6124 - val_loss: 0.6257 - val_acc: 0.6678\n",
      "Epoch 4/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.6378 - acc: 0.6309 - val_loss: 0.6086 - val_acc: 0.6814\n",
      "Epoch 5/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.6230 - acc: 0.6409 - val_loss: 0.5913 - val_acc: 0.7085\n",
      "Epoch 6/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.6090 - acc: 0.6628 - val_loss: 0.5736 - val_acc: 0.7559\n",
      "Epoch 7/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5932 - acc: 0.7164 - val_loss: 0.5545 - val_acc: 0.7797\n",
      "Epoch 8/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.5765 - acc: 0.7433 - val_loss: 0.5332 - val_acc: 0.7898\n",
      "Epoch 9/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.5594 - acc: 0.7383 - val_loss: 0.5160 - val_acc: 0.7966\n",
      "Epoch 10/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5429 - acc: 0.7567 - val_loss: 0.4993 - val_acc: 0.8034\n",
      "Epoch 11/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.5267 - acc: 0.7634 - val_loss: 0.4832 - val_acc: 0.8102\n",
      "Epoch 12/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.5126 - acc: 0.7752 - val_loss: 0.4707 - val_acc: 0.8068\n",
      "Epoch 13/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4985 - acc: 0.7768 - val_loss: 0.4551 - val_acc: 0.8102\n",
      "Epoch 14/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4879 - acc: 0.7802 - val_loss: 0.4450 - val_acc: 0.8102\n",
      "Epoch 15/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4767 - acc: 0.7903 - val_loss: 0.4348 - val_acc: 0.8102\n",
      "Epoch 16/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4704 - acc: 0.7869 - val_loss: 0.4276 - val_acc: 0.8102\n",
      "Epoch 17/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.4644 - acc: 0.7953 - val_loss: 0.4206 - val_acc: 0.8136\n",
      "Epoch 18/400\n",
      "596/596 [==============================] - 0s 40us/sample - loss: 0.4573 - acc: 0.7886 - val_loss: 0.4193 - val_acc: 0.8305\n",
      "Epoch 19/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.4535 - acc: 0.8070 - val_loss: 0.4149 - val_acc: 0.8305\n",
      "Epoch 20/400\n",
      "596/596 [==============================] - 0s 50us/sample - loss: 0.4503 - acc: 0.8087 - val_loss: 0.4129 - val_acc: 0.8237\n",
      "Epoch 21/400\n",
      "596/596 [==============================] - 0s 44us/sample - loss: 0.4487 - acc: 0.8070 - val_loss: 0.4070 - val_acc: 0.8339\n",
      "Epoch 22/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4447 - acc: 0.8171 - val_loss: 0.4078 - val_acc: 0.8305\n",
      "Epoch 23/400\n",
      "596/596 [==============================] - 0s 55us/sample - loss: 0.4439 - acc: 0.8121 - val_loss: 0.4071 - val_acc: 0.8305\n",
      "Epoch 24/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4410 - acc: 0.8104 - val_loss: 0.4040 - val_acc: 0.8339\n",
      "Epoch 25/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4397 - acc: 0.8138 - val_loss: 0.4074 - val_acc: 0.8305\n",
      "Epoch 26/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4374 - acc: 0.8188 - val_loss: 0.4058 - val_acc: 0.8305\n",
      "Epoch 27/400\n",
      "596/596 [==============================] - 0s 52us/sample - loss: 0.4367 - acc: 0.8154 - val_loss: 0.4009 - val_acc: 0.8339\n",
      "Epoch 28/400\n",
      "596/596 [==============================] - 0s 45us/sample - loss: 0.4345 - acc: 0.8188 - val_loss: 0.4076 - val_acc: 0.8237\n",
      "Epoch 29/400\n",
      "596/596 [==============================] - 0s 44us/sample - loss: 0.4322 - acc: 0.8188 - val_loss: 0.3960 - val_acc: 0.8407\n",
      "Epoch 30/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4325 - acc: 0.8205 - val_loss: 0.4033 - val_acc: 0.8271\n",
      "Epoch 31/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.4294 - acc: 0.8171 - val_loss: 0.3966 - val_acc: 0.8339\n",
      "Epoch 32/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.4301 - acc: 0.8171 - val_loss: 0.3952 - val_acc: 0.8407\n",
      "Epoch 33/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.4294 - acc: 0.8171 - val_loss: 0.3969 - val_acc: 0.8305\n",
      "Epoch 34/400\n",
      "596/596 [==============================] - 0s 40us/sample - loss: 0.4276 - acc: 0.8104 - val_loss: 0.3932 - val_acc: 0.8407\n",
      "Epoch 35/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.4248 - acc: 0.8154 - val_loss: 0.3948 - val_acc: 0.8339\n",
      "Epoch 36/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.4255 - acc: 0.8171 - val_loss: 0.3910 - val_acc: 0.8339\n",
      "Epoch 37/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4239 - acc: 0.8121 - val_loss: 0.3955 - val_acc: 0.8339\n",
      "Epoch 38/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4214 - acc: 0.8138 - val_loss: 0.3907 - val_acc: 0.8508\n",
      "Epoch 39/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4240 - acc: 0.8188 - val_loss: 0.3936 - val_acc: 0.8373\n",
      "Epoch 40/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4203 - acc: 0.8171 - val_loss: 0.3908 - val_acc: 0.8441\n",
      "Epoch 41/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.4201 - acc: 0.8188 - val_loss: 0.3955 - val_acc: 0.8339\n",
      "Epoch 42/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4189 - acc: 0.8272 - val_loss: 0.3931 - val_acc: 0.8373\n",
      "Epoch 43/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4182 - acc: 0.8221 - val_loss: 0.3939 - val_acc: 0.8339\n",
      "Epoch 44/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4188 - acc: 0.8205 - val_loss: 0.3883 - val_acc: 0.8441\n",
      "Epoch 45/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4163 - acc: 0.8221 - val_loss: 0.3953 - val_acc: 0.8339\n",
      "Epoch 46/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4152 - acc: 0.8188 - val_loss: 0.3903 - val_acc: 0.8475\n",
      "Epoch 47/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4160 - acc: 0.8171 - val_loss: 0.3937 - val_acc: 0.8373\n",
      "Epoch 48/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4145 - acc: 0.8255 - val_loss: 0.3855 - val_acc: 0.8407\n",
      "Epoch 49/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4161 - acc: 0.8238 - val_loss: 0.3919 - val_acc: 0.8407\n",
      "Epoch 50/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3585 - acc: 0.870 - 0s 30us/sample - loss: 0.4149 - acc: 0.8188 - val_loss: 0.3919 - val_acc: 0.8407\n",
      "Epoch 51/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4149 - acc: 0.8205 - val_loss: 0.3881 - val_acc: 0.8475\n",
      "Epoch 52/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4122 - acc: 0.8238 - val_loss: 0.3844 - val_acc: 0.8407\n",
      "Epoch 53/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4135 - acc: 0.8188 - val_loss: 0.3846 - val_acc: 0.8441\n",
      "Epoch 54/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4130 - acc: 0.8171 - val_loss: 0.3875 - val_acc: 0.8475\n",
      "Epoch 55/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4109 - acc: 0.8188 - val_loss: 0.3850 - val_acc: 0.8475\n",
      "Epoch 56/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4102 - acc: 0.8221 - val_loss: 0.3863 - val_acc: 0.8475\n",
      "Epoch 57/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4124 - acc: 0.8272 - val_loss: 0.3864 - val_acc: 0.8475\n",
      "Epoch 58/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4099 - acc: 0.8205 - val_loss: 0.3851 - val_acc: 0.8475\n",
      "Epoch 59/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.4104 - acc: 0.8205 - val_loss: 0.3838 - val_acc: 0.8475\n",
      "Epoch 60/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4078 - acc: 0.8255 - val_loss: 0.3892 - val_acc: 0.8441\n",
      "Epoch 61/400\n",
      "596/596 [==============================] - 0s 44us/sample - loss: 0.4083 - acc: 0.8272 - val_loss: 0.3820 - val_acc: 0.8407\n",
      "Epoch 62/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4081 - acc: 0.8272 - val_loss: 0.3821 - val_acc: 0.8407\n",
      "Epoch 63/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4066 - acc: 0.8289 - val_loss: 0.3818 - val_acc: 0.8441\n",
      "Epoch 64/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4062 - acc: 0.8289 - val_loss: 0.3856 - val_acc: 0.8475\n",
      "Epoch 65/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4049 - acc: 0.8272 - val_loss: 0.3801 - val_acc: 0.8407\n",
      "Epoch 66/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4047 - acc: 0.8305 - val_loss: 0.3884 - val_acc: 0.8475\n",
      "Epoch 67/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4071 - acc: 0.8289 - val_loss: 0.3964 - val_acc: 0.8203\n",
      "Epoch 68/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4087 - acc: 0.8272 - val_loss: 0.3822 - val_acc: 0.8441\n",
      "Epoch 69/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4035 - acc: 0.8322 - val_loss: 0.3805 - val_acc: 0.8407\n",
      "Epoch 70/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4072 - acc: 0.8289 - val_loss: 0.3792 - val_acc: 0.8407\n",
      "Epoch 71/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4027 - acc: 0.8339 - val_loss: 0.3854 - val_acc: 0.8475\n",
      "Epoch 72/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4030 - acc: 0.8322 - val_loss: 0.3805 - val_acc: 0.8407\n",
      "Epoch 73/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4101 - acc: 0.8255 - val_loss: 0.3861 - val_acc: 0.8475\n",
      "Epoch 74/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4021 - acc: 0.8356 - val_loss: 0.3804 - val_acc: 0.8407\n",
      "Epoch 75/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4012 - acc: 0.8356 - val_loss: 0.3854 - val_acc: 0.8475\n",
      "Epoch 76/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4012 - acc: 0.8305 - val_loss: 0.3833 - val_acc: 0.8508\n",
      "Epoch 77/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4014 - acc: 0.8339 - val_loss: 0.3887 - val_acc: 0.8373\n",
      "Epoch 78/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4027 - acc: 0.8255 - val_loss: 0.3786 - val_acc: 0.8407\n",
      "Epoch 79/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4014 - acc: 0.8289 - val_loss: 0.3811 - val_acc: 0.8475\n",
      "Epoch 80/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4010 - acc: 0.8289 - val_loss: 0.3836 - val_acc: 0.8508\n",
      "Epoch 81/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4049 - acc: 0.8238 - val_loss: 0.3823 - val_acc: 0.8475\n",
      "Epoch 82/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3992 - acc: 0.8322 - val_loss: 0.3790 - val_acc: 0.8407\n",
      "Epoch 83/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4050 - acc: 0.8289 - val_loss: 0.3874 - val_acc: 0.8373\n",
      "Epoch 84/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4009 - acc: 0.8389 - val_loss: 0.3791 - val_acc: 0.8407\n",
      "Epoch 85/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4011 - acc: 0.8372 - val_loss: 0.3782 - val_acc: 0.8407\n",
      "Epoch 86/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3982 - acc: 0.8389 - val_loss: 0.3834 - val_acc: 0.8542\n",
      "Epoch 87/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.4005 - acc: 0.8356 - val_loss: 0.3775 - val_acc: 0.8407\n",
      "Epoch 88/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3994 - acc: 0.8339 - val_loss: 0.3919 - val_acc: 0.8271\n",
      "Epoch 89/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4003 - acc: 0.8322 - val_loss: 0.3808 - val_acc: 0.8475\n",
      "Epoch 90/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4005 - acc: 0.8272 - val_loss: 0.3796 - val_acc: 0.8441\n",
      "Epoch 91/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3968 - acc: 0.8423 - val_loss: 0.3856 - val_acc: 0.8475\n",
      "Epoch 92/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3983 - acc: 0.8356 - val_loss: 0.3878 - val_acc: 0.8339\n",
      "Epoch 93/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3977 - acc: 0.8356 - val_loss: 0.3816 - val_acc: 0.8475\n",
      "Epoch 94/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3990 - acc: 0.8305 - val_loss: 0.3800 - val_acc: 0.8441\n",
      "Epoch 95/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3978 - acc: 0.8423 - val_loss: 0.3789 - val_acc: 0.8339\n",
      "Epoch 96/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3958 - acc: 0.8440 - val_loss: 0.3786 - val_acc: 0.8441\n",
      "Epoch 97/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3958 - acc: 0.8372 - val_loss: 0.3845 - val_acc: 0.8407\n",
      "Epoch 98/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3981 - acc: 0.8456 - val_loss: 0.3822 - val_acc: 0.8475\n",
      "Epoch 99/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3953 - acc: 0.8389 - val_loss: 0.3790 - val_acc: 0.8441\n",
      "Epoch 100/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3957 - acc: 0.8406 - val_loss: 0.3794 - val_acc: 0.8475\n",
      "Epoch 101/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3942 - acc: 0.8406 - val_loss: 0.3833 - val_acc: 0.8407\n",
      "Epoch 102/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3956 - acc: 0.8406 - val_loss: 0.3785 - val_acc: 0.8373\n",
      "Epoch 103/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3956 - acc: 0.8406 - val_loss: 0.3874 - val_acc: 0.8339\n",
      "Epoch 104/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3994 - acc: 0.8289 - val_loss: 0.3830 - val_acc: 0.8441\n",
      "Epoch 105/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3921 - acc: 0.8389 - val_loss: 0.3877 - val_acc: 0.8339\n",
      "Epoch 106/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3938 - acc: 0.8372 - val_loss: 0.3869 - val_acc: 0.8339\n",
      "Epoch 107/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3974 - acc: 0.8255 - val_loss: 0.3888 - val_acc: 0.8305\n",
      "Epoch 108/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3928 - acc: 0.8406 - val_loss: 0.3815 - val_acc: 0.8441\n",
      "Epoch 109/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3929 - acc: 0.8389 - val_loss: 0.3850 - val_acc: 0.8373\n",
      "Epoch 110/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3960 - acc: 0.8406 - val_loss: 0.3805 - val_acc: 0.8407\n",
      "Epoch 111/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3939 - acc: 0.8322 - val_loss: 0.3896 - val_acc: 0.8373\n",
      "Epoch 112/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3943 - acc: 0.8339 - val_loss: 0.3804 - val_acc: 0.8475\n",
      "Epoch 113/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3952 - acc: 0.8322 - val_loss: 0.4089 - val_acc: 0.8102\n",
      "Epoch 114/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3957 - acc: 0.8305 - val_loss: 0.3839 - val_acc: 0.8339\n",
      "Epoch 115/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3902 - acc: 0.8423 - val_loss: 0.3815 - val_acc: 0.8475\n",
      "Epoch 116/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3908 - acc: 0.8406 - val_loss: 0.3796 - val_acc: 0.8475\n",
      "Epoch 117/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3936 - acc: 0.8423 - val_loss: 0.3824 - val_acc: 0.8407\n",
      "Epoch 118/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3898 - acc: 0.8440 - val_loss: 0.3817 - val_acc: 0.8407\n",
      "Epoch 119/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3936 - acc: 0.8356 - val_loss: 0.3828 - val_acc: 0.8441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3884 - acc: 0.8406 - val_loss: 0.3869 - val_acc: 0.8339\n",
      "Epoch 121/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3900 - acc: 0.8372 - val_loss: 0.3781 - val_acc: 0.8441\n",
      "Epoch 122/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3872 - acc: 0.8440 - val_loss: 0.3913 - val_acc: 0.8305\n",
      "Epoch 123/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3902 - acc: 0.8339 - val_loss: 0.3797 - val_acc: 0.8508\n",
      "Epoch 124/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3875 - acc: 0.8440 - val_loss: 0.3800 - val_acc: 0.8475\n",
      "Epoch 125/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3889 - acc: 0.8406 - val_loss: 0.3790 - val_acc: 0.8441\n",
      "Epoch 126/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3893 - acc: 0.8473 - val_loss: 0.3804 - val_acc: 0.8441\n",
      "Epoch 127/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3901 - acc: 0.8423 - val_loss: 0.3828 - val_acc: 0.8475\n",
      "Epoch 128/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3883 - acc: 0.8372 - val_loss: 0.3807 - val_acc: 0.8407\n",
      "Epoch 129/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3871 - acc: 0.8406 - val_loss: 0.3801 - val_acc: 0.8475\n",
      "Epoch 130/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3865 - acc: 0.8389 - val_loss: 0.3779 - val_acc: 0.8339\n",
      "Epoch 131/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3908 - acc: 0.8356 - val_loss: 0.3857 - val_acc: 0.8407\n",
      "Epoch 132/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3869 - acc: 0.8339 - val_loss: 0.3835 - val_acc: 0.8441\n",
      "Epoch 133/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3864 - acc: 0.8423 - val_loss: 0.3800 - val_acc: 0.8441\n",
      "Epoch 134/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3863 - acc: 0.8406 - val_loss: 0.3829 - val_acc: 0.8475\n",
      "Epoch 135/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3878 - acc: 0.8440 - val_loss: 0.3832 - val_acc: 0.8475\n",
      "Epoch 136/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3876 - acc: 0.8406 - val_loss: 0.3841 - val_acc: 0.8475\n",
      "Epoch 137/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3845 - acc: 0.8406 - val_loss: 0.3800 - val_acc: 0.8576\n",
      "Epoch 138/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3861 - acc: 0.8456 - val_loss: 0.3797 - val_acc: 0.8508\n",
      "Epoch 139/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3847 - acc: 0.8372 - val_loss: 0.3843 - val_acc: 0.8407\n",
      "Epoch 140/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3832 - acc: 0.8389 - val_loss: 0.3789 - val_acc: 0.8373\n",
      "Epoch 141/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3882 - acc: 0.8389 - val_loss: 0.3863 - val_acc: 0.8407\n",
      "Epoch 142/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3840 - acc: 0.8456 - val_loss: 0.3848 - val_acc: 0.8441\n",
      "Epoch 143/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3846 - acc: 0.8423 - val_loss: 0.3828 - val_acc: 0.8441\n",
      "Epoch 144/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3835 - acc: 0.8423 - val_loss: 0.3853 - val_acc: 0.8441\n",
      "Epoch 145/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3869 - acc: 0.8389 - val_loss: 0.3912 - val_acc: 0.8339\n",
      "Epoch 146/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3875 - acc: 0.8322 - val_loss: 0.3862 - val_acc: 0.8475\n",
      "Epoch 147/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3901 - acc: 0.8389 - val_loss: 0.3844 - val_acc: 0.8407\n",
      "Epoch 148/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3844 - acc: 0.8440 - val_loss: 0.3811 - val_acc: 0.8441\n",
      "Epoch 149/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3856 - acc: 0.8423 - val_loss: 0.3887 - val_acc: 0.8407\n",
      "Epoch 150/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3845 - acc: 0.8456 - val_loss: 0.3817 - val_acc: 0.8475\n",
      "Epoch 151/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3842 - acc: 0.8440 - val_loss: 0.3835 - val_acc: 0.8407\n",
      "Epoch 152/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3870 - acc: 0.8389 - val_loss: 0.3877 - val_acc: 0.8407\n",
      "Epoch 153/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3825 - acc: 0.8456 - val_loss: 0.3882 - val_acc: 0.8373\n",
      "Epoch 154/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3847 - acc: 0.8473 - val_loss: 0.3811 - val_acc: 0.8475\n",
      "Epoch 155/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3827 - acc: 0.8372 - val_loss: 0.3787 - val_acc: 0.8441\n",
      "Epoch 156/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3812 - acc: 0.8423 - val_loss: 0.3853 - val_acc: 0.8441\n",
      "Epoch 157/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3856 - acc: 0.8406 - val_loss: 0.3845 - val_acc: 0.8441\n",
      "Epoch 158/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3818 - acc: 0.8440 - val_loss: 0.3812 - val_acc: 0.8407\n",
      "Epoch 159/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3812 - acc: 0.8456 - val_loss: 0.3808 - val_acc: 0.8373\n",
      "Epoch 160/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3852 - acc: 0.8456 - val_loss: 0.3991 - val_acc: 0.8271\n",
      "Epoch 161/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3834 - acc: 0.8372 - val_loss: 0.3862 - val_acc: 0.8441\n",
      "Epoch 162/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3802 - acc: 0.8456 - val_loss: 0.3867 - val_acc: 0.8407\n",
      "Epoch 163/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3839 - acc: 0.8389 - val_loss: 0.3806 - val_acc: 0.8373\n",
      "Epoch 164/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3815 - acc: 0.8406 - val_loss: 0.3812 - val_acc: 0.8373\n",
      "Epoch 165/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3830 - acc: 0.8389 - val_loss: 0.3939 - val_acc: 0.8305\n",
      "Epoch 166/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.4121 - acc: 0.810 - 0s 22us/sample - loss: 0.3839 - acc: 0.8356 - val_loss: 0.3849 - val_acc: 0.8508\n",
      "Epoch 167/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3834 - acc: 0.8339 - val_loss: 0.3893 - val_acc: 0.8441\n",
      "Epoch 168/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3792 - acc: 0.8423 - val_loss: 0.3823 - val_acc: 0.8475\n",
      "Epoch 169/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3807 - acc: 0.8406 - val_loss: 0.3920 - val_acc: 0.8407\n",
      "Epoch 170/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3832 - acc: 0.8440 - val_loss: 0.3852 - val_acc: 0.8441\n",
      "Epoch 171/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3787 - acc: 0.8456 - val_loss: 0.3893 - val_acc: 0.8407\n",
      "Epoch 172/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3874 - acc: 0.8322 - val_loss: 0.3875 - val_acc: 0.8407\n",
      "Epoch 173/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3857 - acc: 0.8406 - val_loss: 0.3854 - val_acc: 0.8441\n",
      "Epoch 174/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3858 - acc: 0.8305 - val_loss: 0.3834 - val_acc: 0.8407\n",
      "Epoch 175/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3790 - acc: 0.8440 - val_loss: 0.3850 - val_acc: 0.8407\n",
      "Epoch 176/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3794 - acc: 0.8440 - val_loss: 0.3878 - val_acc: 0.8407\n",
      "Epoch 177/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3795 - acc: 0.8372 - val_loss: 0.4040 - val_acc: 0.8203\n",
      "Epoch 178/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3805 - acc: 0.8372 - val_loss: 0.3926 - val_acc: 0.8339\n",
      "Epoch 179/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3806 - acc: 0.8440 - val_loss: 0.3832 - val_acc: 0.8373\n",
      "Epoch 180/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3849 - acc: 0.8372 - val_loss: 0.3884 - val_acc: 0.8475\n",
      "Epoch 181/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3780 - acc: 0.8473 - val_loss: 0.3850 - val_acc: 0.8441\n",
      "Epoch 182/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3803 - acc: 0.8389 - val_loss: 0.3850 - val_acc: 0.8407\n",
      "Epoch 183/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3829 - acc: 0.8406 - val_loss: 0.3884 - val_acc: 0.8407\n",
      "Epoch 184/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3788 - acc: 0.8389 - val_loss: 0.3857 - val_acc: 0.8407\n",
      "Epoch 185/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3765 - acc: 0.8490 - val_loss: 0.3963 - val_acc: 0.8305\n",
      "Epoch 186/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3778 - acc: 0.8389 - val_loss: 0.3837 - val_acc: 0.8339\n",
      "Epoch 187/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3772 - acc: 0.8490 - val_loss: 0.3888 - val_acc: 0.8441\n",
      "Epoch 188/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3769 - acc: 0.8423 - val_loss: 0.3832 - val_acc: 0.8339\n",
      "Epoch 189/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3782 - acc: 0.8490 - val_loss: 0.3880 - val_acc: 0.8441\n",
      "Epoch 190/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3810 - acc: 0.8423 - val_loss: 0.3895 - val_acc: 0.8542\n",
      "Epoch 191/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3793 - acc: 0.8406 - val_loss: 0.3886 - val_acc: 0.8542\n",
      "Epoch 192/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3776 - acc: 0.8507 - val_loss: 0.3880 - val_acc: 0.8407\n",
      "Epoch 193/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3765 - acc: 0.8490 - val_loss: 0.3926 - val_acc: 0.8373\n",
      "Epoch 194/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3749 - acc: 0.8456 - val_loss: 0.3845 - val_acc: 0.8373\n",
      "Epoch 195/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3793 - acc: 0.8440 - val_loss: 0.3866 - val_acc: 0.8441\n",
      "Epoch 196/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3800 - acc: 0.8356 - val_loss: 0.3864 - val_acc: 0.8441\n",
      "Epoch 197/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3822 - acc: 0.8272 - val_loss: 0.3871 - val_acc: 0.8441\n",
      "Epoch 198/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3737 - acc: 0.8440 - val_loss: 0.3896 - val_acc: 0.8407\n",
      "Epoch 199/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3736 - acc: 0.8356 - val_loss: 0.3936 - val_acc: 0.8339\n",
      "Epoch 200/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3764 - acc: 0.8440 - val_loss: 0.3992 - val_acc: 0.8169\n",
      "Epoch 201/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3817 - acc: 0.8305 - val_loss: 0.3865 - val_acc: 0.8373\n",
      "Epoch 202/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3723 - acc: 0.8356 - val_loss: 0.3966 - val_acc: 0.8373\n",
      "Epoch 203/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3769 - acc: 0.8339 - val_loss: 0.3886 - val_acc: 0.8441\n",
      "Epoch 204/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3741 - acc: 0.8490 - val_loss: 0.3937 - val_acc: 0.8407\n",
      "Epoch 205/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3761 - acc: 0.8423 - val_loss: 0.3898 - val_acc: 0.8475\n",
      "Epoch 206/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3778 - acc: 0.8440 - val_loss: 0.3845 - val_acc: 0.8407\n",
      "Epoch 207/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3716 - acc: 0.8490 - val_loss: 0.3959 - val_acc: 0.8373\n",
      "Epoch 208/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3760 - acc: 0.8440 - val_loss: 0.3858 - val_acc: 0.8373\n",
      "Epoch 209/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3748 - acc: 0.8440 - val_loss: 0.3859 - val_acc: 0.8373\n",
      "Epoch 210/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3769 - acc: 0.8406 - val_loss: 0.3875 - val_acc: 0.8339\n",
      "Epoch 211/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3723 - acc: 0.8440 - val_loss: 0.3856 - val_acc: 0.8373\n",
      "Epoch 212/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3745 - acc: 0.8423 - val_loss: 0.3875 - val_acc: 0.8339\n",
      "Epoch 213/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3730 - acc: 0.8423 - val_loss: 0.3900 - val_acc: 0.8475\n",
      "Epoch 214/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3711 - acc: 0.8423 - val_loss: 0.3851 - val_acc: 0.8373\n",
      "Epoch 215/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3739 - acc: 0.8423 - val_loss: 0.3874 - val_acc: 0.8373\n",
      "Epoch 216/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3741 - acc: 0.8423 - val_loss: 0.3937 - val_acc: 0.8441\n",
      "Epoch 217/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3714 - acc: 0.8440 - val_loss: 0.3861 - val_acc: 0.8373\n",
      "Epoch 218/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3789 - acc: 0.8423 - val_loss: 0.3971 - val_acc: 0.8475\n",
      "Epoch 219/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3728 - acc: 0.8389 - val_loss: 0.3848 - val_acc: 0.8305\n",
      "Epoch 220/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3729 - acc: 0.8440 - val_loss: 0.3908 - val_acc: 0.8339\n",
      "Epoch 221/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3768 - acc: 0.8406 - val_loss: 0.3939 - val_acc: 0.8407\n",
      "Epoch 222/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3706 - acc: 0.8490 - val_loss: 0.3874 - val_acc: 0.8373\n",
      "Epoch 223/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3752 - acc: 0.8456 - val_loss: 0.3917 - val_acc: 0.8441\n",
      "Epoch 224/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3750 - acc: 0.8473 - val_loss: 0.3892 - val_acc: 0.8339\n",
      "Epoch 225/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3700 - acc: 0.8523 - val_loss: 0.4004 - val_acc: 0.8339\n",
      "Epoch 226/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3757 - acc: 0.8490 - val_loss: 0.3893 - val_acc: 0.8407\n",
      "Epoch 227/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3724 - acc: 0.8406 - val_loss: 0.3874 - val_acc: 0.8339\n",
      "Epoch 228/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3681 - acc: 0.8473 - val_loss: 0.3890 - val_acc: 0.8407\n",
      "Epoch 229/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3714 - acc: 0.8473 - val_loss: 0.3895 - val_acc: 0.8305\n",
      "Epoch 230/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3711 - acc: 0.8523 - val_loss: 0.3872 - val_acc: 0.8373\n",
      "Epoch 231/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3694 - acc: 0.8456 - val_loss: 0.3961 - val_acc: 0.8373\n",
      "Epoch 232/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3676 - acc: 0.8473 - val_loss: 0.3925 - val_acc: 0.8339\n",
      "Epoch 233/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3732 - acc: 0.8456 - val_loss: 0.3973 - val_acc: 0.8407\n",
      "Epoch 234/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3750 - acc: 0.8356 - val_loss: 0.3945 - val_acc: 0.8339\n",
      "Epoch 235/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3694 - acc: 0.8406 - val_loss: 0.3934 - val_acc: 0.8339\n",
      "Epoch 236/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3720 - acc: 0.8356 - val_loss: 0.3908 - val_acc: 0.8407\n",
      "Epoch 237/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3713 - acc: 0.8356 - val_loss: 0.3911 - val_acc: 0.8441\n",
      "Epoch 238/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3702 - acc: 0.8490 - val_loss: 0.3897 - val_acc: 0.8305\n",
      "Epoch 239/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3681 - acc: 0.8473 - val_loss: 0.3996 - val_acc: 0.8373\n",
      "Epoch 240/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3675 - acc: 0.8473 - val_loss: 0.4000 - val_acc: 0.8373\n",
      "Epoch 241/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3642 - acc: 0.8456 - val_loss: 0.3929 - val_acc: 0.8441\n",
      "Epoch 242/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3727 - acc: 0.8389 - val_loss: 0.3891 - val_acc: 0.8305\n",
      "Epoch 243/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3717 - acc: 0.8440 - val_loss: 0.4044 - val_acc: 0.8339\n",
      "Epoch 244/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3689 - acc: 0.8389 - val_loss: 0.3967 - val_acc: 0.8305\n",
      "Epoch 245/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3662 - acc: 0.8557 - val_loss: 0.4079 - val_acc: 0.8339\n",
      "Epoch 246/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3697 - acc: 0.8456 - val_loss: 0.4068 - val_acc: 0.8339\n",
      "Epoch 247/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3664 - acc: 0.8423 - val_loss: 0.3929 - val_acc: 0.8339\n",
      "Epoch 248/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3735 - acc: 0.8456 - val_loss: 0.3935 - val_acc: 0.8441\n",
      "Epoch 249/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3674 - acc: 0.8372 - val_loss: 0.3983 - val_acc: 0.8373\n",
      "Epoch 250/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3657 - acc: 0.8389 - val_loss: 0.3940 - val_acc: 0.8305\n",
      "Epoch 251/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3670 - acc: 0.8440 - val_loss: 0.4011 - val_acc: 0.8407\n",
      "Epoch 252/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3813 - acc: 0.8255 - val_loss: 0.3975 - val_acc: 0.8339\n",
      "Epoch 253/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3693 - acc: 0.8372 - val_loss: 0.4009 - val_acc: 0.8339\n",
      "Epoch 254/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3710 - acc: 0.8523 - val_loss: 0.3924 - val_acc: 0.8271\n",
      "Epoch 255/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3653 - acc: 0.8440 - val_loss: 0.4056 - val_acc: 0.8271\n",
      "Epoch 256/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3759 - acc: 0.8490 - val_loss: 0.4018 - val_acc: 0.8271\n",
      "Epoch 257/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3740 - acc: 0.8423 - val_loss: 0.4021 - val_acc: 0.8407\n",
      "Epoch 258/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3660 - acc: 0.8389 - val_loss: 0.4173 - val_acc: 0.8237\n",
      "Epoch 259/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3668 - acc: 0.8440 - val_loss: 0.4382 - val_acc: 0.8068\n",
      "Epoch 260/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3721 - acc: 0.8440 - val_loss: 0.3950 - val_acc: 0.8441\n",
      "Epoch 261/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3650 - acc: 0.8440 - val_loss: 0.3980 - val_acc: 0.8441\n",
      "Epoch 262/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3712 - acc: 0.8423 - val_loss: 0.3999 - val_acc: 0.8373\n",
      "Epoch 263/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3678 - acc: 0.8456 - val_loss: 0.3936 - val_acc: 0.8373\n",
      "Epoch 264/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3639 - acc: 0.8473 - val_loss: 0.4107 - val_acc: 0.8407\n",
      "Epoch 265/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3678 - acc: 0.8473 - val_loss: 0.3953 - val_acc: 0.8237\n",
      "Epoch 266/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3729 - acc: 0.8440 - val_loss: 0.3926 - val_acc: 0.8441\n",
      "Epoch 267/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3654 - acc: 0.8423 - val_loss: 0.4004 - val_acc: 0.8475\n",
      "Epoch 268/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3669 - acc: 0.8523 - val_loss: 0.3948 - val_acc: 0.8339\n",
      "Epoch 269/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3623 - acc: 0.8456 - val_loss: 0.3988 - val_acc: 0.8475\n",
      "Epoch 270/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3640 - acc: 0.8389 - val_loss: 0.3994 - val_acc: 0.8373\n",
      "Epoch 271/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3791 - acc: 0.8372 - val_loss: 0.4112 - val_acc: 0.8339\n",
      "Epoch 272/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3682 - acc: 0.8356 - val_loss: 0.3978 - val_acc: 0.8271\n",
      "Epoch 273/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3641 - acc: 0.8440 - val_loss: 0.3940 - val_acc: 0.8305\n",
      "Epoch 274/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3730 - acc: 0.8473 - val_loss: 0.4018 - val_acc: 0.8373\n",
      "Epoch 275/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3649 - acc: 0.8456 - val_loss: 0.4033 - val_acc: 0.8475\n",
      "Epoch 276/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3628 - acc: 0.8473 - val_loss: 0.3985 - val_acc: 0.8305\n",
      "Epoch 277/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3657 - acc: 0.8473 - val_loss: 0.4165 - val_acc: 0.8237\n",
      "Epoch 278/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3649 - acc: 0.8423 - val_loss: 0.4067 - val_acc: 0.8407\n",
      "Epoch 279/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3653 - acc: 0.8406 - val_loss: 0.3974 - val_acc: 0.8373\n",
      "Epoch 280/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3614 - acc: 0.8456 - val_loss: 0.4032 - val_acc: 0.8339\n",
      "Epoch 281/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3671 - acc: 0.8406 - val_loss: 0.4084 - val_acc: 0.8407\n",
      "Epoch 282/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3668 - acc: 0.8473 - val_loss: 0.4011 - val_acc: 0.8475\n",
      "Epoch 283/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3650 - acc: 0.8372 - val_loss: 0.4257 - val_acc: 0.8237\n",
      "Epoch 284/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3673 - acc: 0.8523 - val_loss: 0.3959 - val_acc: 0.8339\n",
      "Epoch 285/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3683 - acc: 0.8456 - val_loss: 0.4064 - val_acc: 0.8407\n",
      "Epoch 286/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3738 - acc: 0.8473 - val_loss: 0.4064 - val_acc: 0.8407\n",
      "Epoch 287/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3609 - acc: 0.8507 - val_loss: 0.3955 - val_acc: 0.8339\n",
      "Epoch 288/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3672 - acc: 0.8423 - val_loss: 0.4011 - val_acc: 0.8339\n",
      "Epoch 289/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3624 - acc: 0.8456 - val_loss: 0.4107 - val_acc: 0.8441\n",
      "Epoch 290/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3642 - acc: 0.8473 - val_loss: 0.3951 - val_acc: 0.8373\n",
      "Epoch 291/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3627 - acc: 0.8440 - val_loss: 0.4014 - val_acc: 0.8475\n",
      "Epoch 292/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3641 - acc: 0.8456 - val_loss: 0.4046 - val_acc: 0.8373\n",
      "Epoch 293/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3681 - acc: 0.8406 - val_loss: 0.4071 - val_acc: 0.8475\n",
      "Epoch 294/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3581 - acc: 0.8490 - val_loss: 0.4024 - val_acc: 0.8508\n",
      "Epoch 295/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3607 - acc: 0.8423 - val_loss: 0.4166 - val_acc: 0.8305\n",
      "Epoch 296/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3668 - acc: 0.8372 - val_loss: 0.3987 - val_acc: 0.8271\n",
      "Epoch 297/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3669 - acc: 0.8389 - val_loss: 0.3962 - val_acc: 0.8475\n",
      "Epoch 298/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3603 - acc: 0.8507 - val_loss: 0.4193 - val_acc: 0.8305\n",
      "Epoch 299/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3614 - acc: 0.8389 - val_loss: 0.4053 - val_acc: 0.8407\n",
      "Epoch 300/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3611 - acc: 0.8456 - val_loss: 0.4068 - val_acc: 0.8136\n",
      "Epoch 301/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3709 - acc: 0.8423 - val_loss: 0.4166 - val_acc: 0.8305\n",
      "Epoch 302/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3689 - acc: 0.8372 - val_loss: 0.4293 - val_acc: 0.8339\n",
      "Epoch 303/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3678 - acc: 0.8490 - val_loss: 0.3997 - val_acc: 0.8407\n",
      "Epoch 304/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3595 - acc: 0.8440 - val_loss: 0.3974 - val_acc: 0.8305\n",
      "Epoch 305/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3686 - acc: 0.8440 - val_loss: 0.4071 - val_acc: 0.8169\n",
      "Epoch 306/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3605 - acc: 0.8440 - val_loss: 0.4149 - val_acc: 0.8407\n",
      "Epoch 307/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3644 - acc: 0.8490 - val_loss: 0.4053 - val_acc: 0.8407\n",
      "Epoch 308/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3591 - acc: 0.8440 - val_loss: 0.4007 - val_acc: 0.8271\n",
      "Epoch 309/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3687 - acc: 0.8372 - val_loss: 0.4261 - val_acc: 0.8034\n",
      "Epoch 310/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3612 - acc: 0.8389 - val_loss: 0.4223 - val_acc: 0.8237\n",
      "Epoch 311/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3626 - acc: 0.8456 - val_loss: 0.3998 - val_acc: 0.8305\n",
      "Epoch 312/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3642 - acc: 0.8440 - val_loss: 0.4163 - val_acc: 0.8339\n",
      "Epoch 313/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3757 - acc: 0.8356 - val_loss: 0.4101 - val_acc: 0.8339\n",
      "Epoch 314/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3665 - acc: 0.8440 - val_loss: 0.3975 - val_acc: 0.8407\n",
      "Epoch 315/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3628 - acc: 0.8456 - val_loss: 0.4032 - val_acc: 0.8373\n",
      "Epoch 316/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3567 - acc: 0.8456 - val_loss: 0.4253 - val_acc: 0.8169\n",
      "Epoch 317/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3682 - acc: 0.8356 - val_loss: 0.4293 - val_acc: 0.8102\n",
      "Epoch 318/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3710 - acc: 0.8339 - val_loss: 0.4083 - val_acc: 0.8237\n",
      "Epoch 319/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3597 - acc: 0.8423 - val_loss: 0.4010 - val_acc: 0.8373\n",
      "Epoch 320/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3687 - acc: 0.8490 - val_loss: 0.4030 - val_acc: 0.8373\n",
      "Epoch 321/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3601 - acc: 0.8456 - val_loss: 0.4035 - val_acc: 0.8373\n",
      "Epoch 322/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3575 - acc: 0.8473 - val_loss: 0.4049 - val_acc: 0.8237\n",
      "Epoch 323/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3809 - acc: 0.8305 - val_loss: 0.4091 - val_acc: 0.8271\n",
      "Epoch 324/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3561 - acc: 0.8540 - val_loss: 0.4407 - val_acc: 0.8339\n",
      "Epoch 325/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3568 - acc: 0.8490 - val_loss: 0.4212 - val_acc: 0.8373\n",
      "Epoch 326/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3587 - acc: 0.8490 - val_loss: 0.4250 - val_acc: 0.8305\n",
      "Epoch 327/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3662 - acc: 0.8389 - val_loss: 0.4031 - val_acc: 0.8441\n",
      "Epoch 328/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3897 - acc: 0.850 - 0s 30us/sample - loss: 0.3615 - acc: 0.8473 - val_loss: 0.4065 - val_acc: 0.8339\n",
      "Epoch 329/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3580 - acc: 0.8423 - val_loss: 0.4141 - val_acc: 0.8475\n",
      "Epoch 330/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3580 - acc: 0.8456 - val_loss: 0.4090 - val_acc: 0.8407\n",
      "Epoch 331/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3569 - acc: 0.8507 - val_loss: 0.4211 - val_acc: 0.8339\n",
      "Epoch 332/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3695 - acc: 0.8406 - val_loss: 0.4068 - val_acc: 0.8339\n",
      "Epoch 333/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3563 - acc: 0.8540 - val_loss: 0.4184 - val_acc: 0.8441\n",
      "Epoch 334/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3553 - acc: 0.8507 - val_loss: 0.4029 - val_acc: 0.8339\n",
      "Epoch 335/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3577 - acc: 0.8440 - val_loss: 0.4123 - val_acc: 0.8441\n",
      "Epoch 336/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3599 - acc: 0.8440 - val_loss: 0.4347 - val_acc: 0.8271\n",
      "Epoch 337/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3592 - acc: 0.8456 - val_loss: 0.4343 - val_acc: 0.8305\n",
      "Epoch 338/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3593 - acc: 0.8540 - val_loss: 0.4037 - val_acc: 0.8373\n",
      "Epoch 339/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3619 - acc: 0.8372 - val_loss: 0.4069 - val_acc: 0.8305\n",
      "Epoch 340/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3629 - acc: 0.8389 - val_loss: 0.4033 - val_acc: 0.8407\n",
      "Epoch 341/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3640 - acc: 0.8372 - val_loss: 0.4138 - val_acc: 0.8441\n",
      "Epoch 342/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3601 - acc: 0.8423 - val_loss: 0.4463 - val_acc: 0.8102\n",
      "Epoch 343/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3687 - acc: 0.8372 - val_loss: 0.4013 - val_acc: 0.8339\n",
      "Epoch 344/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3558 - acc: 0.8507 - val_loss: 0.4021 - val_acc: 0.8373\n",
      "Epoch 345/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3602 - acc: 0.8490 - val_loss: 0.4319 - val_acc: 0.8373\n",
      "Epoch 346/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3604 - acc: 0.8440 - val_loss: 0.4079 - val_acc: 0.8373\n",
      "Epoch 347/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3608 - acc: 0.8456 - val_loss: 0.4059 - val_acc: 0.8305\n",
      "Epoch 348/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3568 - acc: 0.8456 - val_loss: 0.4218 - val_acc: 0.8441\n",
      "Epoch 349/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3537 - acc: 0.8523 - val_loss: 0.4318 - val_acc: 0.8305\n",
      "Epoch 350/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3668 - acc: 0.8406 - val_loss: 0.4088 - val_acc: 0.8305\n",
      "Epoch 351/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3716 - acc: 0.8389 - val_loss: 0.4186 - val_acc: 0.8475\n",
      "Epoch 352/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3643 - acc: 0.8406 - val_loss: 0.4089 - val_acc: 0.8407\n",
      "Epoch 353/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3668 - acc: 0.8389 - val_loss: 0.4066 - val_acc: 0.8475\n",
      "Epoch 354/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3563 - acc: 0.8557 - val_loss: 0.4130 - val_acc: 0.8475\n",
      "Epoch 355/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3811 - acc: 0.8255 - val_loss: 0.4242 - val_acc: 0.8373\n",
      "Epoch 356/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3566 - acc: 0.8389 - val_loss: 0.4371 - val_acc: 0.8271\n",
      "Epoch 357/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3558 - acc: 0.8540 - val_loss: 0.4281 - val_acc: 0.8271\n",
      "Epoch 358/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3588 - acc: 0.8372 - val_loss: 0.4302 - val_acc: 0.8102\n",
      "Epoch 359/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3559 - acc: 0.8406 - val_loss: 0.4411 - val_acc: 0.8271\n",
      "Epoch 360/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3584 - acc: 0.8473 - val_loss: 0.4513 - val_acc: 0.8102\n",
      "Epoch 361/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3592 - acc: 0.8356 - val_loss: 0.4014 - val_acc: 0.8407\n",
      "Epoch 362/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3629 - acc: 0.8423 - val_loss: 0.4033 - val_acc: 0.8305\n",
      "Epoch 363/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3551 - acc: 0.8423 - val_loss: 0.4096 - val_acc: 0.8407\n",
      "Epoch 364/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3572 - acc: 0.8423 - val_loss: 0.4018 - val_acc: 0.8305\n",
      "Epoch 365/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3619 - acc: 0.8507 - val_loss: 0.4203 - val_acc: 0.8407\n",
      "Epoch 366/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3525 - acc: 0.8540 - val_loss: 0.4120 - val_acc: 0.8373\n",
      "Epoch 367/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3554 - acc: 0.8456 - val_loss: 0.4176 - val_acc: 0.8441\n",
      "Epoch 368/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3570 - acc: 0.8523 - val_loss: 0.4208 - val_acc: 0.8441\n",
      "Epoch 369/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3596 - acc: 0.8490 - val_loss: 0.4176 - val_acc: 0.8441\n",
      "Epoch 370/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3535 - acc: 0.8389 - val_loss: 0.4186 - val_acc: 0.8237\n",
      "Epoch 371/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3646 - acc: 0.8389 - val_loss: 0.4088 - val_acc: 0.8407\n",
      "Epoch 372/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3537 - acc: 0.8507 - val_loss: 0.4149 - val_acc: 0.8339\n",
      "Epoch 373/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3510 - acc: 0.8507 - val_loss: 0.4430 - val_acc: 0.8203\n",
      "Epoch 374/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3551 - acc: 0.8440 - val_loss: 0.4179 - val_acc: 0.8407\n",
      "Epoch 375/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3636 - acc: 0.8339 - val_loss: 0.4037 - val_acc: 0.8373\n",
      "Epoch 376/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3577 - acc: 0.8456 - val_loss: 0.4037 - val_acc: 0.8407\n",
      "Epoch 377/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3564 - acc: 0.8507 - val_loss: 0.4219 - val_acc: 0.8407\n",
      "Epoch 378/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3515 - acc: 0.8507 - val_loss: 0.4341 - val_acc: 0.8339\n",
      "Epoch 379/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3561 - acc: 0.8473 - val_loss: 0.4197 - val_acc: 0.8441\n",
      "Epoch 380/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3643 - acc: 0.8456 - val_loss: 0.4197 - val_acc: 0.8203\n",
      "Epoch 381/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3593 - acc: 0.8490 - val_loss: 0.4145 - val_acc: 0.8339\n",
      "Epoch 382/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3486 - acc: 0.8490 - val_loss: 0.4440 - val_acc: 0.8508\n",
      "Epoch 383/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3539 - acc: 0.8456 - val_loss: 0.4241 - val_acc: 0.8203\n",
      "Epoch 384/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3555 - acc: 0.8423 - val_loss: 0.4066 - val_acc: 0.8339\n",
      "Epoch 385/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3635 - acc: 0.8389 - val_loss: 0.4367 - val_acc: 0.8339\n",
      "Epoch 386/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3538 - acc: 0.8473 - val_loss: 0.4141 - val_acc: 0.8203\n",
      "Epoch 387/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3825 - acc: 0.8356 - val_loss: 0.4210 - val_acc: 0.8508\n",
      "Epoch 388/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3601 - acc: 0.8607 - val_loss: 0.4224 - val_acc: 0.8475\n",
      "Epoch 389/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3532 - acc: 0.8473 - val_loss: 0.4271 - val_acc: 0.8407\n",
      "Epoch 390/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3501 - acc: 0.8507 - val_loss: 0.4097 - val_acc: 0.8339\n",
      "Epoch 391/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3560 - acc: 0.8523 - val_loss: 0.4223 - val_acc: 0.8373\n",
      "Epoch 392/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3527 - acc: 0.8473 - val_loss: 0.4199 - val_acc: 0.8508\n",
      "Epoch 393/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3465 - acc: 0.8591 - val_loss: 0.4277 - val_acc: 0.8271\n",
      "Epoch 394/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3714 - acc: 0.8406 - val_loss: 0.4301 - val_acc: 0.8271\n",
      "Epoch 395/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3517 - acc: 0.8624 - val_loss: 0.4086 - val_acc: 0.8441\n",
      "Epoch 396/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3849 - acc: 0.8305 - val_loss: 0.4121 - val_acc: 0.8373\n",
      "Epoch 397/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3125 - acc: 0.880 - 0s 30us/sample - loss: 0.3555 - acc: 0.8372 - val_loss: 0.4103 - val_acc: 0.8237\n",
      "Epoch 398/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3593 - acc: 0.8456 - val_loss: 0.4154 - val_acc: 0.8373\n",
      "Epoch 399/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3645 - acc: 0.8490 - val_loss: 0.4460 - val_acc: 0.8136\n",
      "Epoch 400/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3590 - acc: 0.8423 - val_loss: 0.4218 - val_acc: 0.8542\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "#label_train = encoder.fit_transform(y_train)\n",
    "label_test = encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(20, activation='relu', input_shape=(9,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = tf.train.GradientDescentOptimizer(0.1),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    model.fit(X_train, y_train, epochs=400, batch_size=100, validation_data = (X_test, y_test))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing Cost Estimator\n",
    "\n",
    "Building off the classifier examples above, this section shows ensemble regressors using bagging and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,) (167, 13) (167,)\n"
     ]
    }
   ],
   "source": [
    "# Load Data Set\n",
    "from sklearn import datasets\n",
    "boston_housing_data = datasets.load_boston()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bouston_housing_data_instances = scaler.fit_transform(boston_housing_data.data)\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    \"\"\"\n",
    "    Plots performance on the training set and testing (validation) set.\n",
    "    X-axis - number of training samples used\n",
    "    Y-axis - RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.20)\n",
    "    \n",
    "    training_errors, validation_errors = [], []\n",
    "    \n",
    "    for m in range(1, len(train_X)):\n",
    "        \n",
    "        model.fit(train_X[:m], train_y[:m])\n",
    "        \n",
    "        train_pred = model.predict(train_X)\n",
    "        test_pred = model.predict(test_X)\n",
    "        \n",
    "        training_errors.append(np.sqrt(mean_squared_error(train_y, train_pred)))\n",
    "        validation_errors.append(np.sqrt(mean_squared_error(test_y, test_pred)))\n",
    "        \n",
    "    plt.plot(training_errors, \"r-+\", label=\"train\")\n",
    "    plt.plot(validation_errors, \"b-\", label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.axis([0, 80, 0, 3])\n",
    "    \n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Train: 463.00775146484375 Test:552.0066528320312\n",
      "1-Train: 411.280517578125 Test:495.46173095703125\n",
      "2-Train: 366.30853271484375 Test:445.9897155761719\n",
      "3-Train: 327.220703125 Test:402.6980285644531\n",
      "4-Train: 293.2575378417969 Test:364.8068542480469\n",
      "5-Train: 263.7569580078125 Test:331.6368408203125\n",
      "6-Train: 238.14178466796875 Test:302.5943298339844\n",
      "7-Train: 215.9088592529297 Test:277.1589050292969\n",
      "8-Train: 196.61973571777344 Test:254.87818908691406\n",
      "9-Train: 179.8921661376953 Test:235.35629272460938\n",
      "10-Train: 165.39317321777344 Test:218.2465362548828\n",
      "11-Train: 152.83251953125 Test:203.24673461914062\n",
      "12-Train: 141.95745849609375 Test:190.0932159423828\n",
      "13-Train: 132.54766845703125 Test:178.5550994873047\n",
      "14-Train: 124.41130065917969 Test:168.42994689941406\n",
      "15-Train: 117.38134002685547 Test:159.54254150390625\n",
      "16-Train: 111.31227111816406 Test:151.7383575439453\n",
      "17-Train: 106.07746124267578 Test:144.8821258544922\n",
      "18-Train: 101.56668090820312 Test:138.8567657470703\n",
      "19-Train: 97.68391418457031 Test:133.5591583251953\n",
      "20-Train: 94.34575653076172 Test:128.89923095703125\n",
      "21-Train: 91.4795150756836 Test:124.79830169677734\n",
      "22-Train: 89.02202606201172 Test:121.18726348876953\n",
      "23-Train: 86.91838073730469 Test:118.00591278076172\n",
      "24-Train: 85.12092590332031 Test:115.20183563232422\n",
      "25-Train: 83.58804321289062 Test:112.7284164428711\n",
      "26-Train: 82.28380584716797 Test:110.545654296875\n",
      "27-Train: 81.17683410644531 Test:108.61785125732422\n",
      "28-Train: 80.239990234375 Test:106.91426849365234\n",
      "29-Train: 79.44966125488281 Test:105.40770721435547\n",
      "30-Train: 78.78548431396484 Test:104.07430267333984\n",
      "31-Train: 78.22966003417969 Test:102.89348602294922\n",
      "32-Train: 77.76683044433594 Test:101.8465805053711\n",
      "33-Train: 77.38369750976562 Test:100.91790771484375\n",
      "34-Train: 77.06876373291016 Test:100.09324645996094\n",
      "35-Train: 76.81207275390625 Test:99.36038970947266\n",
      "36-Train: 76.60499572753906 Test:98.70845794677734\n",
      "37-Train: 76.44013214111328 Test:98.1281509399414\n",
      "38-Train: 76.3110580444336 Test:97.61083221435547\n",
      "39-Train: 76.21221923828125 Test:97.1492919921875\n",
      "40-Train: 76.13887023925781 Test:96.7373046875\n",
      "41-Train: 76.08694458007812 Test:96.36893463134766\n",
      "42-Train: 76.05289459228516 Test:96.0392837524414\n",
      "43-Train: 76.03370666503906 Test:95.74378967285156\n",
      "44-Train: 76.02680206298828 Test:95.47882843017578\n",
      "45-Train: 76.02994537353516 Test:95.24097442626953\n",
      "46-Train: 76.04129028320312 Test:95.02706909179688\n",
      "47-Train: 76.05921173095703 Test:94.8345947265625\n",
      "48-Train: 76.08233642578125 Test:94.6611557006836\n",
      "49-Train: 76.10950469970703 Test:94.5048599243164\n",
      "50-Train: 76.13977813720703 Test:94.36358642578125\n",
      "51-Train: 76.17230224609375 Test:94.2358627319336\n",
      "52-Train: 76.20637512207031 Test:94.12035369873047\n",
      "53-Train: 76.2414779663086 Test:94.01557922363281\n",
      "54-Train: 76.27706909179688 Test:93.92044067382812\n",
      "55-Train: 76.31277465820312 Test:93.83409118652344\n",
      "56-Train: 76.34830474853516 Test:93.75565338134766\n",
      "57-Train: 76.38335418701172 Test:93.6842269897461\n",
      "58-Train: 76.41775512695312 Test:93.6190414428711\n",
      "59-Train: 76.45130920410156 Test:93.55970001220703\n",
      "60-Train: 76.48389434814453 Test:93.50548553466797\n",
      "61-Train: 76.51544189453125 Test:93.45587921142578\n",
      "62-Train: 76.54585266113281 Test:93.41062927246094\n",
      "63-Train: 76.57510375976562 Test:93.36888885498047\n",
      "64-Train: 76.6031265258789 Test:93.33091735839844\n",
      "65-Train: 76.62995910644531 Test:93.29589080810547\n",
      "66-Train: 76.65557861328125 Test:93.26383209228516\n",
      "67-Train: 76.67999267578125 Test:93.23442840576172\n",
      "68-Train: 76.70320892333984 Test:93.20716094970703\n",
      "69-Train: 76.72525787353516 Test:93.18206024169922\n",
      "70-Train: 76.74618530273438 Test:93.1591567993164\n",
      "71-Train: 76.76602935791016 Test:93.137939453125\n",
      "72-Train: 76.7847900390625 Test:93.11830139160156\n",
      "73-Train: 76.80253601074219 Test:93.1002197265625\n",
      "74-Train: 76.81929779052734 Test:93.08345031738281\n",
      "75-Train: 76.83513641357422 Test:93.06805419921875\n",
      "76-Train: 76.85005187988281 Test:93.05377197265625\n",
      "77-Train: 76.86412811279297 Test:93.04044342041016\n",
      "78-Train: 76.87738800048828 Test:93.0283203125\n",
      "79-Train: 76.88983917236328 Test:93.01692199707031\n",
      "80-Train: 76.90158081054688 Test:93.00636291503906\n",
      "81-Train: 76.91261291503906 Test:92.99655151367188\n",
      "82-Train: 76.92296600341797 Test:92.98744201660156\n",
      "83-Train: 76.93270111083984 Test:92.97918701171875\n",
      "84-Train: 76.94184875488281 Test:92.97144317626953\n",
      "85-Train: 76.95042419433594 Test:92.96417999267578\n",
      "86-Train: 76.95848083496094 Test:92.95732879638672\n",
      "87-Train: 76.96604919433594 Test:92.95108032226562\n",
      "88-Train: 76.97313690185547 Test:92.94528198242188\n",
      "89-Train: 76.97978973388672 Test:92.93972778320312\n",
      "90-Train: 76.98601531982422 Test:92.93476104736328\n",
      "91-Train: 76.99186706542969 Test:92.93009948730469\n",
      "92-Train: 76.99732971191406 Test:92.9258041381836\n",
      "93-Train: 77.00245666503906 Test:92.92158508300781\n",
      "94-Train: 77.00726318359375 Test:92.91803741455078\n",
      "95-Train: 77.01173400878906 Test:92.91436004638672\n",
      "96-Train: 77.01593780517578 Test:92.91116333007812\n",
      "97-Train: 77.01990509033203 Test:92.90806579589844\n",
      "98-Train: 77.02360534667969 Test:92.90528869628906\n",
      "99-Train: 77.02706146240234 Test:92.90254211425781\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(boston_housing_data.data,\n",
    "                                                   boston_housing_data.target,\n",
    "                                                   test_size=0.33)\n",
    "\n",
    "\n",
    "def get_batch(X, iter, size):\n",
    "    return X[(iter*batch_size) : ((iter+1)*batch_size)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_features = train_X.shape[1]\n",
    "num_instances = train_y.shape[0]\n",
    "\n",
    "# Construction\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_features), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Boston-MLP\"):\n",
    "    hidden1 = tf.layers.dense(X, 3, name=\"Hidden-1\", activation = tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(X, 2, name=\"Hidden-2\", activation = tf.nn.relu)\n",
    "    output = tf.layers.dense(hidden2, 1, name=\"Price\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.square(y-output))\n",
    "    \n",
    "with tf.name_scope(\"train\"): \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for iteration in range(num_instances // batch_size + 1):\n",
    "            X_batch = get_batch(train_X, iteration, batch_size)\n",
    "            y_batch = get_batch(train_y, iteration, batch_size)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={X: X_batch,\n",
    "                                            y: y_batch})\n",
    "            \n",
    "        mse_train = loss.eval(feed_dict={X: X_batch,\n",
    "                                            y: y_batch})\n",
    "        mse_val = loss.eval(feed_dict={X:test_X, y: test_y})\n",
    "        \n",
    "        print(\"{}-Train: {} Test:{}\".format(epoch,\n",
    "                                           mse_train,\n",
    "                                           mse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example, you will see that the model converges early. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 339 samples, validate on 167 samples\n",
      "Epoch 1/400\n",
      "339/339 [==============================] - 0s 571us/sample - loss: 78132071609060960.0000 - mean_squared_error: 78132069363875840.0000 - val_loss: 9149771133001.5801 - val_mean_squared_error: 9149771415552.0000\n",
      "Epoch 2/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 8573056322967.7871 - mean_squared_error: 8573057236992.0000 - val_loss: 7985221274299.0176 - val_mean_squared_error: 7985220812800.0000\n",
      "Epoch 3/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 7481909470513.0859 - mean_squared_error: 7481909051392.0000 - val_loss: 6968894499171.6406 - val_mean_squared_error: 6968894816256.0000\n",
      "Epoch 4/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 6529640800860.1289 - mean_squared_error: 6529640038400.0000 - val_loss: 6081919845921.7246 - val_mean_squared_error: 6081919057920.0000\n",
      "Epoch 5/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 5698572055724.1768 - mean_squared_error: 5698571730944.0000 - val_loss: 5307836052706.8740 - val_mean_squared_error: 5307836137472.0000\n",
      "Epoch 6/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 4973278935783.0801 - mean_squared_error: 4973278003200.0000 - val_loss: 4632274300511.0420 - val_mean_squared_error: 4632274272256.0000\n",
      "Epoch 7/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 4340296339468.0825 - mean_squared_error: 4340296450048.0000 - val_loss: 4042693323696.2871 - val_mean_squared_error: 4042693541888.0000\n",
      "Epoch 8/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 3787877601872.0474 - mean_squared_error: 3787878039552.0000 - val_loss: 3528151768548.4072 - val_mean_squared_error: 3528151793664.0000\n",
      "Epoch 9/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 3305769097306.6191 - mean_squared_error: 3305769271296.0000 - val_loss: 3079102396109.4131 - val_mean_squared_error: 3079102267392.0000\n",
      "Epoch 10/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 2885023230876.3188 - mean_squared_error: 2885022908416.0000 - val_loss: 2687206113770.5391 - val_mean_squared_error: 2687206162432.0000\n",
      "Epoch 11/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 2517829044737.5103 - mean_squared_error: 2517829156864.0000 - val_loss: 2345189434864.6709 - val_mean_squared_error: 2345189507072.0000\n",
      "Epoch 12/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 2197369066209.0383 - mean_squared_error: 2197369126912.0000 - val_loss: 2046701844075.3057 - val_mean_squared_error: 2046701862912.0000\n",
      "Epoch 13/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 1917695653519.4810 - mean_squared_error: 1917695819776.0000 - val_loss: 1786204846165.8442 - val_mean_squared_error: 1786204651520.0000\n",
      "Epoch 14/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 1673618265767.6462 - mean_squared_error: 1673618259968.0000 - val_loss: 1558863665973.6526 - val_mean_squared_error: 1558863544320.0000\n",
      "Epoch 15/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 1460606001804.4602 - mean_squared_error: 1460606074880.0000 - val_loss: 1360456876228.2156 - val_mean_squared_error: 1360456974336.0000\n",
      "Epoch 16/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 1274705219354.4307 - mean_squared_error: 1274705215488.0000 - val_loss: 1187303053741.2214 - val_mean_squared_error: 1187303129088.0000\n",
      "Epoch 17/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 1112465377648.5193 - mean_squared_error: 1112465473536.0000 - val_loss: 1036187461889.5330 - val_mean_squared_error: 1036187467776.0000\n",
      "Epoch 18/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 970874654587.0914 - mean_squared_error: 970874552320.0000 - val_loss: 904305418485.2694 - val_mean_squared_error: 904305377280.0000\n",
      "Epoch 19/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 847305286429.4513 - mean_squared_error: 847305244672.0000 - val_loss: 789208915722.7306 - val_mean_squared_error: 789208956928.0000\n",
      "Epoch 20/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 739463497323.2330 - mean_squared_error: 739463462912.0000 - val_loss: 688761489420.2634 - val_mean_squared_error: 688761536512.0000\n",
      "Epoch 21/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 645347508513.9823 - mean_squared_error: 645347409920.0000 - val_loss: 601098804414.0839 - val_mean_squared_error: 601098747904.0000\n",
      "Epoch 22/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 563210082587.9410 - mean_squared_error: 563210158080.0000 - val_loss: 524593272574.4670 - val_mean_squared_error: 524593266688.0000\n",
      "Epoch 23/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 491526701007.6696 - mean_squared_error: 491526750208.0000 - val_loss: 457824769398.0359 - val_mean_squared_error: 457824763904.0000\n",
      "Epoch 24/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 428966854027.7050 - mean_squared_error: 428966838272.0000 - val_loss: 399554585213.7006 - val_mean_squared_error: 399554609152.0000\n",
      "Epoch 25/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 374369568314.9026 - mean_squared_error: 374369583104.0000 - val_loss: 348701016750.7545 - val_mean_squared_error: 348700999680.0000\n",
      "Epoch 26/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 326721403864.7316 - mean_squared_error: 326721339392.0000 - val_loss: 304319794635.8802 - val_mean_squared_error: 304319791104.0000\n",
      "Epoch 27/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 285137545898.6667 - mean_squared_error: 285137534976.0000 - val_loss: 265587196983.1857 - val_mean_squared_error: 265587195904.0000\n",
      "Epoch 28/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 248846247458.7375 - mean_squared_error: 248846221312.0000 - val_loss: 231784255218.2036 - val_mean_squared_error: 231784251392.0000\n",
      "Epoch 29/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 217174034993.8407 - mean_squared_error: 217174032384.0000 - val_loss: 202283660416.7665 - val_mean_squared_error: 202283646976.0000\n",
      "Epoch 30/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 189532898343.2684 - mean_squared_error: 189532897280.0000 - val_loss: 176537810441.1976 - val_mean_squared_error: 176537796608.0000\n",
      "Epoch 31/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 165409841127.8348 - mean_squared_error: 165409849344.0000 - val_loss: 154068753205.6527 - val_mean_squared_error: 154068746240.0000\n",
      "Epoch 32/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 144357101314.2655 - mean_squared_error: 144357097472.0000 - val_loss: 134459491432.2395 - val_mean_squared_error: 134459490304.0000\n",
      "Epoch 33/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 125983854894.0649 - mean_squared_error: 125983866880.0000 - val_loss: 117346021989.1737 - val_mean_squared_error: 117346017280.0000\n",
      "Epoch 34/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 109949111075.4926 - mean_squared_error: 109949116416.0000 - val_loss: 102410756488.4311 - val_mean_squared_error: 102410756096.0000\n",
      "Epoch 35/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 95955240047.7640 - mean_squared_error: 95955230720.0000 - val_loss: 89376370565.3653 - val_mean_squared_error: 89376374784.0000\n",
      "Epoch 36/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 83742441979.4690 - mean_squared_error: 83742449664.0000 - val_loss: 78000929491.5449 - val_mean_squared_error: 78000930816.0000\n",
      "Epoch 37/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 73084030299.3746 - mean_squared_error: 73084035072.0000 - val_loss: 68073307963.7844 - val_mean_squared_error: 68073308160.0000\n",
      "Epoch 38/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 63782175674.5251 - mean_squared_error: 63782178816.0000 - val_loss: 59409231258.8263 - val_mean_squared_error: 59409231872.0000\n",
      "Epoch 39/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 100us/sample - loss: 55664213593.1091 - mean_squared_error: 55664218112.0000 - val_loss: 51847857182.6587 - val_mean_squared_error: 51847856128.0000\n",
      "Epoch 40/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 48579478096.0472 - mean_squared_error: 48579485696.0000 - val_loss: 45248891554.4910 - val_mean_squared_error: 45248892928.0000\n",
      "Epoch 41/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 42396470812.6962 - mean_squared_error: 42396475392.0000 - val_loss: 39489817324.0719 - val_mean_squared_error: 39489814528.0000\n",
      "Epoch 42/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 37000411274.9499 - mean_squared_error: 37000413184.0000 - val_loss: 34463721337.1018 - val_mean_squared_error: 34463719424.0000\n",
      "Epoch 43/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 32291127465.1563 - mean_squared_error: 32291129344.0000 - val_loss: 30077318002.9701 - val_mean_squared_error: 30077317120.0000\n",
      "Epoch 44/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 28181225018.9027 - mean_squared_error: 28181227520.0000 - val_loss: 26249197764.2156 - val_mean_squared_error: 26249197568.0000\n",
      "Epoch 45/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 24594415942.2301 - mean_squared_error: 24594415616.0000 - val_loss: 22908315525.3653 - val_mean_squared_error: 22908315648.0000\n",
      "Epoch 46/400\n",
      "339/339 [==============================] - ETA: 0s - loss: 22908571648.0000 - mean_squared_error: 22908571648.00 - 0s 106us/sample - loss: 21464126140.7906 - mean_squared_error: 21464127488.0000 - val_loss: 19992646318.7545 - val_mean_squared_error: 19992645632.0000\n",
      "Epoch 47/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 18732256845.0266 - mean_squared_error: 18732257280.0000 - val_loss: 17448080071.2814 - val_mean_squared_error: 17448081408.0000\n",
      "Epoch 48/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 16348089035.8938 - mean_squared_error: 16348088320.0000 - val_loss: 15227371630.3713 - val_mean_squared_error: 15227371520.0000\n",
      "Epoch 49/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 14267368892.0354 - mean_squared_error: 14267368448.0000 - val_loss: 13289306246.8982 - val_mean_squared_error: 13289305088.0000\n",
      "Epoch 50/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 12451472719.2920 - mean_squared_error: 12451473408.0000 - val_loss: 11597908918.4192 - val_mean_squared_error: 11597908992.0000\n",
      "Epoch 51/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 10866696512.1888 - mean_squared_error: 10866696192.0000 - val_loss: 10121784902.5150 - val_mean_squared_error: 10121786368.0000\n",
      "Epoch 52/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 9483625372.3186 - mean_squared_error: 9483625472.0000 - val_loss: 8833533252.9820 - val_mean_squared_error: 8833532928.0000\n",
      "Epoch 53/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 8276583780.4366 - mean_squared_error: 8276584448.0000 - val_loss: 7709245200.8623 - val_mean_squared_error: 7709245440.0000\n",
      "Epoch 54/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 7223170318.3481 - mean_squared_error: 7223170048.0000 - val_loss: 6728049884.7425 - val_mean_squared_error: 6728049664.0000\n",
      "Epoch 55/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 6303829082.6195 - mean_squared_error: 6303829504.0000 - val_loss: 5871737077.2695 - val_mean_squared_error: 5871736832.0000\n",
      "Epoch 56/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 5501499509.8053 - mean_squared_error: 5501498880.0000 - val_loss: 5124410410.9222 - val_mean_squared_error: 5124410368.0000\n",
      "Epoch 57/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 4801289353.4395 - mean_squared_error: 4801288704.0000 - val_loss: 4472204444.3593 - val_mean_squared_error: 4472204800.0000\n",
      "Epoch 58/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 4190198354.3127 - mean_squared_error: 4190198272.0000 - val_loss: 3903005512.0479 - val_mean_squared_error: 3903005440.0000\n",
      "Epoch 59/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 3656885389.2153 - mean_squared_error: 3656885248.0000 - val_loss: 3406253407.0419 - val_mean_squared_error: 3406253312.0000\n",
      "Epoch 60/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 3191450996.2950 - mean_squared_error: 3191451136.0000 - val_loss: 2972725105.4371 - val_mean_squared_error: 2972725248.0000\n",
      "Epoch 61/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 2785254330.5251 - mean_squared_error: 2785254656.0000 - val_loss: 2594373428.1198 - val_mean_squared_error: 2594373376.0000\n",
      "Epoch 62/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 2430757442.4543 - mean_squared_error: 2430757376.0000 - val_loss: 2264177524.5030 - val_mean_squared_error: 2264177408.0000\n",
      "Epoch 63/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 2121379943.4572 - mean_squared_error: 2121379968.0000 - val_loss: 1976007715.2575 - val_mean_squared_error: 1976007680.0000\n",
      "Epoch 64/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 1851378888.1180 - mean_squared_error: 1851378816.0000 - val_loss: 1724513745.2455 - val_mean_squared_error: 1724513664.0000\n",
      "Epoch 65/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 1615741703.5516 - mean_squared_error: 1615741696.0000 - val_loss: 1505028688.4790 - val_mean_squared_error: 1505028736.0000\n",
      "Epoch 66/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 1410096139.3274 - mean_squared_error: 1410096128.0000 - val_loss: 1313479454.6587 - val_mean_squared_error: 1313479552.0000\n",
      "Epoch 67/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 1230624722.3127 - mean_squared_error: 1230624512.0000 - val_loss: 1146308855.5689 - val_mean_squared_error: 1146308864.0000\n",
      "Epoch 68/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 1073995014.9853 - mean_squared_error: 1073995136.0000 - val_loss: 1000414395.0180 - val_mean_squared_error: 1000414528.0000\n",
      "Epoch 69/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 937300741.4749 - mean_squared_error: 937300736.0000 - val_loss: 873089386.5389 - val_mean_squared_error: 873089408.0000\n",
      "Epoch 70/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 818004664.2596 - mean_squared_error: 818004672.0000 - val_loss: 761969404.5509 - val_mean_squared_error: 761969408.0000\n",
      "Epoch 71/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 713892131.3038 - mean_squared_error: 713892160.0000 - val_loss: 664992139.1138 - val_mean_squared_error: 664992064.0000\n",
      "Epoch 72/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 623030684.5074 - mean_squared_error: 623030720.0000 - val_loss: 580357577.5808 - val_mean_squared_error: 580357632.0000\n",
      "Epoch 73/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 543733659.3746 - mean_squared_error: 543733568.0000 - val_loss: 506494737.8204 - val_mean_squared_error: 506494720.0000\n",
      "Epoch 74/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 474529286.8909 - mean_squared_error: 474529344.0000 - val_loss: 442032577.7246 - val_mean_squared_error: 442032512.0000\n",
      "Epoch 75/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 414132880.5192 - mean_squared_error: 414132864.0000 - val_loss: 385774736.4790 - val_mean_squared_error: 385774688.0000\n",
      "Epoch 76/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 361423524.2478 - mean_squared_error: 361423488.0000 - val_loss: 336677083.0180 - val_mean_squared_error: 336677088.0000\n",
      "Epoch 77/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 315422885.1917 - mean_squared_error: 315422912.0000 - val_loss: 293828378.0599 - val_mean_squared_error: 293828384.0000\n",
      "Epoch 78/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 275277183.6224 - mean_squared_error: 275277152.0000 - val_loss: 256433189.7485 - val_mean_squared_error: 256433200.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 240240989.6401 - mean_squared_error: 240241008.0000 - val_loss: 223797226.6347 - val_mean_squared_error: 223797216.0000\n",
      "Epoch 80/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 209664033.8879 - mean_squared_error: 209664016.0000 - val_loss: 195314970.6347 - val_mean_squared_error: 195314944.0000\n",
      "Epoch 81/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 182978749.5457 - mean_squared_error: 182978736.0000 - val_loss: 170457654.8982 - val_mean_squared_error: 170457632.0000\n",
      "Epoch 82/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 159689898.7611 - mean_squared_error: 159689888.0000 - val_loss: 148764043.5928 - val_mean_squared_error: 148764064.0000\n",
      "Epoch 83/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 139365178.5959 - mean_squared_error: 139365152.0000 - val_loss: 129831296.1437 - val_mean_squared_error: 129831296.0000\n",
      "Epoch 84/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 121627203.9646 - mean_squared_error: 121627192.0000 - val_loss: 113308177.4850 - val_mean_squared_error: 113308176.0000\n",
      "Epoch 85/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 106146957.1681 - mean_squared_error: 106146968.0000 - val_loss: 98888062.1317 - val_mean_squared_error: 98888064.0000\n",
      "Epoch 86/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 92637035.1386 - mean_squared_error: 92637024.0000 - val_loss: 86303175.0419 - val_mean_squared_error: 86303176.0000\n",
      "Epoch 87/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 80846546.6431 - mean_squared_error: 80846536.0000 - val_loss: 75319945.5808 - val_mean_squared_error: 75319952.0000\n",
      "Epoch 88/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 70556686.5015 - mean_squared_error: 70556688.0000 - val_loss: 65734545.1018 - val_mean_squared_error: 65734544.0000\n",
      "Epoch 89/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 61576508.5900 - mean_squared_error: 61576516.0000 - val_loss: 57369078.9461 - val_mean_squared_error: 57369084.0000\n",
      "Epoch 90/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 53739291.3156 - mean_squared_error: 53739296.0000 - val_loss: 50068282.7545 - val_mean_squared_error: 50068280.0000\n",
      "Epoch 91/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 46899582.1829 - mean_squared_error: 46899580.0000 - val_loss: 43696635.4012 - val_mean_squared_error: 43696636.0000\n",
      "Epoch 92/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 40930391.6696 - mean_squared_error: 40930392.0000 - val_loss: 38135923.2335 - val_mean_squared_error: 38135920.0000\n",
      "Epoch 93/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 35720956.0177 - mean_squared_error: 35720952.0000 - val_loss: 33282856.9222 - val_mean_squared_error: 33282858.0000\n",
      "Epoch 94/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 31174519.1209 - mean_squared_error: 31174520.0000 - val_loss: 29047444.2515 - val_mean_squared_error: 29047444.0000\n",
      "Epoch 95/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 27206761.2684 - mean_squared_error: 27206758.0000 - val_loss: 25351050.9820 - val_mean_squared_error: 25351054.0000\n",
      "Epoch 96/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 23744000.0472 - mean_squared_error: 23744002.0000 - val_loss: 22125083.8084 - val_mean_squared_error: 22125084.0000\n",
      "Epoch 97/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 20721970.1239 - mean_squared_error: 20721970.0000 - val_loss: 19309668.8503 - val_mean_squared_error: 19309668.0000\n",
      "Epoch 98/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 18084582.3363 - mean_squared_error: 18084584.0000 - val_loss: 16852549.4132 - val_mean_squared_error: 16852548.0000\n",
      "Epoch 99/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 15782859.5664 - mean_squared_error: 15782861.0000 - val_loss: 14708137.5449 - val_mean_squared_error: 14708138.0000\n",
      "Epoch 100/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 13774095.7640 - mean_squared_error: 13774097.0000 - val_loss: 12836610.8204 - val_mean_squared_error: 12836613.0000\n",
      "Epoch 101/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 12020987.6372 - mean_squared_error: 12020988.0000 - val_loss: 11203242.0060 - val_mean_squared_error: 11203242.0000\n",
      "Epoch 102/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 10491005.3038 - mean_squared_error: 10491006.0000 - val_loss: 9777756.7844 - val_mean_squared_error: 9777756.0000\n",
      "Epoch 103/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 9155769.7552 - mean_squared_error: 9155771.0000 - val_loss: 8533665.7066 - val_mean_squared_error: 8533666.0000\n",
      "Epoch 104/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 7990466.7257 - mean_squared_error: 7990466.0000 - val_loss: 7447888.3802 - val_mean_squared_error: 7447888.5000\n",
      "Epoch 105/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 6973479.4395 - mean_squared_error: 6973478.5000 - val_loss: 6500285.1018 - val_mean_squared_error: 6500285.5000\n",
      "Epoch 106/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 6085930.8156 - mean_squared_error: 6085930.5000 - val_loss: 5673267.3922 - val_mean_squared_error: 5673268.0000\n",
      "Epoch 107/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 5311348.1209 - mean_squared_error: 5311348.5000 - val_loss: 4951495.7844 - val_mean_squared_error: 4951495.5000\n",
      "Epoch 108/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 4635353.5619 - mean_squared_error: 4635354.0000 - val_loss: 4321565.6677 - val_mean_squared_error: 4321565.0000\n",
      "Epoch 109/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 4045393.1814 - mean_squared_error: 4045393.2500 - val_loss: 3771790.0150 - val_mean_squared_error: 3771790.2500\n",
      "Epoch 110/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 3530520.5015 - mean_squared_error: 3530520.5000 - val_loss: 3291969.6063 - val_mean_squared_error: 3291969.5000\n",
      "Epoch 111/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 3081178.9277 - mean_squared_error: 3081178.7500 - val_loss: 2873209.6048 - val_mean_squared_error: 2873209.5000\n",
      "Epoch 112/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 2689028.7588 - mean_squared_error: 2689029.0000 - val_loss: 2507731.2171 - val_mean_squared_error: 2507731.2500\n",
      "Epoch 113/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 2346790.7367 - mean_squared_error: 2346790.7500 - val_loss: 2188757.5060 - val_mean_squared_error: 2188757.2500\n",
      "Epoch 114/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 2048111.4307 - mean_squared_error: 2048111.2500 - val_loss: 1910368.0591 - val_mean_squared_error: 1910368.0000\n",
      "Epoch 115/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 1787446.0763 - mean_squared_error: 1787445.7500 - val_loss: 1667399.9558 - val_mean_squared_error: 1667400.0000\n",
      "Epoch 116/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 1559958.3540 - mean_squared_error: 1559958.3750 - val_loss: 1455343.2088 - val_mean_squared_error: 1455343.3750\n",
      "Epoch 117/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 1361423.4989 - mean_squared_error: 1361423.6250 - val_loss: 1270269.0052 - val_mean_squared_error: 1270269.0000\n",
      "Epoch 118/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 1188159.4727 - mean_squared_error: 1188159.5000 - val_loss: 1108738.9274 - val_mean_squared_error: 1108738.8750\n",
      "Epoch 119/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 1036944.4987 - mean_squared_error: 1036944.5000 - val_loss: 967759.3567 - val_mean_squared_error: 967759.2500\n",
      "Epoch 120/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 904977.6364 - mean_squared_error: 904977.6250 - val_loss: 844714.3387 - val_mean_squared_error: 844714.3750\n",
      "Epoch 121/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 91us/sample - loss: 789805.6514 - mean_squared_error: 789805.5625 - val_loss: 737320.6523 - val_mean_squared_error: 737320.6875\n",
      "Epoch 122/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 689292.0310 - mean_squared_error: 689292.0000 - val_loss: 643590.4779 - val_mean_squared_error: 643590.5000\n",
      "Epoch 123/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 601572.3875 - mean_squared_error: 601572.3125 - val_loss: 561781.6957 - val_mean_squared_error: 561781.6875\n",
      "Epoch 124/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 525016.8664 - mean_squared_error: 525016.8125 - val_loss: 490380.4454 - val_mean_squared_error: 490380.4062\n",
      "Epoch 125/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 458206.0924 - mean_squared_error: 458206.1250 - val_loss: 428060.8249 - val_mean_squared_error: 428060.7812\n",
      "Epoch 126/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 399898.0909 - mean_squared_error: 399898.0938 - val_loss: 373667.2363 - val_mean_squared_error: 373667.2500\n",
      "Epoch 127/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 349010.8288 - mean_squared_error: 349010.8125 - val_loss: 326190.6514 - val_mean_squared_error: 326190.6875\n",
      "Epoch 128/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 304600.7540 - mean_squared_error: 304600.7188 - val_loss: 284752.8814 - val_mean_squared_error: 284752.8750\n",
      "Epoch 129/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 265842.3050 - mean_squared_error: 265842.3125 - val_loss: 248583.0928 - val_mean_squared_error: 248583.1094\n",
      "Epoch 130/400\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 232017.0232 - mean_squared_error: 232016.9844 - val_loss: 217014.9544 - val_mean_squared_error: 217014.9688\n",
      "Epoch 131/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 202498.4354 - mean_squared_error: 202498.4375 - val_loss: 189461.2836 - val_mean_squared_error: 189461.2969\n",
      "Epoch 132/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 176737.0411 - mean_squared_error: 176737.0625 - val_loss: 165409.4421 - val_mean_squared_error: 165409.4688\n",
      "Epoch 133/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 154252.9892 - mean_squared_error: 154253.0000 - val_loss: 144415.1275 - val_mean_squared_error: 144415.1094\n",
      "Epoch 134/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 134631.2588 - mean_squared_error: 134631.2812 - val_loss: 126090.5575 - val_mean_squared_error: 126090.5547\n",
      "Epoch 135/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 117507.0412 - mean_squared_error: 117507.0312 - val_loss: 110094.9444 - val_mean_squared_error: 110094.9453\n",
      "Epoch 136/400\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 102561.9878 - mean_squared_error: 102561.9844 - val_loss: 96132.2944 - val_mean_squared_error: 96132.3047\n",
      "Epoch 137/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 89519.1682 - mean_squared_error: 89519.1641 - val_loss: 83944.3352 - val_mean_squared_error: 83944.3359\n",
      "Epoch 138/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 78136.1678 - mean_squared_error: 78136.1641 - val_loss: 73304.2056 - val_mean_squared_error: 73304.2031\n",
      "Epoch 139/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 68201.8916 - mean_squared_error: 68201.8906 - val_loss: 64017.2576 - val_mean_squared_error: 64017.2500\n",
      "Epoch 140/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 59532.6485 - mean_squared_error: 59532.6367 - val_loss: 55909.8109 - val_mean_squared_error: 55909.8125\n",
      "Epoch 141/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 51966.2666 - mean_squared_error: 51966.2656 - val_loss: 48832.3371 - val_mean_squared_error: 48832.3398\n",
      "Epoch 142/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 45363.5702 - mean_squared_error: 45363.5742 - val_loss: 42654.1460 - val_mean_squared_error: 42654.1484\n",
      "Epoch 143/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 39600.8666 - mean_squared_error: 39600.8672 - val_loss: 37260.2714 - val_mean_squared_error: 37260.2734\n",
      "Epoch 144/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 34571.8021 - mean_squared_error: 34571.8008 - val_loss: 32552.1014 - val_mean_squared_error: 32552.1016\n",
      "Epoch 145/400\n",
      "339/339 [==============================] - ETA: 0s - loss: 32118.1504 - mean_squared_error: 32118.15 - 0s 85us/sample - loss: 30183.1271 - mean_squared_error: 30183.1270 - val_loss: 28440.8807 - val_mean_squared_error: 28440.8809\n",
      "Epoch 146/400\n",
      "339/339 [==============================] - ETA: 0s - loss: 26890.7129 - mean_squared_error: 26890.71 - 0s 103us/sample - loss: 26352.6385 - mean_squared_error: 26352.6406 - val_loss: 24851.3176 - val_mean_squared_error: 24851.3164\n",
      "Epoch 147/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 23009.5123 - mean_squared_error: 23009.5117 - val_loss: 21717.3030 - val_mean_squared_error: 21717.3047\n",
      "Epoch 148/400\n",
      "339/339 [==============================] - 0s 171us/sample - loss: 20091.8795 - mean_squared_error: 20091.8789 - val_loss: 18981.2255 - val_mean_squared_error: 18981.2246\n",
      "Epoch 149/400\n",
      "339/339 [==============================] - 0s 182us/sample - loss: 17545.7175 - mean_squared_error: 17545.7168 - val_loss: 16591.9643 - val_mean_squared_error: 16591.9648\n",
      "Epoch 150/400\n",
      "339/339 [==============================] - 0s 194us/sample - loss: 15323.6575 - mean_squared_error: 15323.6562 - val_loss: 14506.3619 - val_mean_squared_error: 14506.3623\n",
      "Epoch 151/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 13384.6758 - mean_squared_error: 13384.6768 - val_loss: 12684.6282 - val_mean_squared_error: 12684.6289\n",
      "Epoch 152/400\n",
      "339/339 [==============================] - 0s 182us/sample - loss: 11692.0730 - mean_squared_error: 11692.0732 - val_loss: 11094.0052 - val_mean_squared_error: 11094.0049\n",
      "Epoch 153/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 10214.8732 - mean_squared_error: 10214.8730 - val_loss: 9704.4812 - val_mean_squared_error: 9704.4814\n",
      "Epoch 154/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 8925.4977 - mean_squared_error: 8925.4961 - val_loss: 8491.2256 - val_mean_squared_error: 8491.2256\n",
      "Epoch 155/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 7800.3789 - mean_squared_error: 7800.3799 - val_loss: 7431.6961 - val_mean_squared_error: 7431.6963\n",
      "Epoch 156/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 6818.5623 - mean_squared_error: 6818.5635 - val_loss: 6506.2521 - val_mean_squared_error: 6506.2515\n",
      "Epoch 157/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 5961.7526 - mean_squared_error: 5961.7524 - val_loss: 5698.1631 - val_mean_squared_error: 5698.1636\n",
      "Epoch 158/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 5214.0326 - mean_squared_error: 5214.0322 - val_loss: 4992.0591 - val_mean_squared_error: 4992.0591\n",
      "Epoch 159/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 4561.2640 - mean_squared_error: 4561.2642 - val_loss: 4375.2314 - val_mean_squared_error: 4375.2314\n",
      "Epoch 160/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 3991.7294 - mean_squared_error: 3991.7285 - val_loss: 3836.5235 - val_mean_squared_error: 3836.5237\n",
      "Epoch 161/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 3494.6275 - mean_squared_error: 3494.6272 - val_loss: 3365.7314 - val_mean_squared_error: 3365.7312\n",
      "Epoch 162/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 3060.8168 - mean_squared_error: 3060.8171 - val_loss: 2954.5615 - val_mean_squared_error: 2954.5620\n",
      "Epoch 163/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 2682.2758 - mean_squared_error: 2682.2756 - val_loss: 2595.0593 - val_mean_squared_error: 2595.0596\n",
      "Epoch 164/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 2351.7758 - mean_squared_error: 2351.7761 - val_loss: 2280.9397 - val_mean_squared_error: 2280.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 2063.3851 - mean_squared_error: 2063.3853 - val_loss: 2006.5423 - val_mean_squared_error: 2006.5422\n",
      "Epoch 166/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 1811.7596 - mean_squared_error: 1811.7594 - val_loss: 1766.5482 - val_mean_squared_error: 1766.5481\n",
      "Epoch 167/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 1592.0280 - mean_squared_error: 1592.0281 - val_loss: 1556.7508 - val_mean_squared_error: 1556.7510\n",
      "Epoch 168/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 1400.2945 - mean_squared_error: 1400.2944 - val_loss: 1373.3394 - val_mean_squared_error: 1373.3395\n",
      "Epoch 169/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 1232.9645 - mean_squared_error: 1232.9645 - val_loss: 1213.0016 - val_mean_squared_error: 1213.0015\n",
      "Epoch 170/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 1086.9488 - mean_squared_error: 1086.9487 - val_loss: 1072.9126 - val_mean_squared_error: 1072.9126\n",
      "Epoch 171/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 959.6127 - mean_squared_error: 959.6127 - val_loss: 950.3757 - val_mean_squared_error: 950.3755\n",
      "Epoch 172/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 848.4428 - mean_squared_error: 848.4429 - val_loss: 843.1637 - val_mean_squared_error: 843.1636\n",
      "Epoch 173/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 751.4219 - mean_squared_error: 751.4219 - val_loss: 749.4157 - val_mean_squared_error: 749.4157\n",
      "Epoch 174/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 666.7860 - mean_squared_error: 666.7859 - val_loss: 667.3205 - val_mean_squared_error: 667.3206\n",
      "Epoch 175/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 592.8294 - mean_squared_error: 592.8293 - val_loss: 595.4326 - val_mean_squared_error: 595.4326\n",
      "Epoch 176/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 528.2851 - mean_squared_error: 528.2850 - val_loss: 532.5694 - val_mean_squared_error: 532.5695\n",
      "Epoch 177/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 472.0036 - mean_squared_error: 472.0036 - val_loss: 477.5398 - val_mean_squared_error: 477.5398\n",
      "Epoch 178/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 422.8950 - mean_squared_error: 422.8949 - val_loss: 429.3385 - val_mean_squared_error: 429.3385\n",
      "Epoch 179/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 379.9987 - mean_squared_error: 379.9987 - val_loss: 387.1951 - val_mean_squared_error: 387.1951\n",
      "Epoch 180/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 342.6326 - mean_squared_error: 342.6326 - val_loss: 350.2220 - val_mean_squared_error: 350.2220\n",
      "Epoch 181/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 309.9761 - mean_squared_error: 309.9760 - val_loss: 317.8374 - val_mean_squared_error: 317.8374\n",
      "Epoch 182/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 281.4902 - mean_squared_error: 281.4902 - val_loss: 289.4500 - val_mean_squared_error: 289.4500\n",
      "Epoch 183/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 256.6313 - mean_squared_error: 256.6313 - val_loss: 264.5345 - val_mean_squared_error: 264.5345\n",
      "Epoch 184/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 234.9250 - mean_squared_error: 234.9250 - val_loss: 242.7482 - val_mean_squared_error: 242.7482\n",
      "Epoch 185/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 216.0021 - mean_squared_error: 216.0021 - val_loss: 223.6088 - val_mean_squared_error: 223.6088\n",
      "Epoch 186/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 199.4867 - mean_squared_error: 199.4868 - val_loss: 206.7945 - val_mean_squared_error: 206.7945\n",
      "Epoch 187/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 185.0580 - mean_squared_error: 185.0580 - val_loss: 192.0716 - val_mean_squared_error: 192.0716\n",
      "Epoch 188/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 172.4923 - mean_squared_error: 172.4923 - val_loss: 179.1015 - val_mean_squared_error: 179.1014\n",
      "Epoch 189/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 161.4964 - mean_squared_error: 161.4964 - val_loss: 167.7415 - val_mean_squared_error: 167.7415\n",
      "Epoch 190/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 151.9146 - mean_squared_error: 151.9147 - val_loss: 157.7164 - val_mean_squared_error: 157.7164\n",
      "Epoch 191/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 143.5251 - mean_squared_error: 143.5251 - val_loss: 148.8991 - val_mean_squared_error: 148.8991\n",
      "Epoch 192/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 136.2254 - mean_squared_error: 136.2254 - val_loss: 141.1876 - val_mean_squared_error: 141.1876\n",
      "Epoch 193/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 129.8847 - mean_squared_error: 129.8847 - val_loss: 134.4110 - val_mean_squared_error: 134.4110\n",
      "Epoch 194/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 124.3320 - mean_squared_error: 124.3320 - val_loss: 128.4109 - val_mean_squared_error: 128.4109\n",
      "Epoch 195/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 119.4725 - mean_squared_error: 119.4725 - val_loss: 123.1019 - val_mean_squared_error: 123.1019\n",
      "Epoch 196/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 115.2235 - mean_squared_error: 115.2235 - val_loss: 118.4211 - val_mean_squared_error: 118.4211\n",
      "Epoch 197/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 111.5103 - mean_squared_error: 111.5103 - val_loss: 114.3139 - val_mean_squared_error: 114.3139\n",
      "Epoch 198/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 108.2803 - mean_squared_error: 108.2802 - val_loss: 110.6755 - val_mean_squared_error: 110.6755\n",
      "Epoch 199/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 105.4665 - mean_squared_error: 105.4666 - val_loss: 107.4801 - val_mean_squared_error: 107.4801\n",
      "Epoch 200/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 103.0155 - mean_squared_error: 103.0155 - val_loss: 104.6642 - val_mean_squared_error: 104.6642\n",
      "Epoch 201/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 100.8821 - mean_squared_error: 100.8821 - val_loss: 102.1654 - val_mean_squared_error: 102.1654\n",
      "Epoch 202/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 99.0104 - mean_squared_error: 99.0104 - val_loss: 99.9427 - val_mean_squared_error: 99.9427\n",
      "Epoch 203/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 97.3700 - mean_squared_error: 97.3700 - val_loss: 97.9699 - val_mean_squared_error: 97.9699\n",
      "Epoch 204/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 95.9462 - mean_squared_error: 95.9462 - val_loss: 96.2174 - val_mean_squared_error: 96.2174\n",
      "Epoch 205/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 94.6955 - mean_squared_error: 94.6955 - val_loss: 94.6667 - val_mean_squared_error: 94.6667\n",
      "Epoch 206/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 93.5969 - mean_squared_error: 93.5969 - val_loss: 93.2978 - val_mean_squared_error: 93.2978\n",
      "Epoch 207/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 92.6560 - mean_squared_error: 92.6560 - val_loss: 92.0756 - val_mean_squared_error: 92.0756\n",
      "Epoch 208/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 91.8325 - mean_squared_error: 91.8325 - val_loss: 91.0032 - val_mean_squared_error: 91.0032\n",
      "Epoch 209/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 91.1172 - mean_squared_error: 91.1172 - val_loss: 90.0290 - val_mean_squared_error: 90.0290\n",
      "Epoch 210/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 90.4831 - mean_squared_error: 90.4831 - val_loss: 89.1569 - val_mean_squared_error: 89.1569\n",
      "Epoch 211/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 112us/sample - loss: 89.9241 - mean_squared_error: 89.9241 - val_loss: 88.3816 - val_mean_squared_error: 88.3816\n",
      "Epoch 212/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 89.4439 - mean_squared_error: 89.4439 - val_loss: 87.6946 - val_mean_squared_error: 87.6946\n",
      "Epoch 213/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 89.0286 - mean_squared_error: 89.0285 - val_loss: 87.0707 - val_mean_squared_error: 87.0707\n",
      "Epoch 214/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 88.6496 - mean_squared_error: 88.6496 - val_loss: 86.5228 - val_mean_squared_error: 86.5228\n",
      "Epoch 215/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 88.3328 - mean_squared_error: 88.3328 - val_loss: 86.0359 - val_mean_squared_error: 86.0359\n",
      "Epoch 216/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 88.0650 - mean_squared_error: 88.0650 - val_loss: 85.5964 - val_mean_squared_error: 85.5964\n",
      "Epoch 217/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 87.8295 - mean_squared_error: 87.8295 - val_loss: 85.2065 - val_mean_squared_error: 85.2065\n",
      "Epoch 218/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 87.6118 - mean_squared_error: 87.6118 - val_loss: 84.8574 - val_mean_squared_error: 84.8574\n",
      "Epoch 219/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 87.4421 - mean_squared_error: 87.4421 - val_loss: 84.5384 - val_mean_squared_error: 84.5383\n",
      "Epoch 220/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 87.2726 - mean_squared_error: 87.2726 - val_loss: 84.2476 - val_mean_squared_error: 84.2476\n",
      "Epoch 221/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 87.1397 - mean_squared_error: 87.1397 - val_loss: 83.9832 - val_mean_squared_error: 83.9832\n",
      "Epoch 222/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 86.9978 - mean_squared_error: 86.9978 - val_loss: 83.7442 - val_mean_squared_error: 83.7442\n",
      "Epoch 223/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 86.8946 - mean_squared_error: 86.8946 - val_loss: 83.5323 - val_mean_squared_error: 83.5323\n",
      "Epoch 224/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 86.8014 - mean_squared_error: 86.8014 - val_loss: 83.3413 - val_mean_squared_error: 83.3413\n",
      "Epoch 225/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.7200 - mean_squared_error: 86.7200 - val_loss: 83.1604 - val_mean_squared_error: 83.1604\n",
      "Epoch 226/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.6528 - mean_squared_error: 86.6528 - val_loss: 83.0077 - val_mean_squared_error: 83.0077\n",
      "Epoch 227/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 86.5852 - mean_squared_error: 86.5852 - val_loss: 82.8610 - val_mean_squared_error: 82.8610\n",
      "Epoch 228/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.5302 - mean_squared_error: 86.5302 - val_loss: 82.7292 - val_mean_squared_error: 82.7292\n",
      "Epoch 229/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.4839 - mean_squared_error: 86.4839 - val_loss: 82.6145 - val_mean_squared_error: 82.6145\n",
      "Epoch 230/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 86.4419 - mean_squared_error: 86.4419 - val_loss: 82.5011 - val_mean_squared_error: 82.5011\n",
      "Epoch 231/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.4089 - mean_squared_error: 86.4089 - val_loss: 82.3969 - val_mean_squared_error: 82.3969\n",
      "Epoch 232/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 86.3793 - mean_squared_error: 86.3793 - val_loss: 82.3055 - val_mean_squared_error: 82.3055\n",
      "Epoch 233/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 86.3577 - mean_squared_error: 86.3577 - val_loss: 82.2281 - val_mean_squared_error: 82.2281\n",
      "Epoch 234/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 86.3230 - mean_squared_error: 86.3230 - val_loss: 82.1545 - val_mean_squared_error: 82.1545\n",
      "Epoch 235/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 86.3031 - mean_squared_error: 86.3031 - val_loss: 82.0850 - val_mean_squared_error: 82.0850\n",
      "Epoch 236/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.2853 - mean_squared_error: 86.2853 - val_loss: 82.0203 - val_mean_squared_error: 82.0203\n",
      "Epoch 237/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.2714 - mean_squared_error: 86.2714 - val_loss: 81.9621 - val_mean_squared_error: 81.9621\n",
      "Epoch 238/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 86.2600 - mean_squared_error: 86.2600 - val_loss: 81.9073 - val_mean_squared_error: 81.9073\n",
      "Epoch 239/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 86.2443 - mean_squared_error: 86.2443 - val_loss: 81.8568 - val_mean_squared_error: 81.8568\n",
      "Epoch 240/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 86.2300 - mean_squared_error: 86.2300 - val_loss: 81.8147 - val_mean_squared_error: 81.8147\n",
      "Epoch 241/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.2246 - mean_squared_error: 86.2246 - val_loss: 81.7737 - val_mean_squared_error: 81.7737\n",
      "Epoch 242/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 86.2097 - mean_squared_error: 86.2097 - val_loss: 81.7366 - val_mean_squared_error: 81.7366\n",
      "Epoch 243/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.2128 - mean_squared_error: 86.2128 - val_loss: 81.6992 - val_mean_squared_error: 81.6992\n",
      "Epoch 244/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.2079 - mean_squared_error: 86.2079 - val_loss: 81.6644 - val_mean_squared_error: 81.6644\n",
      "Epoch 245/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 86.1952 - mean_squared_error: 86.1952 - val_loss: 81.6339 - val_mean_squared_error: 81.6339\n",
      "Epoch 246/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1954 - mean_squared_error: 86.1954 - val_loss: 81.6100 - val_mean_squared_error: 81.6100\n",
      "Epoch 247/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1870 - mean_squared_error: 86.1870 - val_loss: 81.5835 - val_mean_squared_error: 81.5835\n",
      "Epoch 248/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 86.1901 - mean_squared_error: 86.1901 - val_loss: 81.5566 - val_mean_squared_error: 81.5567\n",
      "Epoch 249/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 86.1810 - mean_squared_error: 86.1810 - val_loss: 81.5375 - val_mean_squared_error: 81.5375\n",
      "Epoch 250/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.1785 - mean_squared_error: 86.1785 - val_loss: 81.5184 - val_mean_squared_error: 81.5184\n",
      "Epoch 251/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1841 - mean_squared_error: 86.1841 - val_loss: 81.5005 - val_mean_squared_error: 81.5005\n",
      "Epoch 252/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.1739 - mean_squared_error: 86.1740 - val_loss: 81.4830 - val_mean_squared_error: 81.4830\n",
      "Epoch 253/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.1735 - mean_squared_error: 86.1735 - val_loss: 81.4661 - val_mean_squared_error: 81.4661\n",
      "Epoch 254/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 86.1705 - mean_squared_error: 86.1705 - val_loss: 81.4523 - val_mean_squared_error: 81.4523\n",
      "Epoch 255/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1785 - mean_squared_error: 86.1785 - val_loss: 81.4362 - val_mean_squared_error: 81.4362\n",
      "Epoch 256/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 86.1805 - mean_squared_error: 86.1805 - val_loss: 81.4235 - val_mean_squared_error: 81.4235\n",
      "Epoch 257/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1695 - mean_squared_error: 86.1695 - val_loss: 81.4138 - val_mean_squared_error: 81.4138\n",
      "Epoch 258/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1663 - mean_squared_error: 86.1664 - val_loss: 81.3987 - val_mean_squared_error: 81.3987\n",
      "Epoch 259/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1637 - mean_squared_error: 86.1637 - val_loss: 81.3889 - val_mean_squared_error: 81.3889\n",
      "Epoch 260/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 86.1695 - mean_squared_error: 86.1695 - val_loss: 81.3819 - val_mean_squared_error: 81.3819\n",
      "Epoch 261/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1673 - mean_squared_error: 86.1673 - val_loss: 81.3690 - val_mean_squared_error: 81.3689\n",
      "Epoch 262/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1668 - mean_squared_error: 86.1668 - val_loss: 81.3585 - val_mean_squared_error: 81.3585\n",
      "Epoch 263/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1588 - mean_squared_error: 86.1588 - val_loss: 81.3471 - val_mean_squared_error: 81.3471\n",
      "Epoch 264/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 86.1643 - mean_squared_error: 86.1643 - val_loss: 81.3389 - val_mean_squared_error: 81.3389\n",
      "Epoch 265/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 86.1606 - mean_squared_error: 86.1606 - val_loss: 81.3324 - val_mean_squared_error: 81.3324\n",
      "Epoch 266/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 86.1649 - mean_squared_error: 86.1648 - val_loss: 81.3246 - val_mean_squared_error: 81.3246\n",
      "Epoch 267/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.1645 - mean_squared_error: 86.1645 - val_loss: 81.3152 - val_mean_squared_error: 81.3152\n",
      "Epoch 268/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1627 - mean_squared_error: 86.1628 - val_loss: 81.3106 - val_mean_squared_error: 81.3106\n",
      "Epoch 269/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1659 - mean_squared_error: 86.1659 - val_loss: 81.3045 - val_mean_squared_error: 81.3045\n",
      "Epoch 270/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 86.1569 - mean_squared_error: 86.1569 - val_loss: 81.3041 - val_mean_squared_error: 81.3041\n",
      "Epoch 271/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1606 - mean_squared_error: 86.1606 - val_loss: 81.2993 - val_mean_squared_error: 81.2993\n",
      "Epoch 272/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1575 - mean_squared_error: 86.1575 - val_loss: 81.2968 - val_mean_squared_error: 81.2968\n",
      "Epoch 273/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1617 - mean_squared_error: 86.1617 - val_loss: 81.2944 - val_mean_squared_error: 81.2944\n",
      "Epoch 274/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.1613 - mean_squared_error: 86.1613 - val_loss: 81.2905 - val_mean_squared_error: 81.2905\n",
      "Epoch 275/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1574 - mean_squared_error: 86.1574 - val_loss: 81.2875 - val_mean_squared_error: 81.2875\n",
      "Epoch 276/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1573 - mean_squared_error: 86.1573 - val_loss: 81.2839 - val_mean_squared_error: 81.2839\n",
      "Epoch 277/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1554 - mean_squared_error: 86.1553 - val_loss: 81.2803 - val_mean_squared_error: 81.2803\n",
      "Epoch 278/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1591 - mean_squared_error: 86.1591 - val_loss: 81.2769 - val_mean_squared_error: 81.2769\n",
      "Epoch 279/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1584 - mean_squared_error: 86.1584 - val_loss: 81.2723 - val_mean_squared_error: 81.2723\n",
      "Epoch 280/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1554 - mean_squared_error: 86.1554 - val_loss: 81.2688 - val_mean_squared_error: 81.2688\n",
      "Epoch 281/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1573 - mean_squared_error: 86.1573 - val_loss: 81.2704 - val_mean_squared_error: 81.2704\n",
      "Epoch 282/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1547 - mean_squared_error: 86.1547 - val_loss: 81.2667 - val_mean_squared_error: 81.2667\n",
      "Epoch 283/400\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 86.1601 - mean_squared_error: 86.1601 - val_loss: 81.2664 - val_mean_squared_error: 81.2664\n",
      "Epoch 284/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1685 - mean_squared_error: 86.1686 - val_loss: 81.2643 - val_mean_squared_error: 81.2643\n",
      "Epoch 285/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1637 - mean_squared_error: 86.1637 - val_loss: 81.2592 - val_mean_squared_error: 81.2592\n",
      "Epoch 286/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1597 - mean_squared_error: 86.1597 - val_loss: 81.2539 - val_mean_squared_error: 81.2539\n",
      "Epoch 287/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1563 - mean_squared_error: 86.1563 - val_loss: 81.2566 - val_mean_squared_error: 81.2566\n",
      "Epoch 288/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1606 - mean_squared_error: 86.1606 - val_loss: 81.2560 - val_mean_squared_error: 81.2560\n",
      "Epoch 289/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1605 - mean_squared_error: 86.1605 - val_loss: 81.2519 - val_mean_squared_error: 81.2519\n",
      "Epoch 290/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1534 - mean_squared_error: 86.1534 - val_loss: 81.2513 - val_mean_squared_error: 81.2513\n",
      "Epoch 291/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1583 - mean_squared_error: 86.1583 - val_loss: 81.2483 - val_mean_squared_error: 81.2483\n",
      "Epoch 292/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1550 - mean_squared_error: 86.1550 - val_loss: 81.2480 - val_mean_squared_error: 81.2480\n",
      "Epoch 293/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1582 - mean_squared_error: 86.1581 - val_loss: 81.2476 - val_mean_squared_error: 81.2476\n",
      "Epoch 294/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 86.1593 - mean_squared_error: 86.1593 - val_loss: 81.2461 - val_mean_squared_error: 81.2461\n",
      "Epoch 295/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1581 - mean_squared_error: 86.1581 - val_loss: 81.2431 - val_mean_squared_error: 81.2431\n",
      "Epoch 296/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1612 - mean_squared_error: 86.1612 - val_loss: 81.2469 - val_mean_squared_error: 81.2470\n",
      "Epoch 297/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1561 - mean_squared_error: 86.1561 - val_loss: 81.2480 - val_mean_squared_error: 81.2480\n",
      "Epoch 298/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1628 - mean_squared_error: 86.1628 - val_loss: 81.2486 - val_mean_squared_error: 81.2486\n",
      "Epoch 299/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1604 - mean_squared_error: 86.1604 - val_loss: 81.2468 - val_mean_squared_error: 81.2468\n",
      "Epoch 300/400\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 86.1591 - mean_squared_error: 86.1591 - val_loss: 81.2461 - val_mean_squared_error: 81.2461\n",
      "Epoch 301/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 86.1628 - mean_squared_error: 86.1628 - val_loss: 81.2485 - val_mean_squared_error: 81.2485\n",
      "Epoch 302/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1579 - mean_squared_error: 86.1579 - val_loss: 81.2481 - val_mean_squared_error: 81.2481\n",
      "Epoch 303/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.1552 - mean_squared_error: 86.1552 - val_loss: 81.2501 - val_mean_squared_error: 81.2501\n",
      "Epoch 304/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1629 - mean_squared_error: 86.1629 - val_loss: 81.2474 - val_mean_squared_error: 81.2474\n",
      "Epoch 305/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 86.1564 - mean_squared_error: 86.1563 - val_loss: 81.2487 - val_mean_squared_error: 81.2487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1587 - mean_squared_error: 86.1587 - val_loss: 81.2511 - val_mean_squared_error: 81.2511\n",
      "Epoch 307/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1611 - mean_squared_error: 86.1610 - val_loss: 81.2510 - val_mean_squared_error: 81.2510\n",
      "Epoch 308/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1602 - mean_squared_error: 86.1602 - val_loss: 81.2497 - val_mean_squared_error: 81.2497\n",
      "Epoch 309/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1571 - mean_squared_error: 86.1571 - val_loss: 81.2477 - val_mean_squared_error: 81.2477\n",
      "Epoch 310/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1553 - mean_squared_error: 86.1553 - val_loss: 81.2454 - val_mean_squared_error: 81.2454\n",
      "Epoch 311/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1640 - mean_squared_error: 86.1640 - val_loss: 81.2464 - val_mean_squared_error: 81.2464\n",
      "Epoch 312/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.1624 - mean_squared_error: 86.1624 - val_loss: 81.2436 - val_mean_squared_error: 81.2436\n",
      "Epoch 313/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1581 - mean_squared_error: 86.1581 - val_loss: 81.2457 - val_mean_squared_error: 81.2458\n",
      "Epoch 314/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1598 - mean_squared_error: 86.1598 - val_loss: 81.2450 - val_mean_squared_error: 81.2450\n",
      "Epoch 315/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1602 - mean_squared_error: 86.1602 - val_loss: 81.2466 - val_mean_squared_error: 81.2466\n",
      "Epoch 316/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1584 - mean_squared_error: 86.1584 - val_loss: 81.2445 - val_mean_squared_error: 81.2445\n",
      "Epoch 317/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1558 - mean_squared_error: 86.1558 - val_loss: 81.2427 - val_mean_squared_error: 81.2428\n",
      "Epoch 318/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1606 - mean_squared_error: 86.1606 - val_loss: 81.2399 - val_mean_squared_error: 81.2399\n",
      "Epoch 319/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1594 - mean_squared_error: 86.1594 - val_loss: 81.2422 - val_mean_squared_error: 81.2422\n",
      "Epoch 320/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1616 - mean_squared_error: 86.1616 - val_loss: 81.2395 - val_mean_squared_error: 81.2395\n",
      "Epoch 321/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1707 - mean_squared_error: 86.1707 - val_loss: 81.2378 - val_mean_squared_error: 81.2378\n",
      "Epoch 322/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1606 - mean_squared_error: 86.1606 - val_loss: 81.2414 - val_mean_squared_error: 81.2414\n",
      "Epoch 323/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1576 - mean_squared_error: 86.1576 - val_loss: 81.2376 - val_mean_squared_error: 81.2376\n",
      "Epoch 324/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 86.1602 - mean_squared_error: 86.1602 - val_loss: 81.2323 - val_mean_squared_error: 81.2323\n",
      "Epoch 325/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 86.1604 - mean_squared_error: 86.1604 - val_loss: 81.2307 - val_mean_squared_error: 81.2307\n",
      "Epoch 326/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: 86.1628 - mean_squared_error: 86.1628 - val_loss: 81.2328 - val_mean_squared_error: 81.2328\n",
      "Epoch 327/400\n",
      "339/339 [==============================] - 0s 221us/sample - loss: 86.1583 - mean_squared_error: 86.1583 - val_loss: 81.2320 - val_mean_squared_error: 81.2320\n",
      "Epoch 328/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: 86.1681 - mean_squared_error: 86.1681 - val_loss: 81.2288 - val_mean_squared_error: 81.2288\n",
      "Epoch 329/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 86.1594 - mean_squared_error: 86.1594 - val_loss: 81.2303 - val_mean_squared_error: 81.2303\n",
      "Epoch 330/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 86.1540 - mean_squared_error: 86.1540 - val_loss: 81.2294 - val_mean_squared_error: 81.2294\n",
      "Epoch 331/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: 86.1581 - mean_squared_error: 86.1581 - val_loss: 81.2266 - val_mean_squared_error: 81.2266\n",
      "Epoch 332/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 86.1579 - mean_squared_error: 86.1579 - val_loss: 81.2288 - val_mean_squared_error: 81.2288\n",
      "Epoch 333/400\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 86.1597 - mean_squared_error: 86.1597 - val_loss: 81.2288 - val_mean_squared_error: 81.2288\n",
      "Epoch 334/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 86.1596 - mean_squared_error: 86.1596 - val_loss: 81.2277 - val_mean_squared_error: 81.2277\n",
      "Epoch 335/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 86.1652 - mean_squared_error: 86.1652 - val_loss: 81.2278 - val_mean_squared_error: 81.2279\n",
      "Epoch 336/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: 86.1550 - mean_squared_error: 86.1550 - val_loss: 81.2298 - val_mean_squared_error: 81.2298\n",
      "Epoch 337/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 86.1682 - mean_squared_error: 86.1682 - val_loss: 81.2274 - val_mean_squared_error: 81.2273\n",
      "Epoch 338/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: 86.1570 - mean_squared_error: 86.1570 - val_loss: 81.2273 - val_mean_squared_error: 81.2272\n",
      "Epoch 339/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 86.1592 - mean_squared_error: 86.1592 - val_loss: 81.2270 - val_mean_squared_error: 81.2270\n",
      "Epoch 340/400\n",
      "339/339 [==============================] - 0s 182us/sample - loss: 86.1632 - mean_squared_error: 86.1632 - val_loss: 81.2288 - val_mean_squared_error: 81.2288\n",
      "Epoch 341/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 86.1567 - mean_squared_error: 86.1567 - val_loss: 81.2280 - val_mean_squared_error: 81.2280\n",
      "Epoch 342/400\n",
      "339/339 [==============================] - 0s 194us/sample - loss: 86.1596 - mean_squared_error: 86.1596 - val_loss: 81.2283 - val_mean_squared_error: 81.2283\n",
      "Epoch 343/400\n",
      "339/339 [==============================] - 0s 206us/sample - loss: 86.1633 - mean_squared_error: 86.1633 - val_loss: 81.2277 - val_mean_squared_error: 81.2277\n",
      "Epoch 344/400\n",
      "339/339 [==============================] - 0s 188us/sample - loss: 86.1666 - mean_squared_error: 86.1666 - val_loss: 81.2268 - val_mean_squared_error: 81.2268\n",
      "Epoch 345/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: 86.1649 - mean_squared_error: 86.1649 - val_loss: 81.2267 - val_mean_squared_error: 81.2267\n",
      "Epoch 346/400\n",
      "339/339 [==============================] - 0s 177us/sample - loss: 86.1599 - mean_squared_error: 86.1599 - val_loss: 81.2277 - val_mean_squared_error: 81.2277\n",
      "Epoch 347/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 86.1619 - mean_squared_error: 86.1619 - val_loss: 81.2248 - val_mean_squared_error: 81.2248\n",
      "Epoch 348/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: 86.1537 - mean_squared_error: 86.1537 - val_loss: 81.2219 - val_mean_squared_error: 81.2219\n",
      "Epoch 349/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 86.1586 - mean_squared_error: 86.1586 - val_loss: 81.2227 - val_mean_squared_error: 81.2227\n",
      "Epoch 350/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 86.1601 - mean_squared_error: 86.1601 - val_loss: 81.2211 - val_mean_squared_error: 81.2211\n",
      "Epoch 351/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: 86.1579 - mean_squared_error: 86.1579 - val_loss: 81.2241 - val_mean_squared_error: 81.2241\n",
      "Epoch 352/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: 86.1630 - mean_squared_error: 86.1630 - val_loss: 81.2258 - val_mean_squared_error: 81.2257\n",
      "Epoch 353/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 135us/sample - loss: 86.1585 - mean_squared_error: 86.1585 - val_loss: 81.2304 - val_mean_squared_error: 81.2304\n",
      "Epoch 354/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 86.1659 - mean_squared_error: 86.1659 - val_loss: 81.2290 - val_mean_squared_error: 81.2290\n",
      "Epoch 355/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 86.1609 - mean_squared_error: 86.1609 - val_loss: 81.2292 - val_mean_squared_error: 81.2292\n",
      "Epoch 356/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: 86.1563 - mean_squared_error: 86.1563 - val_loss: 81.2275 - val_mean_squared_error: 81.2275\n",
      "Epoch 357/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 86.1611 - mean_squared_error: 86.1611 - val_loss: 81.2280 - val_mean_squared_error: 81.2280\n",
      "Epoch 358/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1618 - mean_squared_error: 86.1618 - val_loss: 81.2325 - val_mean_squared_error: 81.2325\n",
      "Epoch 359/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1586 - mean_squared_error: 86.1586 - val_loss: 81.2311 - val_mean_squared_error: 81.2311\n",
      "Epoch 360/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.1594 - mean_squared_error: 86.1594 - val_loss: 81.2307 - val_mean_squared_error: 81.2307\n",
      "Epoch 361/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.1553 - mean_squared_error: 86.1553 - val_loss: 81.2269 - val_mean_squared_error: 81.2269\n",
      "Epoch 362/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 86.1612 - mean_squared_error: 86.1612 - val_loss: 81.2269 - val_mean_squared_error: 81.2269\n",
      "Epoch 363/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: 86.1638 - mean_squared_error: 86.1638 - val_loss: 81.2322 - val_mean_squared_error: 81.2322\n",
      "Epoch 364/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 86.1599 - mean_squared_error: 86.1599 - val_loss: 81.2337 - val_mean_squared_error: 81.2337\n",
      "Epoch 365/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: 86.1584 - mean_squared_error: 86.1584 - val_loss: 81.2377 - val_mean_squared_error: 81.2377\n",
      "Epoch 366/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.1581 - mean_squared_error: 86.1581 - val_loss: 81.2378 - val_mean_squared_error: 81.2378\n",
      "Epoch 367/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.1628 - mean_squared_error: 86.1628 - val_loss: 81.2352 - val_mean_squared_error: 81.2352\n",
      "Epoch 368/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1574 - mean_squared_error: 86.1574 - val_loss: 81.2371 - val_mean_squared_error: 81.2371\n",
      "Epoch 369/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 86.1628 - mean_squared_error: 86.1628 - val_loss: 81.2331 - val_mean_squared_error: 81.2331\n",
      "Epoch 370/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: 86.1596 - mean_squared_error: 86.1596 - val_loss: 81.2330 - val_mean_squared_error: 81.2330\n",
      "Epoch 371/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 86.1618 - mean_squared_error: 86.1618 - val_loss: 81.2302 - val_mean_squared_error: 81.2302\n",
      "Epoch 372/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: 86.1615 - mean_squared_error: 86.1615 - val_loss: 81.2297 - val_mean_squared_error: 81.2297\n",
      "Epoch 373/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: 86.1627 - mean_squared_error: 86.1627 - val_loss: 81.2278 - val_mean_squared_error: 81.2278\n",
      "Epoch 374/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 86.1536 - mean_squared_error: 86.1536 - val_loss: 81.2282 - val_mean_squared_error: 81.2282\n",
      "Epoch 375/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.1669 - mean_squared_error: 86.1669 - val_loss: 81.2295 - val_mean_squared_error: 81.2295\n",
      "Epoch 376/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 86.1638 - mean_squared_error: 86.1638 - val_loss: 81.2305 - val_mean_squared_error: 81.2305\n",
      "Epoch 377/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.1626 - mean_squared_error: 86.1626 - val_loss: 81.2322 - val_mean_squared_error: 81.2322\n",
      "Epoch 378/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 86.1558 - mean_squared_error: 86.1558 - val_loss: 81.2303 - val_mean_squared_error: 81.2303\n",
      "Epoch 379/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 86.1591 - mean_squared_error: 86.1591 - val_loss: 81.2309 - val_mean_squared_error: 81.2309\n",
      "Epoch 380/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 86.1675 - mean_squared_error: 86.1675 - val_loss: 81.2327 - val_mean_squared_error: 81.2327\n",
      "Epoch 381/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1580 - mean_squared_error: 86.1580 - val_loss: 81.2341 - val_mean_squared_error: 81.2341\n",
      "Epoch 382/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: 86.1614 - mean_squared_error: 86.1614 - val_loss: 81.2314 - val_mean_squared_error: 81.2314\n",
      "Epoch 383/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 86.1639 - mean_squared_error: 86.1639 - val_loss: 81.2332 - val_mean_squared_error: 81.2332\n",
      "Epoch 384/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 86.1540 - mean_squared_error: 86.1540 - val_loss: 81.2331 - val_mean_squared_error: 81.2331\n",
      "Epoch 385/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: 86.1636 - mean_squared_error: 86.1636 - val_loss: 81.2336 - val_mean_squared_error: 81.2336\n",
      "Epoch 386/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: 86.1601 - mean_squared_error: 86.1601 - val_loss: 81.2338 - val_mean_squared_error: 81.2338\n",
      "Epoch 387/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 86.1634 - mean_squared_error: 86.1634 - val_loss: 81.2297 - val_mean_squared_error: 81.2297\n",
      "Epoch 388/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 86.1604 - mean_squared_error: 86.1604 - val_loss: 81.2309 - val_mean_squared_error: 81.2309\n",
      "Epoch 389/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1564 - mean_squared_error: 86.1564 - val_loss: 81.2321 - val_mean_squared_error: 81.2321\n",
      "Epoch 390/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: 86.1664 - mean_squared_error: 86.1664 - val_loss: 81.2338 - val_mean_squared_error: 81.2338\n",
      "Epoch 391/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: 86.1575 - mean_squared_error: 86.1575 - val_loss: 81.2337 - val_mean_squared_error: 81.2337\n",
      "Epoch 392/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1569 - mean_squared_error: 86.1569 - val_loss: 81.2325 - val_mean_squared_error: 81.2325\n",
      "Epoch 393/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1602 - mean_squared_error: 86.1602 - val_loss: 81.2342 - val_mean_squared_error: 81.2342\n",
      "Epoch 394/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: 86.1582 - mean_squared_error: 86.1582 - val_loss: 81.2370 - val_mean_squared_error: 81.2370\n",
      "Epoch 395/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1586 - mean_squared_error: 86.1586 - val_loss: 81.2410 - val_mean_squared_error: 81.2410\n",
      "Epoch 396/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: 86.1588 - mean_squared_error: 86.1588 - val_loss: 81.2413 - val_mean_squared_error: 81.2413\n",
      "Epoch 397/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: 86.1559 - mean_squared_error: 86.1559 - val_loss: 81.2387 - val_mean_squared_error: 81.2387\n",
      "Epoch 398/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: 86.1657 - mean_squared_error: 86.1657 - val_loss: 81.2393 - val_mean_squared_error: 81.2393\n",
      "Epoch 399/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: 86.1631 - mean_squared_error: 86.1631 - val_loss: 81.2371 - val_mean_squared_error: 81.2370\n",
      "Epoch 400/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/339 [..............................] - ETA: 0s - loss: 39.3448 - mean_squared_error: 39.3448\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "339/339 [==============================] - 0s 121us/sample - loss: 86.1578 - mean_squared_error: 86.1578 - val_loss: 81.2359 - val_mean_squared_error: 81.2359\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(boston_housing_data.data,\n",
    "                                                   boston_housing_data.target,\n",
    "                                                   test_size=0.33)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu', input_shape=(13,)))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "\n",
    "model.compile(optimizer = tf.train.GradientDescentOptimizer(0.001),loss='mean_squared_error',metrics=['mse'])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    model.fit(train_X, train_y, epochs=400, batch_size=10, validation_data = (test_X, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
