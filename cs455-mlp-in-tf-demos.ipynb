{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CS 455/595a: MLP Demo using TensorFlow</center></h1>\n",
    "<center>Richard S. Stansbury</center>\n",
    "\n",
    "This notebook applies the ANN techniques for the Titanic Survivors and Boston Housing Prediction models covered in [1] with the [Titanic](https://www.kaggle.com/c/titanic/) and [Boston Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) data sets for DT-based classification and regression, respectively.\n",
    "\n",
    "Several different approaches to model construction are shown ihe demos below\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1] Aurelen Geron. *Hands on Machine Learning with Scikit-Learn & TensorFlow* O'Reilley Media Inc, 2017.\n",
    "\n",
    "[2] Aurelen Geron. \"ageron/handson-ml: A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow.\" Github.com, online at: https://github.com/ageron/handson-ml [last accessed 2019-03-01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "1. [Titanic Survivor ANN Classifiers](#Titanic-Survivor-Classifier)\n",
    " \n",
    "2. [Boston Housing Cost Ensemble ANN Regressor](#Boston-Housing-Cost-Estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survivor Classifier\n",
    "\n",
    "## Set up - Imports of libraries and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# From: https://github.com/ageron/handson-ml/blob/master/09_up_and_running_with_tensorflow.ipynb    \n",
    "def reset_graph():\n",
    "    tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data and apply pipelines to pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 9)\n",
      "(891, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data from input files into Pandas data frames\n",
    "data_path = os.path.join(\"datasets\",\"titanic\")\n",
    "train_filename = \"train.csv\"\n",
    "test_filename = \"test.csv\"\n",
    "\n",
    "def read_csv(data_path, filename):\n",
    "    joined_path = os.path.join(data_path, filename)\n",
    "    return pd.read_csv(joined_path)\n",
    "\n",
    "# Read CSV file into Pandas Dataframes\n",
    "train_df = read_csv(data_path, train_filename)\n",
    "\n",
    "# Defining Data Pre-Processing Pipelines\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, attributes):\n",
    "        self.attributes = attributes\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attributes]\n",
    "\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent = pd.Series([X[c].value_counts().index[0] for c in X], \n",
    "                                       index = X.columns)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.fillna(self.most_frequent)\n",
    "\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "        (\"Select\", DataFrameSelector([\"Age\", \"Fare\", \"SibSp\", \"Parch\"])), # Selects Fields from dataframe\n",
    "        (\"Imputer\", SimpleImputer(strategy=\"median\")),   # Fills in NaN w/ median value for its column\n",
    "        (\"Scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "categories_pipe = Pipeline([\n",
    "        (\"Select\", DataFrameSelector([\"Pclass\", \"Sex\"])), # Selects Fields from dataframe\n",
    "        (\"MostFreqImp\", MostFrequentImputer()), # Fill in NaN with most frequent\n",
    "        (\"OneHot\", OneHotEncoder(sparse=False, categories='auto')), # Onehot encode\n",
    "    ])\n",
    "\n",
    "preprocessing_pipe = FeatureUnion(transformer_list = [\n",
    "        (\"numeric pipeline\", numeric_pipe), \n",
    "        (\"categories pipeline\", categories_pipe)\n",
    "     ]) \n",
    "\n",
    "# Process Input Data Using Pipleines\n",
    "X_data = preprocessing_pipe.fit_transform(train_df)\n",
    "y_data = train_df[\"Survived\"].values.reshape(-1,1)\n",
    "\n",
    "# Process the output data.\n",
    "feature_names = [\"Age\", \"Fare\", \"SibSp\", \"Parch\", \"Class0\", \"class1\",\"Sex0\", \"Sex1\"]\n",
    "\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596, 9) (596, 1) (295, 9) (295, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.33)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the TF.Estimator.DNNClassifier (formerly of TFLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\richa\\\\AppData\\\\Local\\\\Temp\\\\tmp152d877p', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DD025F89E8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\\model.ckpt.\n",
      "INFO:tensorflow:loss = 38.381626, step = 1\n",
      "INFO:tensorflow:global_step/sec: 447.622\n",
      "INFO:tensorflow:loss = 21.983316, step = 101 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.538\n",
      "INFO:tensorflow:loss = 21.217855, step = 201 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.538\n",
      "INFO:tensorflow:loss = 20.855549, step = 301 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.719\n",
      "INFO:tensorflow:loss = 18.536669, step = 401 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 871.886\n",
      "INFO:tensorflow:loss = 17.773914, step = 501 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 911.524\n",
      "INFO:tensorflow:loss = 25.729843, step = 601 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 864.377\n",
      "INFO:tensorflow:loss = 21.810678, step = 701 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 879.54\n",
      "INFO:tensorflow:loss = 20.115366, step = 801 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 856.977\n",
      "INFO:tensorflow:loss = 12.671253, step = 901 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 895.245\n",
      "INFO:tensorflow:loss = 18.845486, step = 1001 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.321\n",
      "INFO:tensorflow:loss = 16.177044, step = 1101 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 887.325\n",
      "INFO:tensorflow:loss = 21.307896, step = 1201 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 864.374\n",
      "INFO:tensorflow:loss = 20.480532, step = 1301 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 600.403\n",
      "INFO:tensorflow:loss = 18.540726, step = 1401 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 642.736\n",
      "INFO:tensorflow:loss = 13.092168, step = 1501 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 672.934\n",
      "INFO:tensorflow:loss = 13.699125, step = 1601 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 783.342\n",
      "INFO:tensorflow:loss = 19.529427, step = 1701 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.578\n",
      "INFO:tensorflow:loss = 20.49198, step = 1801 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 742.72\n",
      "INFO:tensorflow:loss = 18.131166, step = 1901 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.337\n",
      "INFO:tensorflow:loss = 16.043087, step = 2001 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.672\n",
      "INFO:tensorflow:loss = 12.737453, step = 2101 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 691.538\n",
      "INFO:tensorflow:loss = 26.872074, step = 2201 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.587\n",
      "INFO:tensorflow:loss = 20.14505, step = 2301 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 871.888\n",
      "INFO:tensorflow:loss = 17.428999, step = 2401 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.658\n",
      "INFO:tensorflow:loss = 23.612007, step = 2501 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.582\n",
      "INFO:tensorflow:loss = 11.4780035, step = 2601 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.72\n",
      "INFO:tensorflow:loss = 13.60433, step = 2701 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 771.287\n",
      "INFO:tensorflow:loss = 14.203247, step = 2801 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.583\n",
      "INFO:tensorflow:loss = 13.395087, step = 2901 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.655\n",
      "INFO:tensorflow:loss = 12.900935, step = 3001 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 765.399\n",
      "INFO:tensorflow:loss = 12.678525, step = 3101 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 821.86\n",
      "INFO:tensorflow:loss = 13.676514, step = 3201 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 777.26\n",
      "INFO:tensorflow:loss = 22.814781, step = 3301 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 579.584\n",
      "INFO:tensorflow:loss = 16.14307, step = 3401 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.137\n",
      "INFO:tensorflow:loss = 16.56072, step = 3501 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 742.72\n",
      "INFO:tensorflow:loss = 18.072622, step = 3601 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.582\n",
      "INFO:tensorflow:loss = 16.15227, step = 3701 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 802.139\n",
      "INFO:tensorflow:loss = 15.808844, step = 3801 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 835.557\n",
      "INFO:tensorflow:loss = 11.586376, step = 3901 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 821.866\n",
      "INFO:tensorflow:loss = 17.958904, step = 4001 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.578\n",
      "INFO:tensorflow:loss = 25.965998, step = 4101 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.65\n",
      "INFO:tensorflow:loss = 20.961824, step = 4201 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 748.268\n",
      "INFO:tensorflow:loss = 12.407464, step = 4301 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 864.356\n",
      "INFO:tensorflow:loss = 14.689327, step = 4401 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.739\n",
      "INFO:tensorflow:loss = 16.462431, step = 4501 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.606\n",
      "INFO:tensorflow:loss = 17.770634, step = 4601 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.706\n",
      "INFO:tensorflow:loss = 15.8100815, step = 4701 (0.120 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4768 into C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 21.607399.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07T17:15:46Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\\model.ckpt-4768\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-17:15:46\n",
      "INFO:tensorflow:Saving dict for global step 4768: accuracy = 0.8305085, accuracy_baseline = 0.64406776, auc = 0.85167915, auc_precision_recall = 0.82776606, average_loss = 0.4634297, global_step = 4768, label/mean = 0.3559322, loss = 45.570587, precision = 0.7522936, prediction/mean = 0.38629514, recall = 0.7809524\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4768: C:\\Users\\richa\\AppData\\Local\\Temp\\tmp152d877p\\model.ckpt-4768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8305085,\n",
       " 'accuracy_baseline': 0.64406776,\n",
       " 'auc': 0.85167915,\n",
       " 'auc_precision_recall': 0.82776606,\n",
       " 'average_loss': 0.4634297,\n",
       " 'label/mean': 0.3559322,\n",
       " 'loss': 45.570587,\n",
       " 'precision': 0.7522936,\n",
       " 'prediction/mean': 0.38629514,\n",
       " 'recall': 0.7809524,\n",
       " 'global_step': 4768}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construction Phase\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "\n",
    "feature_cols = [tf.feature_column.numeric_column(\"X\", shape=[X_data.shape[1]])]\n",
    "\n",
    "dnn_clf = tf.estimator.DNNClassifier(hidden_units=[20,20], n_classes=2,\n",
    "                                     feature_columns=feature_cols)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_train}, y=y_train, batch_size=50, num_epochs=400, shuffle=True)\n",
    "dnn_clf.train(input_fn=train_input_fn)\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_test}, y=y_test, shuffle=False)\n",
    "eval_results = dnn_clf.evaluate(input_fn=test_input_fn)\n",
    "                                \n",
    "eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Train: 0.6304348111152649 Test:0.6847457885742188\n",
      "1-Train: 0.695652186870575 Test:0.7593220472335815\n",
      "2-Train: 0.760869562625885 Test:0.7762711644172668\n",
      "3-Train: 0.782608687877655 Test:0.806779682636261\n",
      "4-Train: 0.804347813129425 Test:0.806779682636261\n",
      "5-Train: 0.804347813129425 Test:0.8237287998199463\n",
      "6-Train: 0.8478260636329651 Test:0.8305084705352783\n",
      "7-Train: 0.8478260636329651 Test:0.8338983058929443\n",
      "8-Train: 0.8260869383811951 Test:0.8338983058929443\n",
      "9-Train: 0.8478260636329651 Test:0.8406779766082764\n",
      "10-Train: 0.8478260636329651 Test:0.8406779766082764\n",
      "11-Train: 0.8478260636329651 Test:0.8406779766082764\n",
      "12-Train: 0.8478260636329651 Test:0.8406779766082764\n",
      "13-Train: 0.8260869383811951 Test:0.8406779766082764\n",
      "14-Train: 0.8260869383811951 Test:0.8338983058929443\n",
      "15-Train: 0.804347813129425 Test:0.8305084705352783\n",
      "16-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "17-Train: 0.804347813129425 Test:0.8305084705352783\n",
      "18-Train: 0.804347813129425 Test:0.8305084705352783\n",
      "19-Train: 0.804347813129425 Test:0.8271186351776123\n",
      "20-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "21-Train: 0.804347813129425 Test:0.8237287998199463\n",
      "22-Train: 0.804347813129425 Test:0.8271186351776123\n",
      "23-Train: 0.8260869383811951 Test:0.8338983058929443\n",
      "24-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "25-Train: 0.8478260636329651 Test:0.8305084705352783\n",
      "26-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "27-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "28-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "29-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "30-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "31-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "32-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "33-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "34-Train: 0.8260869383811951 Test:0.8169491291046143\n",
      "35-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "36-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "37-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "38-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "39-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "40-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "41-Train: 0.8260869383811951 Test:0.8237287998199463\n",
      "42-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "43-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "44-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "45-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "46-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "47-Train: 0.804347813129425 Test:0.8203389644622803\n",
      "48-Train: 0.8260869383811951 Test:0.8169491291046143\n",
      "49-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "50-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "51-Train: 0.8260869383811951 Test:0.8169491291046143\n",
      "52-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "53-Train: 0.8260869383811951 Test:0.8203389644622803\n",
      "54-Train: 0.8260869383811951 Test:0.8237287998199463\n",
      "55-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "56-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "57-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "58-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "59-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "60-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "61-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "62-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "63-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "64-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "65-Train: 0.8260869383811951 Test:0.8305084705352783\n",
      "66-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "67-Train: 0.8260869383811951 Test:0.8271186351776123\n",
      "68-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "69-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "70-Train: 0.8478260636329651 Test:0.8237287998199463\n",
      "71-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "72-Train: 0.8695651888847351 Test:0.8305084705352783\n",
      "73-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "74-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "75-Train: 0.8478260636329651 Test:0.8203389644622803\n",
      "76-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "77-Train: 0.8478260636329651 Test:0.8305084705352783\n",
      "78-Train: 0.8695651888847351 Test:0.8338983058929443\n",
      "79-Train: 0.8695651888847351 Test:0.8338983058929443\n",
      "80-Train: 0.8478260636329651 Test:0.8271186351776123\n",
      "81-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "82-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "83-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "84-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "85-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "86-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "87-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "88-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "89-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "90-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "91-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "92-Train: 0.8695651888847351 Test:0.8305084705352783\n",
      "93-Train: 0.8695651888847351 Test:0.8237287998199463\n",
      "94-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "95-Train: 0.8695651888847351 Test:0.8305084705352783\n",
      "96-Train: 0.8695651888847351 Test:0.8271186351776123\n",
      "97-Train: 0.8695651888847351 Test:0.8338983058929443\n",
      "98-Train: 0.8695651888847351 Test:0.8305084705352783\n",
      "99-Train: 0.8695651888847351 Test:0.8305084705352783\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "def get_batch(X, iter, size):\n",
    "    return X[(iter*batch_size) : ((iter+1)*batch_size)]\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_instances = X_train.shape[0]\n",
    "\n",
    "# Construction\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_features), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Titanic_MLP\"):\n",
    "    hidden1 = tf.layers.dense(X, 20, name=\"Hidden-1\", activation = tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, 10, name=\"Hidden-2\", activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, 5, name=\"Hidden-3\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden3, 2, name=\"Survived\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"): \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for iteration in range(num_instances // batch_size + 1):\n",
    "            X_batch = get_batch(X_train, iteration, batch_size)\n",
    "            y_batch = get_batch(y_train, iteration, batch_size)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={X: X_batch,\n",
    "                                            y: y_batch.reshape(y_batch.shape[0])})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch,\n",
    "                                            y: y_batch.reshape(y_batch.shape[0])})\n",
    "        acc_val = accuracy.eval(feed_dict={X:X_test, y: y_test.reshape(y_test.shape[0])})\n",
    "        \n",
    "        print(\"{}-Train: {} Test:{}\".format(epoch,\n",
    "                                           acc_train,\n",
    "                                           acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 596 samples, validate on 295 samples\n",
      "Epoch 1/400\n",
      "596/596 [==============================] - 0s 423us/sample - loss: 0.6953 - acc: 0.5134 - val_loss: 0.6740 - val_acc: 0.5932\n",
      "Epoch 2/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.6713 - acc: 0.5839 - val_loss: 0.6451 - val_acc: 0.6576\n",
      "Epoch 3/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.6527 - acc: 0.6124 - val_loss: 0.6257 - val_acc: 0.6678\n",
      "Epoch 4/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.6378 - acc: 0.6309 - val_loss: 0.6086 - val_acc: 0.6814\n",
      "Epoch 5/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.6230 - acc: 0.6409 - val_loss: 0.5913 - val_acc: 0.7085\n",
      "Epoch 6/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.6090 - acc: 0.6628 - val_loss: 0.5736 - val_acc: 0.7559\n",
      "Epoch 7/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5932 - acc: 0.7164 - val_loss: 0.5545 - val_acc: 0.7797\n",
      "Epoch 8/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.5765 - acc: 0.7433 - val_loss: 0.5332 - val_acc: 0.7898\n",
      "Epoch 9/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.5594 - acc: 0.7383 - val_loss: 0.5160 - val_acc: 0.7966\n",
      "Epoch 10/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.5429 - acc: 0.7567 - val_loss: 0.4993 - val_acc: 0.8034\n",
      "Epoch 11/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.5267 - acc: 0.7634 - val_loss: 0.4832 - val_acc: 0.8102\n",
      "Epoch 12/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.5126 - acc: 0.7752 - val_loss: 0.4707 - val_acc: 0.8068\n",
      "Epoch 13/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4985 - acc: 0.7768 - val_loss: 0.4551 - val_acc: 0.8102\n",
      "Epoch 14/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4879 - acc: 0.7802 - val_loss: 0.4450 - val_acc: 0.8102\n",
      "Epoch 15/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4767 - acc: 0.7903 - val_loss: 0.4348 - val_acc: 0.8102\n",
      "Epoch 16/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4704 - acc: 0.7869 - val_loss: 0.4276 - val_acc: 0.8102\n",
      "Epoch 17/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.4644 - acc: 0.7953 - val_loss: 0.4206 - val_acc: 0.8136\n",
      "Epoch 18/400\n",
      "596/596 [==============================] - 0s 40us/sample - loss: 0.4573 - acc: 0.7886 - val_loss: 0.4193 - val_acc: 0.8305\n",
      "Epoch 19/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.4535 - acc: 0.8070 - val_loss: 0.4149 - val_acc: 0.8305\n",
      "Epoch 20/400\n",
      "596/596 [==============================] - 0s 50us/sample - loss: 0.4503 - acc: 0.8087 - val_loss: 0.4129 - val_acc: 0.8237\n",
      "Epoch 21/400\n",
      "596/596 [==============================] - 0s 44us/sample - loss: 0.4487 - acc: 0.8070 - val_loss: 0.4070 - val_acc: 0.8339\n",
      "Epoch 22/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4447 - acc: 0.8171 - val_loss: 0.4078 - val_acc: 0.8305\n",
      "Epoch 23/400\n",
      "596/596 [==============================] - 0s 55us/sample - loss: 0.4439 - acc: 0.8121 - val_loss: 0.4071 - val_acc: 0.8305\n",
      "Epoch 24/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4410 - acc: 0.8104 - val_loss: 0.4040 - val_acc: 0.8339\n",
      "Epoch 25/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4397 - acc: 0.8138 - val_loss: 0.4074 - val_acc: 0.8305\n",
      "Epoch 26/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4374 - acc: 0.8188 - val_loss: 0.4058 - val_acc: 0.8305\n",
      "Epoch 27/400\n",
      "596/596 [==============================] - 0s 52us/sample - loss: 0.4367 - acc: 0.8154 - val_loss: 0.4009 - val_acc: 0.8339\n",
      "Epoch 28/400\n",
      "596/596 [==============================] - 0s 45us/sample - loss: 0.4345 - acc: 0.8188 - val_loss: 0.4076 - val_acc: 0.8237\n",
      "Epoch 29/400\n",
      "596/596 [==============================] - 0s 44us/sample - loss: 0.4322 - acc: 0.8188 - val_loss: 0.3960 - val_acc: 0.8407\n",
      "Epoch 30/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4325 - acc: 0.8205 - val_loss: 0.4033 - val_acc: 0.8271\n",
      "Epoch 31/400\n",
      "596/596 [==============================] - 0s 35us/sample - loss: 0.4294 - acc: 0.8171 - val_loss: 0.3966 - val_acc: 0.8339\n",
      "Epoch 32/400\n",
      "596/596 [==============================] - 0s 38us/sample - loss: 0.4301 - acc: 0.8171 - val_loss: 0.3952 - val_acc: 0.8407\n",
      "Epoch 33/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.4294 - acc: 0.8171 - val_loss: 0.3969 - val_acc: 0.8305\n",
      "Epoch 34/400\n",
      "596/596 [==============================] - 0s 40us/sample - loss: 0.4276 - acc: 0.8104 - val_loss: 0.3932 - val_acc: 0.8407\n",
      "Epoch 35/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.4248 - acc: 0.8154 - val_loss: 0.3948 - val_acc: 0.8339\n",
      "Epoch 36/400\n",
      "596/596 [==============================] - 0s 42us/sample - loss: 0.4255 - acc: 0.8171 - val_loss: 0.3910 - val_acc: 0.8339\n",
      "Epoch 37/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4239 - acc: 0.8121 - val_loss: 0.3955 - val_acc: 0.8339\n",
      "Epoch 38/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4214 - acc: 0.8138 - val_loss: 0.3907 - val_acc: 0.8508\n",
      "Epoch 39/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4240 - acc: 0.8188 - val_loss: 0.3936 - val_acc: 0.8373\n",
      "Epoch 40/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4203 - acc: 0.8171 - val_loss: 0.3908 - val_acc: 0.8441\n",
      "Epoch 41/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.4201 - acc: 0.8188 - val_loss: 0.3955 - val_acc: 0.8339\n",
      "Epoch 42/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4189 - acc: 0.8272 - val_loss: 0.3931 - val_acc: 0.8373\n",
      "Epoch 43/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4182 - acc: 0.8221 - val_loss: 0.3939 - val_acc: 0.8339\n",
      "Epoch 44/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4188 - acc: 0.8205 - val_loss: 0.3883 - val_acc: 0.8441\n",
      "Epoch 45/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4163 - acc: 0.8221 - val_loss: 0.3953 - val_acc: 0.8339\n",
      "Epoch 46/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4152 - acc: 0.8188 - val_loss: 0.3903 - val_acc: 0.8475\n",
      "Epoch 47/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4160 - acc: 0.8171 - val_loss: 0.3937 - val_acc: 0.8373\n",
      "Epoch 48/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4145 - acc: 0.8255 - val_loss: 0.3855 - val_acc: 0.8407\n",
      "Epoch 49/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4161 - acc: 0.8238 - val_loss: 0.3919 - val_acc: 0.8407\n",
      "Epoch 50/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3585 - acc: 0.870 - 0s 30us/sample - loss: 0.4149 - acc: 0.8188 - val_loss: 0.3919 - val_acc: 0.8407\n",
      "Epoch 51/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4149 - acc: 0.8205 - val_loss: 0.3881 - val_acc: 0.8475\n",
      "Epoch 52/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4122 - acc: 0.8238 - val_loss: 0.3844 - val_acc: 0.8407\n",
      "Epoch 53/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4135 - acc: 0.8188 - val_loss: 0.3846 - val_acc: 0.8441\n",
      "Epoch 54/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4130 - acc: 0.8171 - val_loss: 0.3875 - val_acc: 0.8475\n",
      "Epoch 55/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4109 - acc: 0.8188 - val_loss: 0.3850 - val_acc: 0.8475\n",
      "Epoch 56/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4102 - acc: 0.8221 - val_loss: 0.3863 - val_acc: 0.8475\n",
      "Epoch 57/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.4124 - acc: 0.8272 - val_loss: 0.3864 - val_acc: 0.8475\n",
      "Epoch 58/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4099 - acc: 0.8205 - val_loss: 0.3851 - val_acc: 0.8475\n",
      "Epoch 59/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.4104 - acc: 0.8205 - val_loss: 0.3838 - val_acc: 0.8475\n",
      "Epoch 60/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 33us/sample - loss: 0.4078 - acc: 0.8255 - val_loss: 0.3892 - val_acc: 0.8441\n",
      "Epoch 61/400\n",
      "596/596 [==============================] - 0s 44us/sample - loss: 0.4083 - acc: 0.8272 - val_loss: 0.3820 - val_acc: 0.8407\n",
      "Epoch 62/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4081 - acc: 0.8272 - val_loss: 0.3821 - val_acc: 0.8407\n",
      "Epoch 63/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4066 - acc: 0.8289 - val_loss: 0.3818 - val_acc: 0.8441\n",
      "Epoch 64/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.4062 - acc: 0.8289 - val_loss: 0.3856 - val_acc: 0.8475\n",
      "Epoch 65/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4049 - acc: 0.8272 - val_loss: 0.3801 - val_acc: 0.8407\n",
      "Epoch 66/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4047 - acc: 0.8305 - val_loss: 0.3884 - val_acc: 0.8475\n",
      "Epoch 67/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.4071 - acc: 0.8289 - val_loss: 0.3964 - val_acc: 0.8203\n",
      "Epoch 68/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4087 - acc: 0.8272 - val_loss: 0.3822 - val_acc: 0.8441\n",
      "Epoch 69/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4035 - acc: 0.8322 - val_loss: 0.3805 - val_acc: 0.8407\n",
      "Epoch 70/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4072 - acc: 0.8289 - val_loss: 0.3792 - val_acc: 0.8407\n",
      "Epoch 71/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4027 - acc: 0.8339 - val_loss: 0.3854 - val_acc: 0.8475\n",
      "Epoch 72/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4030 - acc: 0.8322 - val_loss: 0.3805 - val_acc: 0.8407\n",
      "Epoch 73/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4101 - acc: 0.8255 - val_loss: 0.3861 - val_acc: 0.8475\n",
      "Epoch 74/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4021 - acc: 0.8356 - val_loss: 0.3804 - val_acc: 0.8407\n",
      "Epoch 75/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4012 - acc: 0.8356 - val_loss: 0.3854 - val_acc: 0.8475\n",
      "Epoch 76/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4012 - acc: 0.8305 - val_loss: 0.3833 - val_acc: 0.8508\n",
      "Epoch 77/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4014 - acc: 0.8339 - val_loss: 0.3887 - val_acc: 0.8373\n",
      "Epoch 78/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.4027 - acc: 0.8255 - val_loss: 0.3786 - val_acc: 0.8407\n",
      "Epoch 79/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4014 - acc: 0.8289 - val_loss: 0.3811 - val_acc: 0.8475\n",
      "Epoch 80/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4010 - acc: 0.8289 - val_loss: 0.3836 - val_acc: 0.8508\n",
      "Epoch 81/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.4049 - acc: 0.8238 - val_loss: 0.3823 - val_acc: 0.8475\n",
      "Epoch 82/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3992 - acc: 0.8322 - val_loss: 0.3790 - val_acc: 0.8407\n",
      "Epoch 83/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4050 - acc: 0.8289 - val_loss: 0.3874 - val_acc: 0.8373\n",
      "Epoch 84/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4009 - acc: 0.8389 - val_loss: 0.3791 - val_acc: 0.8407\n",
      "Epoch 85/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4011 - acc: 0.8372 - val_loss: 0.3782 - val_acc: 0.8407\n",
      "Epoch 86/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3982 - acc: 0.8389 - val_loss: 0.3834 - val_acc: 0.8542\n",
      "Epoch 87/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.4005 - acc: 0.8356 - val_loss: 0.3775 - val_acc: 0.8407\n",
      "Epoch 88/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3994 - acc: 0.8339 - val_loss: 0.3919 - val_acc: 0.8271\n",
      "Epoch 89/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.4003 - acc: 0.8322 - val_loss: 0.3808 - val_acc: 0.8475\n",
      "Epoch 90/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.4005 - acc: 0.8272 - val_loss: 0.3796 - val_acc: 0.8441\n",
      "Epoch 91/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3968 - acc: 0.8423 - val_loss: 0.3856 - val_acc: 0.8475\n",
      "Epoch 92/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3983 - acc: 0.8356 - val_loss: 0.3878 - val_acc: 0.8339\n",
      "Epoch 93/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3977 - acc: 0.8356 - val_loss: 0.3816 - val_acc: 0.8475\n",
      "Epoch 94/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3990 - acc: 0.8305 - val_loss: 0.3800 - val_acc: 0.8441\n",
      "Epoch 95/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3978 - acc: 0.8423 - val_loss: 0.3789 - val_acc: 0.8339\n",
      "Epoch 96/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3958 - acc: 0.8440 - val_loss: 0.3786 - val_acc: 0.8441\n",
      "Epoch 97/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3958 - acc: 0.8372 - val_loss: 0.3845 - val_acc: 0.8407\n",
      "Epoch 98/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3981 - acc: 0.8456 - val_loss: 0.3822 - val_acc: 0.8475\n",
      "Epoch 99/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3953 - acc: 0.8389 - val_loss: 0.3790 - val_acc: 0.8441\n",
      "Epoch 100/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3957 - acc: 0.8406 - val_loss: 0.3794 - val_acc: 0.8475\n",
      "Epoch 101/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3942 - acc: 0.8406 - val_loss: 0.3833 - val_acc: 0.8407\n",
      "Epoch 102/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3956 - acc: 0.8406 - val_loss: 0.3785 - val_acc: 0.8373\n",
      "Epoch 103/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3956 - acc: 0.8406 - val_loss: 0.3874 - val_acc: 0.8339\n",
      "Epoch 104/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3994 - acc: 0.8289 - val_loss: 0.3830 - val_acc: 0.8441\n",
      "Epoch 105/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3921 - acc: 0.8389 - val_loss: 0.3877 - val_acc: 0.8339\n",
      "Epoch 106/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3938 - acc: 0.8372 - val_loss: 0.3869 - val_acc: 0.8339\n",
      "Epoch 107/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3974 - acc: 0.8255 - val_loss: 0.3888 - val_acc: 0.8305\n",
      "Epoch 108/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3928 - acc: 0.8406 - val_loss: 0.3815 - val_acc: 0.8441\n",
      "Epoch 109/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3929 - acc: 0.8389 - val_loss: 0.3850 - val_acc: 0.8373\n",
      "Epoch 110/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3960 - acc: 0.8406 - val_loss: 0.3805 - val_acc: 0.8407\n",
      "Epoch 111/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3939 - acc: 0.8322 - val_loss: 0.3896 - val_acc: 0.8373\n",
      "Epoch 112/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3943 - acc: 0.8339 - val_loss: 0.3804 - val_acc: 0.8475\n",
      "Epoch 113/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3952 - acc: 0.8322 - val_loss: 0.4089 - val_acc: 0.8102\n",
      "Epoch 114/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3957 - acc: 0.8305 - val_loss: 0.3839 - val_acc: 0.8339\n",
      "Epoch 115/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3902 - acc: 0.8423 - val_loss: 0.3815 - val_acc: 0.8475\n",
      "Epoch 116/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3908 - acc: 0.8406 - val_loss: 0.3796 - val_acc: 0.8475\n",
      "Epoch 117/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3936 - acc: 0.8423 - val_loss: 0.3824 - val_acc: 0.8407\n",
      "Epoch 118/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3898 - acc: 0.8440 - val_loss: 0.3817 - val_acc: 0.8407\n",
      "Epoch 119/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3936 - acc: 0.8356 - val_loss: 0.3828 - val_acc: 0.8441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3884 - acc: 0.8406 - val_loss: 0.3869 - val_acc: 0.8339\n",
      "Epoch 121/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3900 - acc: 0.8372 - val_loss: 0.3781 - val_acc: 0.8441\n",
      "Epoch 122/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3872 - acc: 0.8440 - val_loss: 0.3913 - val_acc: 0.8305\n",
      "Epoch 123/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3902 - acc: 0.8339 - val_loss: 0.3797 - val_acc: 0.8508\n",
      "Epoch 124/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3875 - acc: 0.8440 - val_loss: 0.3800 - val_acc: 0.8475\n",
      "Epoch 125/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3889 - acc: 0.8406 - val_loss: 0.3790 - val_acc: 0.8441\n",
      "Epoch 126/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3893 - acc: 0.8473 - val_loss: 0.3804 - val_acc: 0.8441\n",
      "Epoch 127/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3901 - acc: 0.8423 - val_loss: 0.3828 - val_acc: 0.8475\n",
      "Epoch 128/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3883 - acc: 0.8372 - val_loss: 0.3807 - val_acc: 0.8407\n",
      "Epoch 129/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3871 - acc: 0.8406 - val_loss: 0.3801 - val_acc: 0.8475\n",
      "Epoch 130/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3865 - acc: 0.8389 - val_loss: 0.3779 - val_acc: 0.8339\n",
      "Epoch 131/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3908 - acc: 0.8356 - val_loss: 0.3857 - val_acc: 0.8407\n",
      "Epoch 132/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3869 - acc: 0.8339 - val_loss: 0.3835 - val_acc: 0.8441\n",
      "Epoch 133/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3864 - acc: 0.8423 - val_loss: 0.3800 - val_acc: 0.8441\n",
      "Epoch 134/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3863 - acc: 0.8406 - val_loss: 0.3829 - val_acc: 0.8475\n",
      "Epoch 135/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3878 - acc: 0.8440 - val_loss: 0.3832 - val_acc: 0.8475\n",
      "Epoch 136/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3876 - acc: 0.8406 - val_loss: 0.3841 - val_acc: 0.8475\n",
      "Epoch 137/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3845 - acc: 0.8406 - val_loss: 0.3800 - val_acc: 0.8576\n",
      "Epoch 138/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3861 - acc: 0.8456 - val_loss: 0.3797 - val_acc: 0.8508\n",
      "Epoch 139/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3847 - acc: 0.8372 - val_loss: 0.3843 - val_acc: 0.8407\n",
      "Epoch 140/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3832 - acc: 0.8389 - val_loss: 0.3789 - val_acc: 0.8373\n",
      "Epoch 141/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3882 - acc: 0.8389 - val_loss: 0.3863 - val_acc: 0.8407\n",
      "Epoch 142/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3840 - acc: 0.8456 - val_loss: 0.3848 - val_acc: 0.8441\n",
      "Epoch 143/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3846 - acc: 0.8423 - val_loss: 0.3828 - val_acc: 0.8441\n",
      "Epoch 144/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3835 - acc: 0.8423 - val_loss: 0.3853 - val_acc: 0.8441\n",
      "Epoch 145/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3869 - acc: 0.8389 - val_loss: 0.3912 - val_acc: 0.8339\n",
      "Epoch 146/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3875 - acc: 0.8322 - val_loss: 0.3862 - val_acc: 0.8475\n",
      "Epoch 147/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3901 - acc: 0.8389 - val_loss: 0.3844 - val_acc: 0.8407\n",
      "Epoch 148/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3844 - acc: 0.8440 - val_loss: 0.3811 - val_acc: 0.8441\n",
      "Epoch 149/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3856 - acc: 0.8423 - val_loss: 0.3887 - val_acc: 0.8407\n",
      "Epoch 150/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3845 - acc: 0.8456 - val_loss: 0.3817 - val_acc: 0.8475\n",
      "Epoch 151/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3842 - acc: 0.8440 - val_loss: 0.3835 - val_acc: 0.8407\n",
      "Epoch 152/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3870 - acc: 0.8389 - val_loss: 0.3877 - val_acc: 0.8407\n",
      "Epoch 153/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3825 - acc: 0.8456 - val_loss: 0.3882 - val_acc: 0.8373\n",
      "Epoch 154/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3847 - acc: 0.8473 - val_loss: 0.3811 - val_acc: 0.8475\n",
      "Epoch 155/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3827 - acc: 0.8372 - val_loss: 0.3787 - val_acc: 0.8441\n",
      "Epoch 156/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3812 - acc: 0.8423 - val_loss: 0.3853 - val_acc: 0.8441\n",
      "Epoch 157/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3856 - acc: 0.8406 - val_loss: 0.3845 - val_acc: 0.8441\n",
      "Epoch 158/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3818 - acc: 0.8440 - val_loss: 0.3812 - val_acc: 0.8407\n",
      "Epoch 159/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3812 - acc: 0.8456 - val_loss: 0.3808 - val_acc: 0.8373\n",
      "Epoch 160/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3852 - acc: 0.8456 - val_loss: 0.3991 - val_acc: 0.8271\n",
      "Epoch 161/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3834 - acc: 0.8372 - val_loss: 0.3862 - val_acc: 0.8441\n",
      "Epoch 162/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3802 - acc: 0.8456 - val_loss: 0.3867 - val_acc: 0.8407\n",
      "Epoch 163/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3839 - acc: 0.8389 - val_loss: 0.3806 - val_acc: 0.8373\n",
      "Epoch 164/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3815 - acc: 0.8406 - val_loss: 0.3812 - val_acc: 0.8373\n",
      "Epoch 165/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3830 - acc: 0.8389 - val_loss: 0.3939 - val_acc: 0.8305\n",
      "Epoch 166/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.4121 - acc: 0.810 - 0s 22us/sample - loss: 0.3839 - acc: 0.8356 - val_loss: 0.3849 - val_acc: 0.8508\n",
      "Epoch 167/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3834 - acc: 0.8339 - val_loss: 0.3893 - val_acc: 0.8441\n",
      "Epoch 168/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3792 - acc: 0.8423 - val_loss: 0.3823 - val_acc: 0.8475\n",
      "Epoch 169/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3807 - acc: 0.8406 - val_loss: 0.3920 - val_acc: 0.8407\n",
      "Epoch 170/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3832 - acc: 0.8440 - val_loss: 0.3852 - val_acc: 0.8441\n",
      "Epoch 171/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3787 - acc: 0.8456 - val_loss: 0.3893 - val_acc: 0.8407\n",
      "Epoch 172/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3874 - acc: 0.8322 - val_loss: 0.3875 - val_acc: 0.8407\n",
      "Epoch 173/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3857 - acc: 0.8406 - val_loss: 0.3854 - val_acc: 0.8441\n",
      "Epoch 174/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3858 - acc: 0.8305 - val_loss: 0.3834 - val_acc: 0.8407\n",
      "Epoch 175/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3790 - acc: 0.8440 - val_loss: 0.3850 - val_acc: 0.8407\n",
      "Epoch 176/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3794 - acc: 0.8440 - val_loss: 0.3878 - val_acc: 0.8407\n",
      "Epoch 177/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3795 - acc: 0.8372 - val_loss: 0.4040 - val_acc: 0.8203\n",
      "Epoch 178/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3805 - acc: 0.8372 - val_loss: 0.3926 - val_acc: 0.8339\n",
      "Epoch 179/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3806 - acc: 0.8440 - val_loss: 0.3832 - val_acc: 0.8373\n",
      "Epoch 180/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3849 - acc: 0.8372 - val_loss: 0.3884 - val_acc: 0.8475\n",
      "Epoch 181/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3780 - acc: 0.8473 - val_loss: 0.3850 - val_acc: 0.8441\n",
      "Epoch 182/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3803 - acc: 0.8389 - val_loss: 0.3850 - val_acc: 0.8407\n",
      "Epoch 183/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3829 - acc: 0.8406 - val_loss: 0.3884 - val_acc: 0.8407\n",
      "Epoch 184/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3788 - acc: 0.8389 - val_loss: 0.3857 - val_acc: 0.8407\n",
      "Epoch 185/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3765 - acc: 0.8490 - val_loss: 0.3963 - val_acc: 0.8305\n",
      "Epoch 186/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3778 - acc: 0.8389 - val_loss: 0.3837 - val_acc: 0.8339\n",
      "Epoch 187/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3772 - acc: 0.8490 - val_loss: 0.3888 - val_acc: 0.8441\n",
      "Epoch 188/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3769 - acc: 0.8423 - val_loss: 0.3832 - val_acc: 0.8339\n",
      "Epoch 189/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3782 - acc: 0.8490 - val_loss: 0.3880 - val_acc: 0.8441\n",
      "Epoch 190/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3810 - acc: 0.8423 - val_loss: 0.3895 - val_acc: 0.8542\n",
      "Epoch 191/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3793 - acc: 0.8406 - val_loss: 0.3886 - val_acc: 0.8542\n",
      "Epoch 192/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3776 - acc: 0.8507 - val_loss: 0.3880 - val_acc: 0.8407\n",
      "Epoch 193/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3765 - acc: 0.8490 - val_loss: 0.3926 - val_acc: 0.8373\n",
      "Epoch 194/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3749 - acc: 0.8456 - val_loss: 0.3845 - val_acc: 0.8373\n",
      "Epoch 195/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3793 - acc: 0.8440 - val_loss: 0.3866 - val_acc: 0.8441\n",
      "Epoch 196/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3800 - acc: 0.8356 - val_loss: 0.3864 - val_acc: 0.8441\n",
      "Epoch 197/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3822 - acc: 0.8272 - val_loss: 0.3871 - val_acc: 0.8441\n",
      "Epoch 198/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3737 - acc: 0.8440 - val_loss: 0.3896 - val_acc: 0.8407\n",
      "Epoch 199/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3736 - acc: 0.8356 - val_loss: 0.3936 - val_acc: 0.8339\n",
      "Epoch 200/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3764 - acc: 0.8440 - val_loss: 0.3992 - val_acc: 0.8169\n",
      "Epoch 201/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3817 - acc: 0.8305 - val_loss: 0.3865 - val_acc: 0.8373\n",
      "Epoch 202/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3723 - acc: 0.8356 - val_loss: 0.3966 - val_acc: 0.8373\n",
      "Epoch 203/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3769 - acc: 0.8339 - val_loss: 0.3886 - val_acc: 0.8441\n",
      "Epoch 204/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3741 - acc: 0.8490 - val_loss: 0.3937 - val_acc: 0.8407\n",
      "Epoch 205/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3761 - acc: 0.8423 - val_loss: 0.3898 - val_acc: 0.8475\n",
      "Epoch 206/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3778 - acc: 0.8440 - val_loss: 0.3845 - val_acc: 0.8407\n",
      "Epoch 207/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3716 - acc: 0.8490 - val_loss: 0.3959 - val_acc: 0.8373\n",
      "Epoch 208/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3760 - acc: 0.8440 - val_loss: 0.3858 - val_acc: 0.8373\n",
      "Epoch 209/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3748 - acc: 0.8440 - val_loss: 0.3859 - val_acc: 0.8373\n",
      "Epoch 210/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3769 - acc: 0.8406 - val_loss: 0.3875 - val_acc: 0.8339\n",
      "Epoch 211/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3723 - acc: 0.8440 - val_loss: 0.3856 - val_acc: 0.8373\n",
      "Epoch 212/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3745 - acc: 0.8423 - val_loss: 0.3875 - val_acc: 0.8339\n",
      "Epoch 213/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3730 - acc: 0.8423 - val_loss: 0.3900 - val_acc: 0.8475\n",
      "Epoch 214/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3711 - acc: 0.8423 - val_loss: 0.3851 - val_acc: 0.8373\n",
      "Epoch 215/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3739 - acc: 0.8423 - val_loss: 0.3874 - val_acc: 0.8373\n",
      "Epoch 216/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3741 - acc: 0.8423 - val_loss: 0.3937 - val_acc: 0.8441\n",
      "Epoch 217/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3714 - acc: 0.8440 - val_loss: 0.3861 - val_acc: 0.8373\n",
      "Epoch 218/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3789 - acc: 0.8423 - val_loss: 0.3971 - val_acc: 0.8475\n",
      "Epoch 219/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3728 - acc: 0.8389 - val_loss: 0.3848 - val_acc: 0.8305\n",
      "Epoch 220/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3729 - acc: 0.8440 - val_loss: 0.3908 - val_acc: 0.8339\n",
      "Epoch 221/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3768 - acc: 0.8406 - val_loss: 0.3939 - val_acc: 0.8407\n",
      "Epoch 222/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3706 - acc: 0.8490 - val_loss: 0.3874 - val_acc: 0.8373\n",
      "Epoch 223/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3752 - acc: 0.8456 - val_loss: 0.3917 - val_acc: 0.8441\n",
      "Epoch 224/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3750 - acc: 0.8473 - val_loss: 0.3892 - val_acc: 0.8339\n",
      "Epoch 225/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3700 - acc: 0.8523 - val_loss: 0.4004 - val_acc: 0.8339\n",
      "Epoch 226/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3757 - acc: 0.8490 - val_loss: 0.3893 - val_acc: 0.8407\n",
      "Epoch 227/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3724 - acc: 0.8406 - val_loss: 0.3874 - val_acc: 0.8339\n",
      "Epoch 228/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3681 - acc: 0.8473 - val_loss: 0.3890 - val_acc: 0.8407\n",
      "Epoch 229/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3714 - acc: 0.8473 - val_loss: 0.3895 - val_acc: 0.8305\n",
      "Epoch 230/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3711 - acc: 0.8523 - val_loss: 0.3872 - val_acc: 0.8373\n",
      "Epoch 231/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3694 - acc: 0.8456 - val_loss: 0.3961 - val_acc: 0.8373\n",
      "Epoch 232/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3676 - acc: 0.8473 - val_loss: 0.3925 - val_acc: 0.8339\n",
      "Epoch 233/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3732 - acc: 0.8456 - val_loss: 0.3973 - val_acc: 0.8407\n",
      "Epoch 234/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3750 - acc: 0.8356 - val_loss: 0.3945 - val_acc: 0.8339\n",
      "Epoch 235/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3694 - acc: 0.8406 - val_loss: 0.3934 - val_acc: 0.8339\n",
      "Epoch 236/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3720 - acc: 0.8356 - val_loss: 0.3908 - val_acc: 0.8407\n",
      "Epoch 237/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3713 - acc: 0.8356 - val_loss: 0.3911 - val_acc: 0.8441\n",
      "Epoch 238/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3702 - acc: 0.8490 - val_loss: 0.3897 - val_acc: 0.8305\n",
      "Epoch 239/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3681 - acc: 0.8473 - val_loss: 0.3996 - val_acc: 0.8373\n",
      "Epoch 240/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3675 - acc: 0.8473 - val_loss: 0.4000 - val_acc: 0.8373\n",
      "Epoch 241/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3642 - acc: 0.8456 - val_loss: 0.3929 - val_acc: 0.8441\n",
      "Epoch 242/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3727 - acc: 0.8389 - val_loss: 0.3891 - val_acc: 0.8305\n",
      "Epoch 243/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3717 - acc: 0.8440 - val_loss: 0.4044 - val_acc: 0.8339\n",
      "Epoch 244/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3689 - acc: 0.8389 - val_loss: 0.3967 - val_acc: 0.8305\n",
      "Epoch 245/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3662 - acc: 0.8557 - val_loss: 0.4079 - val_acc: 0.8339\n",
      "Epoch 246/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3697 - acc: 0.8456 - val_loss: 0.4068 - val_acc: 0.8339\n",
      "Epoch 247/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3664 - acc: 0.8423 - val_loss: 0.3929 - val_acc: 0.8339\n",
      "Epoch 248/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3735 - acc: 0.8456 - val_loss: 0.3935 - val_acc: 0.8441\n",
      "Epoch 249/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3674 - acc: 0.8372 - val_loss: 0.3983 - val_acc: 0.8373\n",
      "Epoch 250/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3657 - acc: 0.8389 - val_loss: 0.3940 - val_acc: 0.8305\n",
      "Epoch 251/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3670 - acc: 0.8440 - val_loss: 0.4011 - val_acc: 0.8407\n",
      "Epoch 252/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3813 - acc: 0.8255 - val_loss: 0.3975 - val_acc: 0.8339\n",
      "Epoch 253/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3693 - acc: 0.8372 - val_loss: 0.4009 - val_acc: 0.8339\n",
      "Epoch 254/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3710 - acc: 0.8523 - val_loss: 0.3924 - val_acc: 0.8271\n",
      "Epoch 255/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3653 - acc: 0.8440 - val_loss: 0.4056 - val_acc: 0.8271\n",
      "Epoch 256/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3759 - acc: 0.8490 - val_loss: 0.4018 - val_acc: 0.8271\n",
      "Epoch 257/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3740 - acc: 0.8423 - val_loss: 0.4021 - val_acc: 0.8407\n",
      "Epoch 258/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3660 - acc: 0.8389 - val_loss: 0.4173 - val_acc: 0.8237\n",
      "Epoch 259/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3668 - acc: 0.8440 - val_loss: 0.4382 - val_acc: 0.8068\n",
      "Epoch 260/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3721 - acc: 0.8440 - val_loss: 0.3950 - val_acc: 0.8441\n",
      "Epoch 261/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3650 - acc: 0.8440 - val_loss: 0.3980 - val_acc: 0.8441\n",
      "Epoch 262/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3712 - acc: 0.8423 - val_loss: 0.3999 - val_acc: 0.8373\n",
      "Epoch 263/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3678 - acc: 0.8456 - val_loss: 0.3936 - val_acc: 0.8373\n",
      "Epoch 264/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3639 - acc: 0.8473 - val_loss: 0.4107 - val_acc: 0.8407\n",
      "Epoch 265/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3678 - acc: 0.8473 - val_loss: 0.3953 - val_acc: 0.8237\n",
      "Epoch 266/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3729 - acc: 0.8440 - val_loss: 0.3926 - val_acc: 0.8441\n",
      "Epoch 267/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3654 - acc: 0.8423 - val_loss: 0.4004 - val_acc: 0.8475\n",
      "Epoch 268/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3669 - acc: 0.8523 - val_loss: 0.3948 - val_acc: 0.8339\n",
      "Epoch 269/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3623 - acc: 0.8456 - val_loss: 0.3988 - val_acc: 0.8475\n",
      "Epoch 270/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3640 - acc: 0.8389 - val_loss: 0.3994 - val_acc: 0.8373\n",
      "Epoch 271/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3791 - acc: 0.8372 - val_loss: 0.4112 - val_acc: 0.8339\n",
      "Epoch 272/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3682 - acc: 0.8356 - val_loss: 0.3978 - val_acc: 0.8271\n",
      "Epoch 273/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3641 - acc: 0.8440 - val_loss: 0.3940 - val_acc: 0.8305\n",
      "Epoch 274/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3730 - acc: 0.8473 - val_loss: 0.4018 - val_acc: 0.8373\n",
      "Epoch 275/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3649 - acc: 0.8456 - val_loss: 0.4033 - val_acc: 0.8475\n",
      "Epoch 276/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3628 - acc: 0.8473 - val_loss: 0.3985 - val_acc: 0.8305\n",
      "Epoch 277/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3657 - acc: 0.8473 - val_loss: 0.4165 - val_acc: 0.8237\n",
      "Epoch 278/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3649 - acc: 0.8423 - val_loss: 0.4067 - val_acc: 0.8407\n",
      "Epoch 279/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3653 - acc: 0.8406 - val_loss: 0.3974 - val_acc: 0.8373\n",
      "Epoch 280/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3614 - acc: 0.8456 - val_loss: 0.4032 - val_acc: 0.8339\n",
      "Epoch 281/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3671 - acc: 0.8406 - val_loss: 0.4084 - val_acc: 0.8407\n",
      "Epoch 282/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3668 - acc: 0.8473 - val_loss: 0.4011 - val_acc: 0.8475\n",
      "Epoch 283/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3650 - acc: 0.8372 - val_loss: 0.4257 - val_acc: 0.8237\n",
      "Epoch 284/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3673 - acc: 0.8523 - val_loss: 0.3959 - val_acc: 0.8339\n",
      "Epoch 285/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3683 - acc: 0.8456 - val_loss: 0.4064 - val_acc: 0.8407\n",
      "Epoch 286/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3738 - acc: 0.8473 - val_loss: 0.4064 - val_acc: 0.8407\n",
      "Epoch 287/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3609 - acc: 0.8507 - val_loss: 0.3955 - val_acc: 0.8339\n",
      "Epoch 288/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3672 - acc: 0.8423 - val_loss: 0.4011 - val_acc: 0.8339\n",
      "Epoch 289/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3624 - acc: 0.8456 - val_loss: 0.4107 - val_acc: 0.8441\n",
      "Epoch 290/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3642 - acc: 0.8473 - val_loss: 0.3951 - val_acc: 0.8373\n",
      "Epoch 291/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3627 - acc: 0.8440 - val_loss: 0.4014 - val_acc: 0.8475\n",
      "Epoch 292/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3641 - acc: 0.8456 - val_loss: 0.4046 - val_acc: 0.8373\n",
      "Epoch 293/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3681 - acc: 0.8406 - val_loss: 0.4071 - val_acc: 0.8475\n",
      "Epoch 294/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3581 - acc: 0.8490 - val_loss: 0.4024 - val_acc: 0.8508\n",
      "Epoch 295/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3607 - acc: 0.8423 - val_loss: 0.4166 - val_acc: 0.8305\n",
      "Epoch 296/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3668 - acc: 0.8372 - val_loss: 0.3987 - val_acc: 0.8271\n",
      "Epoch 297/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3669 - acc: 0.8389 - val_loss: 0.3962 - val_acc: 0.8475\n",
      "Epoch 298/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3603 - acc: 0.8507 - val_loss: 0.4193 - val_acc: 0.8305\n",
      "Epoch 299/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3614 - acc: 0.8389 - val_loss: 0.4053 - val_acc: 0.8407\n",
      "Epoch 300/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3611 - acc: 0.8456 - val_loss: 0.4068 - val_acc: 0.8136\n",
      "Epoch 301/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3709 - acc: 0.8423 - val_loss: 0.4166 - val_acc: 0.8305\n",
      "Epoch 302/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3689 - acc: 0.8372 - val_loss: 0.4293 - val_acc: 0.8339\n",
      "Epoch 303/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3678 - acc: 0.8490 - val_loss: 0.3997 - val_acc: 0.8407\n",
      "Epoch 304/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3595 - acc: 0.8440 - val_loss: 0.3974 - val_acc: 0.8305\n",
      "Epoch 305/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3686 - acc: 0.8440 - val_loss: 0.4071 - val_acc: 0.8169\n",
      "Epoch 306/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3605 - acc: 0.8440 - val_loss: 0.4149 - val_acc: 0.8407\n",
      "Epoch 307/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3644 - acc: 0.8490 - val_loss: 0.4053 - val_acc: 0.8407\n",
      "Epoch 308/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3591 - acc: 0.8440 - val_loss: 0.4007 - val_acc: 0.8271\n",
      "Epoch 309/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3687 - acc: 0.8372 - val_loss: 0.4261 - val_acc: 0.8034\n",
      "Epoch 310/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3612 - acc: 0.8389 - val_loss: 0.4223 - val_acc: 0.8237\n",
      "Epoch 311/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3626 - acc: 0.8456 - val_loss: 0.3998 - val_acc: 0.8305\n",
      "Epoch 312/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3642 - acc: 0.8440 - val_loss: 0.4163 - val_acc: 0.8339\n",
      "Epoch 313/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3757 - acc: 0.8356 - val_loss: 0.4101 - val_acc: 0.8339\n",
      "Epoch 314/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3665 - acc: 0.8440 - val_loss: 0.3975 - val_acc: 0.8407\n",
      "Epoch 315/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3628 - acc: 0.8456 - val_loss: 0.4032 - val_acc: 0.8373\n",
      "Epoch 316/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3567 - acc: 0.8456 - val_loss: 0.4253 - val_acc: 0.8169\n",
      "Epoch 317/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3682 - acc: 0.8356 - val_loss: 0.4293 - val_acc: 0.8102\n",
      "Epoch 318/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3710 - acc: 0.8339 - val_loss: 0.4083 - val_acc: 0.8237\n",
      "Epoch 319/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3597 - acc: 0.8423 - val_loss: 0.4010 - val_acc: 0.8373\n",
      "Epoch 320/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3687 - acc: 0.8490 - val_loss: 0.4030 - val_acc: 0.8373\n",
      "Epoch 321/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3601 - acc: 0.8456 - val_loss: 0.4035 - val_acc: 0.8373\n",
      "Epoch 322/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3575 - acc: 0.8473 - val_loss: 0.4049 - val_acc: 0.8237\n",
      "Epoch 323/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3809 - acc: 0.8305 - val_loss: 0.4091 - val_acc: 0.8271\n",
      "Epoch 324/400\n",
      "596/596 [==============================] - 0s 37us/sample - loss: 0.3561 - acc: 0.8540 - val_loss: 0.4407 - val_acc: 0.8339\n",
      "Epoch 325/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3568 - acc: 0.8490 - val_loss: 0.4212 - val_acc: 0.8373\n",
      "Epoch 326/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3587 - acc: 0.8490 - val_loss: 0.4250 - val_acc: 0.8305\n",
      "Epoch 327/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3662 - acc: 0.8389 - val_loss: 0.4031 - val_acc: 0.8441\n",
      "Epoch 328/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3897 - acc: 0.850 - 0s 30us/sample - loss: 0.3615 - acc: 0.8473 - val_loss: 0.4065 - val_acc: 0.8339\n",
      "Epoch 329/400\n",
      "596/596 [==============================] - 0s 32us/sample - loss: 0.3580 - acc: 0.8423 - val_loss: 0.4141 - val_acc: 0.8475\n",
      "Epoch 330/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3580 - acc: 0.8456 - val_loss: 0.4090 - val_acc: 0.8407\n",
      "Epoch 331/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3569 - acc: 0.8507 - val_loss: 0.4211 - val_acc: 0.8339\n",
      "Epoch 332/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3695 - acc: 0.8406 - val_loss: 0.4068 - val_acc: 0.8339\n",
      "Epoch 333/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3563 - acc: 0.8540 - val_loss: 0.4184 - val_acc: 0.8441\n",
      "Epoch 334/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3553 - acc: 0.8507 - val_loss: 0.4029 - val_acc: 0.8339\n",
      "Epoch 335/400\n",
      "596/596 [==============================] - 0s 33us/sample - loss: 0.3577 - acc: 0.8440 - val_loss: 0.4123 - val_acc: 0.8441\n",
      "Epoch 336/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3599 - acc: 0.8440 - val_loss: 0.4347 - val_acc: 0.8271\n",
      "Epoch 337/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3592 - acc: 0.8456 - val_loss: 0.4343 - val_acc: 0.8305\n",
      "Epoch 338/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3593 - acc: 0.8540 - val_loss: 0.4037 - val_acc: 0.8373\n",
      "Epoch 339/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3619 - acc: 0.8372 - val_loss: 0.4069 - val_acc: 0.8305\n",
      "Epoch 340/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3629 - acc: 0.8389 - val_loss: 0.4033 - val_acc: 0.8407\n",
      "Epoch 341/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3640 - acc: 0.8372 - val_loss: 0.4138 - val_acc: 0.8441\n",
      "Epoch 342/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3601 - acc: 0.8423 - val_loss: 0.4463 - val_acc: 0.8102\n",
      "Epoch 343/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3687 - acc: 0.8372 - val_loss: 0.4013 - val_acc: 0.8339\n",
      "Epoch 344/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3558 - acc: 0.8507 - val_loss: 0.4021 - val_acc: 0.8373\n",
      "Epoch 345/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3602 - acc: 0.8490 - val_loss: 0.4319 - val_acc: 0.8373\n",
      "Epoch 346/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3604 - acc: 0.8440 - val_loss: 0.4079 - val_acc: 0.8373\n",
      "Epoch 347/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3608 - acc: 0.8456 - val_loss: 0.4059 - val_acc: 0.8305\n",
      "Epoch 348/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3568 - acc: 0.8456 - val_loss: 0.4218 - val_acc: 0.8441\n",
      "Epoch 349/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3537 - acc: 0.8523 - val_loss: 0.4318 - val_acc: 0.8305\n",
      "Epoch 350/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3668 - acc: 0.8406 - val_loss: 0.4088 - val_acc: 0.8305\n",
      "Epoch 351/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3716 - acc: 0.8389 - val_loss: 0.4186 - val_acc: 0.8475\n",
      "Epoch 352/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3643 - acc: 0.8406 - val_loss: 0.4089 - val_acc: 0.8407\n",
      "Epoch 353/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3668 - acc: 0.8389 - val_loss: 0.4066 - val_acc: 0.8475\n",
      "Epoch 354/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3563 - acc: 0.8557 - val_loss: 0.4130 - val_acc: 0.8475\n",
      "Epoch 355/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3811 - acc: 0.8255 - val_loss: 0.4242 - val_acc: 0.8373\n",
      "Epoch 356/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3566 - acc: 0.8389 - val_loss: 0.4371 - val_acc: 0.8271\n",
      "Epoch 357/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3558 - acc: 0.8540 - val_loss: 0.4281 - val_acc: 0.8271\n",
      "Epoch 358/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3588 - acc: 0.8372 - val_loss: 0.4302 - val_acc: 0.8102\n",
      "Epoch 359/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3559 - acc: 0.8406 - val_loss: 0.4411 - val_acc: 0.8271\n",
      "Epoch 360/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3584 - acc: 0.8473 - val_loss: 0.4513 - val_acc: 0.8102\n",
      "Epoch 361/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3592 - acc: 0.8356 - val_loss: 0.4014 - val_acc: 0.8407\n",
      "Epoch 362/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3629 - acc: 0.8423 - val_loss: 0.4033 - val_acc: 0.8305\n",
      "Epoch 363/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3551 - acc: 0.8423 - val_loss: 0.4096 - val_acc: 0.8407\n",
      "Epoch 364/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3572 - acc: 0.8423 - val_loss: 0.4018 - val_acc: 0.8305\n",
      "Epoch 365/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3619 - acc: 0.8507 - val_loss: 0.4203 - val_acc: 0.8407\n",
      "Epoch 366/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3525 - acc: 0.8540 - val_loss: 0.4120 - val_acc: 0.8373\n",
      "Epoch 367/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3554 - acc: 0.8456 - val_loss: 0.4176 - val_acc: 0.8441\n",
      "Epoch 368/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3570 - acc: 0.8523 - val_loss: 0.4208 - val_acc: 0.8441\n",
      "Epoch 369/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3596 - acc: 0.8490 - val_loss: 0.4176 - val_acc: 0.8441\n",
      "Epoch 370/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3535 - acc: 0.8389 - val_loss: 0.4186 - val_acc: 0.8237\n",
      "Epoch 371/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3646 - acc: 0.8389 - val_loss: 0.4088 - val_acc: 0.8407\n",
      "Epoch 372/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3537 - acc: 0.8507 - val_loss: 0.4149 - val_acc: 0.8339\n",
      "Epoch 373/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3510 - acc: 0.8507 - val_loss: 0.4430 - val_acc: 0.8203\n",
      "Epoch 374/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3551 - acc: 0.8440 - val_loss: 0.4179 - val_acc: 0.8407\n",
      "Epoch 375/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3636 - acc: 0.8339 - val_loss: 0.4037 - val_acc: 0.8373\n",
      "Epoch 376/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3577 - acc: 0.8456 - val_loss: 0.4037 - val_acc: 0.8407\n",
      "Epoch 377/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3564 - acc: 0.8507 - val_loss: 0.4219 - val_acc: 0.8407\n",
      "Epoch 378/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3515 - acc: 0.8507 - val_loss: 0.4341 - val_acc: 0.8339\n",
      "Epoch 379/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3561 - acc: 0.8473 - val_loss: 0.4197 - val_acc: 0.8441\n",
      "Epoch 380/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3643 - acc: 0.8456 - val_loss: 0.4197 - val_acc: 0.8203\n",
      "Epoch 381/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3593 - acc: 0.8490 - val_loss: 0.4145 - val_acc: 0.8339\n",
      "Epoch 382/400\n",
      "596/596 [==============================] - 0s 18us/sample - loss: 0.3486 - acc: 0.8490 - val_loss: 0.4440 - val_acc: 0.8508\n",
      "Epoch 383/400\n",
      "596/596 [==============================] - 0s 20us/sample - loss: 0.3539 - acc: 0.8456 - val_loss: 0.4241 - val_acc: 0.8203\n",
      "Epoch 384/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3555 - acc: 0.8423 - val_loss: 0.4066 - val_acc: 0.8339\n",
      "Epoch 385/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3635 - acc: 0.8389 - val_loss: 0.4367 - val_acc: 0.8339\n",
      "Epoch 386/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3538 - acc: 0.8473 - val_loss: 0.4141 - val_acc: 0.8203\n",
      "Epoch 387/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3825 - acc: 0.8356 - val_loss: 0.4210 - val_acc: 0.8508\n",
      "Epoch 388/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3601 - acc: 0.8607 - val_loss: 0.4224 - val_acc: 0.8475\n",
      "Epoch 389/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3532 - acc: 0.8473 - val_loss: 0.4271 - val_acc: 0.8407\n",
      "Epoch 390/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3501 - acc: 0.8507 - val_loss: 0.4097 - val_acc: 0.8339\n",
      "Epoch 391/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3560 - acc: 0.8523 - val_loss: 0.4223 - val_acc: 0.8373\n",
      "Epoch 392/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3527 - acc: 0.8473 - val_loss: 0.4199 - val_acc: 0.8508\n",
      "Epoch 393/400\n",
      "596/596 [==============================] - 0s 28us/sample - loss: 0.3465 - acc: 0.8591 - val_loss: 0.4277 - val_acc: 0.8271\n",
      "Epoch 394/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3714 - acc: 0.8406 - val_loss: 0.4301 - val_acc: 0.8271\n",
      "Epoch 395/400\n",
      "596/596 [==============================] - 0s 30us/sample - loss: 0.3517 - acc: 0.8624 - val_loss: 0.4086 - val_acc: 0.8441\n",
      "Epoch 396/400\n",
      "596/596 [==============================] - 0s 27us/sample - loss: 0.3849 - acc: 0.8305 - val_loss: 0.4121 - val_acc: 0.8373\n",
      "Epoch 397/400\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.3125 - acc: 0.880 - 0s 30us/sample - loss: 0.3555 - acc: 0.8372 - val_loss: 0.4103 - val_acc: 0.8237\n",
      "Epoch 398/400\n",
      "596/596 [==============================] - 0s 25us/sample - loss: 0.3593 - acc: 0.8456 - val_loss: 0.4154 - val_acc: 0.8373\n",
      "Epoch 399/400\n",
      "596/596 [==============================] - 0s 23us/sample - loss: 0.3645 - acc: 0.8490 - val_loss: 0.4460 - val_acc: 0.8136\n",
      "Epoch 400/400\n",
      "596/596 [==============================] - 0s 22us/sample - loss: 0.3590 - acc: 0.8423 - val_loss: 0.4218 - val_acc: 0.8542\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "#label_train = encoder.fit_transform(y_train)\n",
    "label_test = encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(20, activation='relu', input_shape=(9,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = tf.train.GradientDescentOptimizer(0.1),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    model.fit(X_train, y_train, epochs=400, batch_size=100, validation_data = (X_test, y_test))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing Cost Estimator\n",
    "\n",
    "Building off the classifier examples above, this section shows ensemble regressors using bagging and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,) (167, 13) (167,)\n"
     ]
    }
   ],
   "source": [
    "# Load Data Set\n",
    "from sklearn import datasets\n",
    "boston_housing_data = datasets.load_boston()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bouston_housing_data_instances = scaler.fit_transform(boston_housing_data.data)\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    \"\"\"\n",
    "    Plots performance on the training set and testing (validation) set.\n",
    "    X-axis - number of training samples used\n",
    "    Y-axis - RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.20)\n",
    "    \n",
    "    training_errors, validation_errors = [], []\n",
    "    \n",
    "    for m in range(1, len(train_X)):\n",
    "        \n",
    "        model.fit(train_X[:m], train_y[:m])\n",
    "        \n",
    "        train_pred = model.predict(train_X)\n",
    "        test_pred = model.predict(test_X)\n",
    "        \n",
    "        training_errors.append(np.sqrt(mean_squared_error(train_y, train_pred)))\n",
    "        validation_errors.append(np.sqrt(mean_squared_error(test_y, test_pred)))\n",
    "        \n",
    "    plt.plot(training_errors, \"r-+\", label=\"train\")\n",
    "    plt.plot(validation_errors, \"b-\", label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.axis([0, 80, 0, 3])\n",
    "    \n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Train: 463.00775146484375 Test:552.0066528320312\n",
      "1-Train: 411.280517578125 Test:495.46173095703125\n",
      "2-Train: 366.30853271484375 Test:445.9897155761719\n",
      "3-Train: 327.220703125 Test:402.6980285644531\n",
      "4-Train: 293.2575378417969 Test:364.8068542480469\n",
      "5-Train: 263.7569580078125 Test:331.6368408203125\n",
      "6-Train: 238.14178466796875 Test:302.5943298339844\n",
      "7-Train: 215.9088592529297 Test:277.1589050292969\n",
      "8-Train: 196.61973571777344 Test:254.87818908691406\n",
      "9-Train: 179.8921661376953 Test:235.35629272460938\n",
      "10-Train: 165.39317321777344 Test:218.2465362548828\n",
      "11-Train: 152.83251953125 Test:203.24673461914062\n",
      "12-Train: 141.95745849609375 Test:190.0932159423828\n",
      "13-Train: 132.54766845703125 Test:178.5550994873047\n",
      "14-Train: 124.41130065917969 Test:168.42994689941406\n",
      "15-Train: 117.38134002685547 Test:159.54254150390625\n",
      "16-Train: 111.31227111816406 Test:151.7383575439453\n",
      "17-Train: 106.07746124267578 Test:144.8821258544922\n",
      "18-Train: 101.56668090820312 Test:138.8567657470703\n",
      "19-Train: 97.68391418457031 Test:133.5591583251953\n",
      "20-Train: 94.34575653076172 Test:128.89923095703125\n",
      "21-Train: 91.4795150756836 Test:124.79830169677734\n",
      "22-Train: 89.02202606201172 Test:121.18726348876953\n",
      "23-Train: 86.91838073730469 Test:118.00591278076172\n",
      "24-Train: 85.12092590332031 Test:115.20183563232422\n",
      "25-Train: 83.58804321289062 Test:112.7284164428711\n",
      "26-Train: 82.28380584716797 Test:110.545654296875\n",
      "27-Train: 81.17683410644531 Test:108.61785125732422\n",
      "28-Train: 80.239990234375 Test:106.91426849365234\n",
      "29-Train: 79.44966125488281 Test:105.40770721435547\n",
      "30-Train: 78.78548431396484 Test:104.07430267333984\n",
      "31-Train: 78.22966003417969 Test:102.89348602294922\n",
      "32-Train: 77.76683044433594 Test:101.8465805053711\n",
      "33-Train: 77.38369750976562 Test:100.91790771484375\n",
      "34-Train: 77.06876373291016 Test:100.09324645996094\n",
      "35-Train: 76.81207275390625 Test:99.36038970947266\n",
      "36-Train: 76.60499572753906 Test:98.70845794677734\n",
      "37-Train: 76.44013214111328 Test:98.1281509399414\n",
      "38-Train: 76.3110580444336 Test:97.61083221435547\n",
      "39-Train: 76.21221923828125 Test:97.1492919921875\n",
      "40-Train: 76.13887023925781 Test:96.7373046875\n",
      "41-Train: 76.08694458007812 Test:96.36893463134766\n",
      "42-Train: 76.05289459228516 Test:96.0392837524414\n",
      "43-Train: 76.03370666503906 Test:95.74378967285156\n",
      "44-Train: 76.02680206298828 Test:95.47882843017578\n",
      "45-Train: 76.02994537353516 Test:95.24097442626953\n",
      "46-Train: 76.04129028320312 Test:95.02706909179688\n",
      "47-Train: 76.05921173095703 Test:94.8345947265625\n",
      "48-Train: 76.08233642578125 Test:94.6611557006836\n",
      "49-Train: 76.10950469970703 Test:94.5048599243164\n",
      "50-Train: 76.13977813720703 Test:94.36358642578125\n",
      "51-Train: 76.17230224609375 Test:94.2358627319336\n",
      "52-Train: 76.20637512207031 Test:94.12035369873047\n",
      "53-Train: 76.2414779663086 Test:94.01557922363281\n",
      "54-Train: 76.27706909179688 Test:93.92044067382812\n",
      "55-Train: 76.31277465820312 Test:93.83409118652344\n",
      "56-Train: 76.34830474853516 Test:93.75565338134766\n",
      "57-Train: 76.38335418701172 Test:93.6842269897461\n",
      "58-Train: 76.41775512695312 Test:93.6190414428711\n",
      "59-Train: 76.45130920410156 Test:93.55970001220703\n",
      "60-Train: 76.48389434814453 Test:93.50548553466797\n",
      "61-Train: 76.51544189453125 Test:93.45587921142578\n",
      "62-Train: 76.54585266113281 Test:93.41062927246094\n",
      "63-Train: 76.57510375976562 Test:93.36888885498047\n",
      "64-Train: 76.6031265258789 Test:93.33091735839844\n",
      "65-Train: 76.62995910644531 Test:93.29589080810547\n",
      "66-Train: 76.65557861328125 Test:93.26383209228516\n",
      "67-Train: 76.67999267578125 Test:93.23442840576172\n",
      "68-Train: 76.70320892333984 Test:93.20716094970703\n",
      "69-Train: 76.72525787353516 Test:93.18206024169922\n",
      "70-Train: 76.74618530273438 Test:93.1591567993164\n",
      "71-Train: 76.76602935791016 Test:93.137939453125\n",
      "72-Train: 76.7847900390625 Test:93.11830139160156\n",
      "73-Train: 76.80253601074219 Test:93.1002197265625\n",
      "74-Train: 76.81929779052734 Test:93.08345031738281\n",
      "75-Train: 76.83513641357422 Test:93.06805419921875\n",
      "76-Train: 76.85005187988281 Test:93.05377197265625\n",
      "77-Train: 76.86412811279297 Test:93.04044342041016\n",
      "78-Train: 76.87738800048828 Test:93.0283203125\n",
      "79-Train: 76.88983917236328 Test:93.01692199707031\n",
      "80-Train: 76.90158081054688 Test:93.00636291503906\n",
      "81-Train: 76.91261291503906 Test:92.99655151367188\n",
      "82-Train: 76.92296600341797 Test:92.98744201660156\n",
      "83-Train: 76.93270111083984 Test:92.97918701171875\n",
      "84-Train: 76.94184875488281 Test:92.97144317626953\n",
      "85-Train: 76.95042419433594 Test:92.96417999267578\n",
      "86-Train: 76.95848083496094 Test:92.95732879638672\n",
      "87-Train: 76.96604919433594 Test:92.95108032226562\n",
      "88-Train: 76.97313690185547 Test:92.94528198242188\n",
      "89-Train: 76.97978973388672 Test:92.93972778320312\n",
      "90-Train: 76.98601531982422 Test:92.93476104736328\n",
      "91-Train: 76.99186706542969 Test:92.93009948730469\n",
      "92-Train: 76.99732971191406 Test:92.9258041381836\n",
      "93-Train: 77.00245666503906 Test:92.92158508300781\n",
      "94-Train: 77.00726318359375 Test:92.91803741455078\n",
      "95-Train: 77.01173400878906 Test:92.91436004638672\n",
      "96-Train: 77.01593780517578 Test:92.91116333007812\n",
      "97-Train: 77.01990509033203 Test:92.90806579589844\n",
      "98-Train: 77.02360534667969 Test:92.90528869628906\n",
      "99-Train: 77.02706146240234 Test:92.90254211425781\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(boston_housing_data.data,\n",
    "                                                   boston_housing_data.target,\n",
    "                                                   test_size=0.33)\n",
    "\n",
    "\n",
    "def get_batch(X, iter, size):\n",
    "    return X[(iter*batch_size) : ((iter+1)*batch_size)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_features = train_X.shape[1]\n",
    "num_instances = train_y.shape[0]\n",
    "\n",
    "# Construction\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_features), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Boston-MLP\"):\n",
    "    hidden1 = tf.layers.dense(X, 3, name=\"Hidden-1\", activation = tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(X, 2, name=\"Hidden-2\", activation = tf.nn.relu)\n",
    "    output = tf.layers.dense(hidden2, 1, name=\"Price\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.square(y-output))\n",
    "    \n",
    "with tf.name_scope(\"train\"): \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for iteration in range(num_instances // batch_size + 1):\n",
    "            X_batch = get_batch(train_X, iteration, batch_size)\n",
    "            y_batch = get_batch(train_y, iteration, batch_size)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={X: X_batch,\n",
    "                                            y: y_batch})\n",
    "            \n",
    "        mse_train = loss.eval(feed_dict={X: X_batch,\n",
    "                                            y: y_batch})\n",
    "        mse_val = loss.eval(feed_dict={X:test_X, y: test_y})\n",
    "        \n",
    "        print(\"{}-Train: {} Test:{}\".format(epoch,\n",
    "                                           mse_train,\n",
    "                                           mse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example, you will see that the model converges early. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 339 samples, validate on 167 samples\n",
      "Epoch 1/400\n",
      "339/339 [==============================] - 0s 656us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 2/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 3/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 4/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 5/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 6/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 7/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 8/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 9/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 10/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 11/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 12/400\n",
      "339/339 [==============================] - ETA: 0s - loss: nan - mean_squared_error: n - 0s 109us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 13/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 14/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 15/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 16/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 17/400\n",
      "339/339 [==============================] - 0s 126us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 18/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 19/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 20/400\n",
      "339/339 [==============================] - 0s 153us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 21/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 22/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 23/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 24/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 25/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 26/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 27/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 28/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 29/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 30/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 31/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 32/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 33/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 34/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 35/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 36/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 37/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 38/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 39/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 40/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 41/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 42/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 43/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 44/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 45/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 46/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 47/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 48/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 49/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 50/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 51/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 52/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 54/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 55/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 56/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 57/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 58/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 59/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 60/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 61/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 62/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 63/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 64/400\n",
      "339/339 [==============================] - 0s 191us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 65/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 66/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 67/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 68/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 69/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 70/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 71/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 72/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 73/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 74/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 75/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 76/400\n",
      "339/339 [==============================] - 0s 124us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 77/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 78/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 79/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 80/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 81/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 82/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 83/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 84/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 85/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 86/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 87/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 88/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 89/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 90/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 91/400\n",
      "339/339 [==============================] - 0s 174us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 92/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 93/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 94/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 95/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 96/400\n",
      "339/339 [==============================] - 0s 185us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 97/400\n",
      "339/339 [==============================] - 0s 168us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 98/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 99/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 100/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 101/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 102/400\n",
      "339/339 [==============================] - 0s 144us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 103/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 104/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 105/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 126us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 106/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 107/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 108/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 109/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 110/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 111/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 112/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 113/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 114/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 115/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 116/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 117/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 118/400\n",
      "339/339 [==============================] - 0s 129us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 119/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 120/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 121/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 122/400\n",
      "339/339 [==============================] - 0s 109us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 123/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 124/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 125/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 126/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 127/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 128/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 129/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 130/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 131/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 132/400\n",
      "339/339 [==============================] - 0s 165us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 133/400\n",
      "339/339 [==============================] - 0s 159us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 134/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 135/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 136/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 137/400\n",
      "339/339 [==============================] - 0s 147us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 138/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 139/400\n",
      "339/339 [==============================] - ETA: 0s - loss: nan - mean_squared_error: n - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 140/400\n",
      "339/339 [==============================] - 0s 121us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 141/400\n",
      "339/339 [==============================] - 0s 126us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 142/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 143/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 144/400\n",
      "339/339 [==============================] - 0s 156us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 145/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 146/400\n",
      "339/339 [==============================] - 0s 179us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 147/400\n",
      "339/339 [==============================] - 0s 171us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 148/400\n",
      "339/339 [==============================] - 0s 150us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 149/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 150/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 151/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 152/400\n",
      "339/339 [==============================] - 0s 127us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 153/400\n",
      "339/339 [==============================] - 0s 132us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 154/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 155/400\n",
      "339/339 [==============================] - 0s 118us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 156/400\n",
      "339/339 [==============================] - 0s 138us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/400\n",
      "339/339 [==============================] - 0s 141us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 158/400\n",
      "339/339 [==============================] - 0s 135us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 159/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 160/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 161/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 162/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 163/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 164/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 165/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 166/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 167/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 168/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 169/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 170/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 171/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 172/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 173/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 174/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 175/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 176/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 177/400\n",
      "339/339 [==============================] - 0s 171us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 178/400\n",
      "339/339 [==============================] - 0s 162us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 179/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 180/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 181/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 182/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 183/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 184/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 185/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 186/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 187/400\n",
      "339/339 [==============================] - 0s 106us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 188/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 189/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 190/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 191/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 192/400\n",
      "339/339 [==============================] - 0s 112us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 193/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 194/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 195/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 196/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 197/400\n",
      "339/339 [==============================] - 0s 97us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 198/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 199/400\n",
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 200/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 201/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 202/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 203/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 204/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 205/400\n",
      "339/339 [==============================] - 0s 88us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 206/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 207/400\n",
      "339/339 [==============================] - 0s 115us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 208/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 209/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 0s 100us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 210/400\n",
      "339/339 [==============================] - 0s 103us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 211/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 212/400\n",
      "339/339 [==============================] - 0s 94us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 213/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 214/400\n",
      "339/339 [==============================] - 0s 91us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 215/400\n",
      " 10/339 [..............................] - ETA: 0s - loss: nan - mean_squared_error: nan"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(boston_housing_data.data,\n",
    "                                                   boston_housing_data.target,\n",
    "                                                   test_size=0.33)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu', input_shape=(13,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "\n",
    "model.compile(optimizer = tf.train.GradientDescentOptimizer(0.001),loss='mean_squared_error',metrics=['mse'])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    model.fit(train_X, train_y, epochs=400, batch_size=10, validation_data = (test_X, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
