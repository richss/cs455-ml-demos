{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook presents examples based upon the Hands on Machine Learning textbook by Geron covering chapter for on linear model training.\n",
    "\n",
    "* [Linear Regression](#linear-regression) \n",
    "* [Polynomial Regression](#Polynomial-Regression)\n",
    "* [Performance Analysis Using Learning Curves](#Performance-Analysis-Using-Learning-Curves)\n",
    "* [Regularization Techniques](#Regularization-Techniques)\n",
    "* [Boston Housing Regression Examples](#Boston-Housing-Regression-Examples) \n",
    "* [Logistic Regression and Softmax](#Logistic-Regression-and-Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "This section demonstrates multiple regression types including: **Closed-form solutions**, **Batch Gradient Descent**, **Stochastic Gradient Descent**, and **Mini-batch Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.linear_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-264b4bf9d451>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_models\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.linear_models'"
     ]
    }
   ],
   "source": [
    "# Includes\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_models import LinearRegression\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "X = 2 * np.random.rand(100,1)\n",
    "X_b = np.c_[np.ones((100,1)), X]\n",
    "y = 4 + 3*X + np.random.randn(100,1)  #y = 3x + 4  +guassian noise\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(X,y, \"b.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showProgress(i, X, y, theta):\n",
    "    \"\"\"\n",
    "    Helper function to show progress by (a) printing current theta and (b) plotting current\n",
    "    regression\n",
    "    \"\"\"\n",
    "    print(\"Theta (i=\" + str(i+1) + \"): \" + str(theta))\n",
    "    \n",
    "    # Predict on x=0 and x=2\n",
    "    X_new = np.array([[0],[2]])\n",
    "    X_new_b = np.c_[np.ones((2,1)), X_new] # [[1,0][1,2]]\n",
    "    y_predict = X_new_b.dot(theta)\n",
    "    \n",
    "    # Plot prediction\n",
    "    plt.plot(X_new, y_predict)\n",
    "    if (i==0): \n",
    "        plt.plot(X,y, \"b.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Closed-Form Solution\n",
    "\n",
    "This section shows the closed-form solution for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solve normal equation for theta\n",
    "theta = np.linalg.inv( X_b.T.dot(X_b) ).dot(X_b.T).dot(y)\n",
    "\n",
    "showProgress(0, X, y, theta)\n",
    "\n",
    "# Predictions based upon theta\n",
    "X_new = np.array([[0],[0.5],[1],[1.5],[2]])\n",
    "X_new_b = np.c_[np.ones((5,1)), X_new]\n",
    "y_predict = X_new_b.dot(theta)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Scikit-Learn\n",
    "\n",
    "This shows how to use the built-in function within SKLearn to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a linear regression and train it\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X,y)\n",
    "\n",
    "theta = [lin_reg.intercept_, lin_reg.coef_]\n",
    "showProgress(0, X, y, theta)\n",
    "\n",
    "# Make a prediction using trained model\n",
    "X_new = np.array([[0],[0.5],[1],[1.5],[2]])\n",
    "X_new_b = np.c_[np.ones((5,1)), X_new]\n",
    "y_predict = lin_reg.predict(X_new)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Linear Regression\n",
    "\n",
    "This section presents the batch linear regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set algorithm parameters\n",
    "eta = 0.1 \n",
    "n_iterations = 10000\n",
    "m = (len(X)) # number of inputs\n",
    "epsilon = 0.00001\n",
    "\n",
    "# Initialize Linear Model's parameters\n",
    "theta = np.random.randn(2,1)\n",
    "showProgress(0, X, y, theta)\n",
    "\n",
    "# Train Linear Model\n",
    "for i in range(0,n_iterations):\n",
    "    \n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)  # 2/m * XT.(X.theta - y)\n",
    "\n",
    "    if np.mean(-gradients) < epsilon:\n",
    "        break\n",
    "    \n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "    showProgress(i, X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions based upon theta\n",
    "X_new = np.array([[0],[0.5],[1],[1.5],[2]])\n",
    "X_new_b = np.c_[np.ones((5,1)), X_new] \n",
    "\n",
    "y_predict = X_new_b.dot(theta)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "This section implements a stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning Schedule\n",
    "t0, tn = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loop\n",
    "n_epochs = 50\n",
    "m = (len(X)) # number of inputs\n",
    "\n",
    "# Initialize Linear Model's parameters\n",
    "theta = np.random.randn(2,1)\n",
    "showProgress(0, X, y, theta)\n",
    "\n",
    "\n",
    "# Train Linear Model\n",
    "for epoch in range(0, n_epochs):\n",
    "    for i in range(0,m):\n",
    "    \n",
    "        rand_idx = np.random.randint(m)\n",
    "        xi = X_b[rand_idx:rand_idx+1]\n",
    "        yi = y[rand_idx:rand_idx+1]\n",
    "        \n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)  # 2 * XT.(X.theta - y)\n",
    "        eta = learning_schedule(epoch*m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "    showProgress(epoch, X, y, theta)\n",
    "\n",
    "plt.plot(X,y, \"b.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictions based upon theta\n",
    "# Data\n",
    "X_new = np.array([[0],[0.5],[1],[1.5],[2]])\n",
    "X_new_b = np.c_[np.ones((5,1)), X_new]\n",
    "\n",
    "# Prediction\n",
    "y_predict = X_new_b.dot(theta)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "Demonstrates an implementation of the mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning Schedule\n",
    "t0, tn = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loop\n",
    "n_epochs = 50\n",
    "m = (len(X)) # number of inputs\n",
    "\n",
    "# Initialize Linear Model's parameters\n",
    "theta = np.random.randn(2,1)\n",
    "showProgress(0, X, y, theta)\n",
    "\n",
    "batch_size = 5 # b\n",
    "\n",
    "\n",
    "# Train Linear Model\n",
    "for epoch in range(0, n_epochs):\n",
    "    for i in range(0,m):\n",
    "    \n",
    "        rand_idx = np.arange(m) #array from 0 to m-1\n",
    "        np.random.shuffle(rand_idx) #shuffle index arry\n",
    "\n",
    "        xi = X_b[rand_idx[:batch_size]] #grab labels for first b rows\n",
    "        yi = y[rand_idx[:batch_size]]   #grab labels for first b rows\n",
    "        \n",
    "        gradients = 2/batch_size * xi.T.dot(xi.dot(theta) - yi)  # 2/b * XT.(X.theta - y)\n",
    "        eta = learning_schedule(epoch*m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "    showProgress(epoch, X, y, theta)\n",
    "\n",
    "plt.plot(X,y, \"b.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictions based upon theta\n",
    "\n",
    "# Data\n",
    "X_new = np.array([[0],[0.5],[1],[1.5],[2]])\n",
    "X_new_b = np.c_[np.ones((5,1)), X_new]\n",
    "\n",
    "# Prediction\n",
    "y_predict = X_new_b.dot(theta)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "Implements polynomial regression using the PolynomialFeatures function of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Training Data\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m,1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m,1)\n",
    "\n",
    "plt.plot(X,y,\"b.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demonstrates the polynomial regression.  By changing the variable *degree* you can observe the change in the plot of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust degree to observe change in model\n",
    "degree = 10\n",
    "\n",
    "# Add extra features using PolynomialFeatures transform\n",
    "poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Train the modified input data set.\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "print(\"Trained Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "\n",
    "# Prepare test data\n",
    "x_test = [[0.1*x] for x in range(-30,30)]\n",
    "test_X_poly = poly_features.fit_transform(x_test)\n",
    "\n",
    "# Predict using test data\n",
    "predict = lin_reg.predict(test_X_poly)\n",
    "\n",
    "# Plot Scatter and Model\n",
    "plt.plot(X,y,\"b.\")\n",
    "plt.plot(x_test, predict,\"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis Using Learning Curves\n",
    "\n",
    "Demonstrated using Polynomial Regression examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y):\n",
    "    \"\"\"\n",
    "    Plots performance on the training set and testing (validation) set.\n",
    "    X-axis - number of training samples used\n",
    "    Y-axis - RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.20)\n",
    "    \n",
    "    training_errors, validation_errors = [], []\n",
    "    \n",
    "    for m in range(1, len(train_X)):\n",
    "        \n",
    "        model.fit(train_X[:m], train_y[:m])\n",
    "        \n",
    "        train_pred = model.predict(train_X)\n",
    "        test_pred = model.predict(test_X)\n",
    "        \n",
    "        training_errors.append(np.sqrt(mean_squared_error(train_y, train_pred)))\n",
    "        validation_errors.append(np.sqrt(mean_squared_error(test_y, test_pred)))\n",
    "        \n",
    "    plt.plot(training_errors, \"r-+\", label=\"train\")\n",
    "    plt.plot(validation_errors, \"b-\", label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.axis([0, 80, 0, 3])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate our plot\n",
    "plot_learning_curves(LinearRegression(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can observe the performance of a polynomial regression algorithm.\n",
    "\n",
    "Here, we use the Pipeline to first add the polynomial features and then perform a linear regression.  We can call this as a model in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "    (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "\n",
    "poly_reg = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "    (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(poly_reg, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Techniques\n",
    "\n",
    "Regularizaton helps reduce the overfitting for regression algorithms.  Here, we are demonstrating three variations of the regression algorithms previously covered called **Ridge**, **Lasso**, and **Elastic Net** Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New Includes\n",
    "from sklearn.linear_model import Ridge, SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration Data Set and Test Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Training Data\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m,1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m,1)\n",
    "\n",
    "plt.plot(X,y,\"b.\")\n",
    "\n",
    "# Test data - values between -3 to 3 in 0.1 increments\n",
    "x_test = [[0.1*x] for x in range(-30,30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed-Form Approach using Ridge class w/ Cholesky solver (formulation of closed form solution for regression with ridge regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "\n",
    "# Build and Train\n",
    "model = Ridge(alpha=alpha, solver=\"cholesky\")\n",
    "model.fit(X,y)\n",
    "\n",
    "# Plot polynomials for each alpha\n",
    "predict = model.predict(x_test)\n",
    "plt.plot(X,y,\"b.\")          \n",
    "plt.plot(x_test, predict,\"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how our model changes as we adjust our alpha from 1 to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(0,1001,100):\n",
    "    \n",
    "    alpha = (a+1)\n",
    "    \n",
    "    model = Ridge(alpha=alpha, solver=\"cholesky\")\n",
    "    model.fit(X,y.ravel())\n",
    "    \n",
    "    plt.figure(0)\n",
    "    \n",
    "    # Plot polynomials for each alpha\n",
    "    predict = model.predict(x_test)\n",
    "    plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "    plt.figure(a+1)\n",
    "   \n",
    "    # Plot learning curve for each alpha\n",
    "    plt.title(\"Alpha=\" + str(alpha))\n",
    "    plot_learning_curves(model, X, y.ravel())\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")\n",
    "\n",
    "# Note on Syntax\n",
    "# Figure 0 - plots curves for each model\n",
    "# Figure (a+1) - plots the figure for each alpha=(a+1)*0.1\n",
    "#  By specifying the figure before each plot, we tell matplot\n",
    "#  lib which plot to update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet shows the impact of Ridge regularization on polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 2\n",
    "for a in range(0,1001,100):\n",
    "    \n",
    "    alpha = a+1\n",
    "    \n",
    "    model = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "        (\"Ridged\", Ridge(alpha=alpha, solver=\"cholesky\")),\n",
    "        ])\n",
    "    model.fit(X,y.ravel())\n",
    "    \n",
    "    plt.figure(0)\n",
    "    \n",
    "    # Plot polynomials for each alpha\n",
    "    predict = model.predict(x_test)\n",
    "    plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "    plt.figure(a+1)\n",
    "   \n",
    "    # Plot learning curve for each alpha\n",
    "    plt.title(\"Alpha=\" + str(alpha))\n",
    "    plot_learning_curves(model, X, y.ravel())\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent w/ Ridge Regression Enabled as L2 penalty\n",
    "\n",
    "Using the stochastic gradient descent class in scikit-learn, we can apply the l2 penalty to achieve a Ridge Stochastic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 2\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")\n",
    "    \n",
    "# Build Polynomial SGD w/ Lasso Regularization\n",
    "model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "    (\"SGD Ridge\", SGDRegressor(penalty=\"l2\", max_iter=100000, tol=0.00001)),\n",
    "    ])\n",
    "    \n",
    "model.fit(X,y.ravel())\n",
    "\n",
    "# Plot polynomials for each alpha\n",
    "predict = model.predict(x_test)\n",
    "plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "plt.figure(1)\n",
    "   \n",
    "plot_learning_curves(model, X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regularization\n",
    "\n",
    "Lasso regularization can be used to implement lasso regression using the l1 penalty as the term for the regularization factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Includes\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration Data Set and Test Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Training Data\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m,1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m,1)\n",
    "\n",
    "plt.plot(X,y,\"b.\")\n",
    "\n",
    "# Test data - values between -3 to 3 in 0.1 increments\n",
    "x_test = [[0.1*x] for x in range(-30,30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression - Linear Example\n",
    "\n",
    "This demonstrates the Linear Regression with Lasso Model using the Lasso class built into SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "\n",
    "# Build and Train\n",
    "model = Lasso(alpha=alpha)\n",
    "model.fit(X,y)\n",
    "\n",
    "# Plot polynomials for each alpha\n",
    "predict = model.predict(x_test)\n",
    "plt.plot(X,y,\"b.\")          \n",
    "plt.plot(x_test, predict,\"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe our models and their learning curves across alphas from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(0,101,20):\n",
    "    \n",
    "    alpha = 0.01*(a+1)\n",
    "    \n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(X,y.ravel())\n",
    "    \n",
    "    plt.figure(0)\n",
    "    \n",
    "    # Plot polynomials for each alpha\n",
    "    predict = model.predict(x_test)\n",
    "    plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "    plt.figure(a+1)\n",
    "   \n",
    "    # Plot learning curve for each alpha\n",
    "    plt.title(\"Alpha=\" + str(a*0.01))\n",
    "    plot_learning_curves(model, X, y.ravel())\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")\n",
    "\n",
    "# Note on Syntax\n",
    "# Figure 0 - plots curves for each model\n",
    "# Figure (a+1) - plots the figure for each alpha=(a+1)*0.1\n",
    "#  By specifying the figure before each plot, we tell matplot\n",
    "#  lib which plot to update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression - Polynomial Example\n",
    "\n",
    "Now, we will change our model to a polynomial of a specified degreee.  We can then observe how our model is impacted as alpha is adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "deg = 2\n",
    "\n",
    "# Build and Train\n",
    "model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "    (\"Lasso\", Lasso(alpha=alpha)),\n",
    "    ])\n",
    "    \n",
    "model.fit(X,y)\n",
    "\n",
    "# Plot polynomials for each alpha\n",
    "predict = model.predict(x_test)\n",
    "plt.plot(X,y,\"b.\")          \n",
    "plt.plot(x_test, predict,\"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe our models and their learning curves across alphas from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in range(0,101,20):\n",
    "    \n",
    "    alpha = 0.01*(a+1)\n",
    "    \n",
    "    model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "    (\"Lasso\", Lasso(alpha=alpha)),\n",
    "    ])\n",
    "    \n",
    "    model.fit(X,y.ravel())\n",
    "    \n",
    "    plt.figure(0)\n",
    "    \n",
    "    # Plot polynomials for each alpha\n",
    "    predict = model.predict(x_test)\n",
    "    plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "    plt.figure(a+1)\n",
    "   \n",
    "    # Plot learning curve for each alpha\n",
    "    plt.title(\"Alpha=\" + str(a*0.01))\n",
    "    plot_learning_curves(model, X, y.ravel())\n",
    "    \n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression - Stochastic Gradient Descent Example\n",
    "\n",
    "Now, we will utilize steepest gradient descent with an l1 penalty to implement our lasso regression.  For our example, we will use a polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 2\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")\n",
    "    \n",
    "# Build Polynomial SGD w/ Lasso Regularization\n",
    "model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "    (\"SGD Lasso\", SGDRegressor(penalty=\"l1\", max_iter=1000, tol=0.01)),\n",
    "    ])\n",
    "    \n",
    "model.fit(X,y.ravel())\n",
    "\n",
    "# Plot polynomials for each alpha\n",
    "predict = model.predict(x_test)\n",
    "plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "plt.figure(1)\n",
    "   \n",
    "plot_learning_curves(model, X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regularization\n",
    "\n",
    "Elastic Net blends Ridge and Lasso regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Includes\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration Data Set and Test Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Training Data\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m,1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m,1)\n",
    "\n",
    "plt.plot(X,y,\"b.\")\n",
    "\n",
    "# Test data - values between -3 to 3 in 0.1 increments\n",
    "x_test = [[0.1*x] for x in range(-30,30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression - Linear Example\n",
    "\n",
    "This demonstrates the Linear Regression with Lasso Model using the Lasso class built into SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "\n",
    "# Build and Train\n",
    "model = ElasticNet(alpha=alpha)\n",
    "model.fit(X,y)\n",
    "\n",
    "# Plot polynomials for each alpha\n",
    "predict = model.predict(x_test)\n",
    "plt.plot(X,y,\"b.\")          \n",
    "plt.plot(x_test, predict,\"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe our models and their learning curves across alphas from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(0,101,20):\n",
    "    \n",
    "    alpha = 0.01*(a+1)\n",
    "    \n",
    "    model = ElasticNet(alpha=alpha)\n",
    "    model.fit(X,y.ravel())\n",
    "    \n",
    "    plt.figure(0)\n",
    "    \n",
    "    # Plot polynomials for each alpha\n",
    "    predict = model.predict(x_test)\n",
    "    plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "    plt.figure(a+1)\n",
    "   \n",
    "    # Plot learning curve for each alpha\n",
    "    plt.title(\"Alpha=\" + str(a*0.01))\n",
    "    plot_learning_curves(model, X, y.ravel())\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")\n",
    "\n",
    "# Note on Syntax\n",
    "# Figure 0 - plots curves for each model\n",
    "# Figure (a+1) - plots the figure for each alpha=(a+1)*0.1\n",
    "#  By specifying the figure before each plot, we tell matplot\n",
    "#  lib which plot to update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression - Polynomial Example\n",
    "\n",
    "Now, we will change our model to a polynomial of a specified degreee.  We can then observe how our model is impacted as alpha is adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "deg = 2\n",
    "\n",
    "# Build and Train\n",
    "model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "    (\"Elastic\", ElasticNet(alpha=alpha)),\n",
    "    ])\n",
    "    \n",
    "model.fit(X,y)\n",
    "\n",
    "# Plot polynomials for each alpha\n",
    "predict = model.predict(x_test)\n",
    "plt.plot(X,y,\"b.\")          \n",
    "plt.plot(x_test, predict,\"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe our models and their learning curves across alphas from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in range(0,101,20):\n",
    "    \n",
    "    alpha = 0.01*(a+1)\n",
    "    \n",
    "    model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "    (\"Elastic\", ElasticNet(alpha=alpha)),\n",
    "    ])\n",
    "    \n",
    "    model.fit(X,y.ravel())\n",
    "    \n",
    "    plt.figure(0)\n",
    "    \n",
    "    # Plot polynomials for each alpha\n",
    "    predict = model.predict(x_test)\n",
    "    plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "    plt.figure(a+1)\n",
    "   \n",
    "    # Plot learning curve for each alpha\n",
    "    plt.title(\"Alpha=\" + str(a*0.01))\n",
    "    plot_learning_curves(model, X, y.ravel())\n",
    "    \n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression - Stochastic Gradient Descent Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 2\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(X,y,\"b.\")\n",
    "    \n",
    "# Build Polynomial SGD w/ Lasso Regularization\n",
    "model = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "    (\"SGD ElasticNet\", SGDRegressor(penalty=\"elasticnet\", max_iter=1000, tol=0.01)),\n",
    "    ])\n",
    "    \n",
    "model.fit(X,y.ravel())\n",
    "\n",
    "# Plot polynomials for each alpha\n",
    "predict = model.predict(x_test)\n",
    "plt.plot(x_test, predict,\"r-\")\n",
    "    \n",
    "plt.figure(1)\n",
    "   \n",
    "plot_learning_curves(model, X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing Regression Examples\n",
    "\n",
    "Dataset User Guide: https://scikit-learn.org/stable/datasets/index.html#boston-dataset\n",
    "Dataset API Info: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston\n",
    "\n",
    "\n",
    "   * CRIM per capita crime rate by town\n",
    "   * ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "   * INDUS proportion of non-retail business acres per town\n",
    "   * CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "   * NOX nitric oxides concentration (parts per 10 million)\n",
    "   * RM average number of rooms per dwelling\n",
    "   * AGE proportion of owner-occupied units built prior to 1940\n",
    "   * DIS weighted distances to five Boston employment centres\n",
    "   * RAD index of accessibility to radial highways\n",
    "   * TAX full-value property-tax rate per \\$10,000\n",
    "   * PTRATIO pupil-teacher ratio by town\n",
    "   * B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "   * LSTAT % lower status of the population\n",
    "   * MEDV Median value of owner-occupied homes in \\$1000’s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# New Includes\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "boston_housing_data = datasets.load_boston()\n",
    "\n",
    "# Build data frame for visualization\n",
    "boston_df = pd.DataFrame(np.c_[boston_housing_data.data, boston_housing_data.target], \n",
    "                  columns=[\"CRIM\", \"ZN\",\"INDUS\",\"CHAS\", \"NOX\",\"RM\",\"AGE\",\n",
    "                           \"DIS\",\"RAD\",\"TAX\",\"PTRatio\",\"BK\", \"LSTAT\",\"MEDV\"])\n",
    "\n",
    "# Plot relationship between each attribute and MEDV\n",
    "for column in boston_df.columns:\n",
    "    \n",
    "    if column == \"MEDV\": continue\n",
    "    \n",
    "    plt.figure(column)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"MEDV\")\n",
    "    plt.plot(boston_df[column], boston_df[\"MEDV\"], \"b.\")\n",
    "\n",
    "scatter_matrix(boston_df, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "scaler = StandardScaler()\n",
    "boston_data_set = scaler.fit_transform(boston_housing_data.data)\n",
    "train_X, test_X, train_y, test_y = train_test_split(boston_data_set,\n",
    "                                                   boston_housing_data.target,\n",
    "                                                   test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how well we can model the boston housing data using a linear regressino function built into sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "print(\"Mean error: \" + str(mean_absolute_error(test_y, pred_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can observe the performance of a stochastic gradient regressor model **without any regularization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = SGDRegressor(penalty=\"none\", max_iter=10000, tol=0.001)\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can observe the performance of a gradient regressor model **with ridge regularization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lin_reg = Ridge(alpha=100)\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can observe the performance of a gradient regressor model **with lasso regularization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = Lasso(alpha=0.2)\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's observe performance with the elasticnet regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = ElasticNet(alpha=0.5)\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how the algorithm performs if we reduce the number of features based upon some of intiutions from viewing the data earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the new data set\n",
    "\n",
    "attributes = [\"AGE\",\"RM\",\"LSTAT\"]\n",
    "\n",
    "boston_data_set = boston_df[attributes]\n",
    "boston_data_set = scaler.fit_transform(boston_housing_data.data)\n",
    "train_X, test_X, train_y, test_y = train_test_split(boston_data_set,\n",
    "                                                   boston_housing_data.target,\n",
    "                                                   test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Demo\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = SGDRegressor(penalty=\"none\", max_iter=10000, tol=0.001)\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = Ridge(alpha=100)\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = Lasso(alpha=0.2)\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = ElasticNet(alpha=0.25)\n",
    "lin_reg.fit(train_X,train_y)\n",
    "pred_y = lin_reg.predict(test_X)\n",
    "\n",
    "# Outputs the intercept and coefficient of the model (theta_0 and theta_1 respectively)\n",
    "print(\"Theta:\")\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "plt.hist(abs(test_y - pred_y),bins=100)\n",
    "plt.xlabel(\"Error ($k)\")\n",
    "\n",
    "mean_absolute_error(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Softmax\n",
    "\n",
    "Logistic regression and softmax handle binary classification and multi-class classification, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the following plot of the logistic function.  As its input moves away from 0.5 it quickly outputs 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Logistic Function\n",
    "\n",
    "t = np.array([0.01*x for x in range(-1000,1000)])\n",
    "logit = 1 / (1 + np.exp(t-1))\n",
    "plt.plot(t,logit,'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use the Iris data set to demonstrate the logistic and softmax regression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris Data Set\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "categories = list(iris.feature_names)\n",
    "categories.append(\"Target\")\n",
    "iris_df = pd.DataFrame(np.c_[iris.data, iris.target], columns=categories)\n",
    "\n",
    "iris_df.describe()\n",
    "scatter_matrix(iris_df, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows a binary classification using the logistic function to determine if a particular flower is a virginica based upon solely its petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classify if virginica or not\n",
    "\n",
    "# Prepare Data\n",
    "X = iris[\"data\"][:,3].reshape(-1,1) # formats the petal width column from the data set\n",
    "y = (iris[\"target\"] == 2).astype(np.int) # only virginica is true (1)\n",
    "\n",
    "# build model\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X,y)\n",
    "\n",
    "# Determine probability curves for for x from 0 to 3.\n",
    "X_test = np.linspace(0,3,10000).reshape(-1,1)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot probability curves and prediction curves.\n",
    "plt.figure(\"Probabilities from logistic.\")\n",
    "plt.plot(X_test, y_proba[:,1], \"r-\")\n",
    "plt.plot(X_test, y_proba[:,0], \"b--\")\n",
    "plt.plot(X_test, y_pred, \"g-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now observe the algorithm's performance via a confusion matrix from a 10-fold cross-validation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score, accuracy_score, recall_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print(\"Pecision Score = \" + str(precision_score(y, y_pred)))\n",
    "print(\"Recall Score = \" + str(recall_score(y,y_pred)))\n",
    "print(\"F1 Score = \" + str(f1_score(y,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, observe the multi-class classification across all flower types using the petal width and length as our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:,(2,3)]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
    "model.fit(X,y)\n",
    "\n",
    "# Test the classifier\n",
    "print(model.predict([[5,2],[1,3]]))\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine our classifier's confusion matrix.  It is performing quite well for our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
